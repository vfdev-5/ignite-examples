{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of default SSD300 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "root_path = Path('..').absolute().resolve()\n",
    "if root_path.as_posix() not in sys.path:\n",
    "    sys.path.append(root_path.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ignite._utils import to_variable\n",
    "\n",
    "from torchvision.models.vgg import vgg16\n",
    "from customized_torchcv.models.ssd.net import SSD300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model description:\n",
    "\n",
    "```\n",
    "SSD300 = Extractor + Location & Label predictors\n",
    "```\n",
    "\n",
    "Input image size is 300x300 pixels\n",
    "\n",
    "Model parameters:\n",
    "- Number of classes\n",
    "- `num_anchors`: `(4, 6, 6, 6, 4, 4)` \n",
    "    - Related with number of boxes in location & class predictions\n",
    "    - Number of default boxes:\n",
    "      * for a given feature map size there are (2 + num aspect ratios * 2) * fm_size^2 of boxes generated\n",
    "      * num_anchors[i] = (2 + num_aspect_ratios[i] * 2)\n",
    "      \n",
    "- `steps`: `(8, 16, 32, 64, 100, 300)`\n",
    "    - Defines default boxes in `SSDBoxCoder`\n",
    "- `box_sizes`: `(30, 60, 111, 162, 213, 264, 315)`\n",
    "    - Defines default boxes in `SSDBoxCoder`\n",
    "- `aspect_ratios`: `((2,), (2,3), (2,3), (2,3), (2,), (2,))`\n",
    "    - Defines default boxes in `SSDBoxCoder`\n",
    "\n",
    "### Extractor\n",
    "\n",
    "Extractor is based on VGG16 network and produces 6 feature maps of size:\n",
    "\n",
    "- 512, 38x38\n",
    "- 1024, 19x19\n",
    "- 512, 10x10\n",
    "- 256, 3x3\n",
    "- 256, 1x1\n",
    "\n",
    "Extractor network is composed of the following layers:\n",
    "```\n",
    "                                                   6     7           8             9             10            11\n",
    "Extractor = [VGG16 features (block1-block5)] --> [C|R]-[C|R] -T- [C|R|C|R] -T- [C|R|C|R] -T- [C|R|C|R] -T- [C|R|C|R] -- fm5\n",
    "                                        |                     |             |             |             |\n",
    "                                    [L2 norm]--- fm0         fm1           fm2           fm3           fm4\n",
    "``` \n",
    "\n",
    "- Block 6 = Dilated convolution\n",
    "- Blocks 8_1, 9_1, 10_1, 11_1 = Compressions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SSD300(num_classes=10)\n",
    "vgg = vgg16(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  vs  Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)  vs  ReLU(inplace)\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  vs  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)  vs  ReLU(inplace)\n",
      "MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=True)  vs  MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  vs  Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)  vs  ReLU(inplace)\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  vs  Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)  vs  ReLU(inplace)\n",
      "MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=True)  vs  MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  vs  Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)  vs  ReLU(inplace)\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  vs  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)  vs  ReLU(inplace)\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  vs  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)  vs  ReLU(inplace)\n",
      "MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=True)  vs  MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  vs  Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)  vs  ReLU(inplace)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  vs  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)  vs  ReLU(inplace)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  vs  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)  vs  ReLU(inplace)\n"
     ]
    }
   ],
   "source": [
    "# Compare extractor layers (block1-block4) with VGG16\n",
    "for i in range(23):\n",
    "    l1 = repr(model.extractor.features.layers[i])\n",
    "    l2 = repr(vgg.features[i])\n",
    "    print(\"{}  vs  {}\".format(l1, l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  vs  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  vs  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  vs  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "# Compare block 5\n",
    "extractor_block5 = [model.extractor.conv5_1, model.extractor.conv5_2, model.extractor.conv5_3]\n",
    "vgg16_block5 = [vgg.features[24], vgg.features[26], vgg.features[28]]\n",
    "\n",
    "for l1, l2 in zip(extractor_block5, vgg16_block5):\n",
    "    l1 = repr(l1)\n",
    "    l2 = repr(l2)\n",
    "    print(\"{}  vs  {}\".format(l1, l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blocks 8, 9, 10, 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6, Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
      "7, Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "8_1, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "8_2, Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "9_1, Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "9_2, Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "10_1, Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "10_2, Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "11_1, Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "11_2, Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "extractor_last_blocks = [\n",
    "    ('6', model.extractor.conv6),\n",
    "    ('7', model.extractor.conv7),\n",
    "    ('8_1', model.extractor.conv8_1),\n",
    "    ('8_2', model.extractor.conv8_2),\n",
    "    ('9_1', model.extractor.conv9_1),\n",
    "    ('9_2', model.extractor.conv9_2),    \n",
    "    ('10_1', model.extractor.conv10_1),\n",
    "    ('10_2', model.extractor.conv10_2),    \n",
    "    ('11_1', model.extractor.conv11_1),\n",
    "    ('11_2', model.extractor.conv11_2)\n",
    "]\n",
    "\n",
    "for n, l in extractor_last_blocks:\n",
    "    print(\"{}, {}\".format(n, repr(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = to_variable(torch.rand(12, 3, 300, 300))\n",
    "feature_maps = model.extractor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 <class 'list'>\n",
      "0 torch.Size([12, 512, 38, 38])\n",
      "1 torch.Size([12, 1024, 19, 19])\n",
      "2 torch.Size([12, 512, 10, 10])\n",
      "3 torch.Size([12, 256, 5, 5])\n",
      "4 torch.Size([12, 256, 3, 3])\n",
      "5 torch.Size([12, 256, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(len(feature_maps), type(feature_maps))\n",
    "for i, o in enumerate(feature_maps):\n",
    "    print(i, o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location & Label predictors\n",
    "\n",
    "These predictors are just convolutions with a particular number of output feature maps\n",
    "\n",
    "Location layer number of output feature maps is fixed by number of anchors x 4. For all 6 feature maps they are:\n",
    "```\n",
    "[16, 24, 24, 24, 16, 16]\n",
    "```\n",
    "Classification layer number of output feature maps is computed as number of anchors x number of classes (e.g. 10):\n",
    "```\n",
    "[40, 60, 60, 60, 40, 40]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 24, 24, 24, 16, 16]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([n * 4 for n in model.num_anchors])\n",
    "model.loc_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 60, 60, 60, 40, 40]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Conv2d(512, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): Conv2d(1024, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (2): Conv2d(512, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): Conv2d(256, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): Conv2d(256, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): Conv2d(256, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([n * 10 for n in model.num_anchors])\n",
    "model.cls_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final predictions on a feature map from extractor is computed as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: torch.Size([12, 512, 38, 38])\n",
      "1: torch.Size([12, 16, 38, 38])\n",
      "2: torch.Size([12, 38, 38, 16])\n",
      "3: torch.Size([12, 5776, 4])\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"0:\", feature_maps[index].shape)\n",
    "loc_pred = model.loc_layers[index](feature_maps[index])\n",
    "print(\"1:\", loc_pred.shape)\n",
    "loc_pred = loc_pred.permute(0,2,3,1).contiguous()\n",
    "print(\"2:\", loc_pred.shape)\n",
    "loc_pred = loc_pred.view(loc_pred.size(0),-1,4)\n",
    "print(\"3:\", loc_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: torch.Size([12, 512, 38, 38])\n",
      "1: torch.Size([12, 40, 38, 38])\n",
      "2: torch.Size([12, 38, 38, 40])\n",
      "3: torch.Size([12, 5776, 10])\n"
     ]
    }
   ],
   "source": [
    "n_classes = 10\n",
    "index = 0\n",
    "\n",
    "print(\"0:\", feature_maps[index].shape)\n",
    "cls_pred = model.cls_layers[index](feature_maps[index])\n",
    "print(\"1:\", cls_pred.shape)\n",
    "cls_pred = cls_pred.permute(0,2,3,1).contiguous()\n",
    "print(\"2:\", cls_pred.shape)\n",
    "cls_pred = cls_pred.view(cls_pred.size(0),-1, n_classes)\n",
    "print(\"3:\", cls_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output loc pred 0 : torch.Size([12, 5776, 4])\n",
      "Output loc pred 1 : torch.Size([12, 2166, 4])\n",
      "Output loc pred 2 : torch.Size([12, 600, 4])\n",
      "Output loc pred 3 : torch.Size([12, 150, 4])\n",
      "Output loc pred 4 : torch.Size([12, 36, 4])\n",
      "Output loc pred 5 : torch.Size([12, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "for index in range(6):\n",
    "    loc_pred = model.loc_layers[index](feature_maps[index])\n",
    "    loc_pred = loc_pred.permute(0,2,3,1).contiguous()\n",
    "    loc_pred = loc_pred.view(loc_pred.size(0),-1,4)\n",
    "    print(\"Output loc pred {} :\".format(index), loc_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output cls pred 0 : torch.Size([12, 5776, 10])\n",
      "Output cls pred 1 : torch.Size([12, 2166, 10])\n",
      "Output cls pred 2 : torch.Size([12, 600, 10])\n",
      "Output cls pred 3 : torch.Size([12, 150, 10])\n",
      "Output cls pred 4 : torch.Size([12, 36, 10])\n",
      "Output cls pred 5 : torch.Size([12, 4, 10])\n"
     ]
    }
   ],
   "source": [
    "for index in range(6):\n",
    "    cls_pred = model.cls_layers[index](feature_maps[index])\n",
    "    cls_pred = cls_pred.permute(0,2,3,1).contiguous()\n",
    "    cls_pred = cls_pred.view(cls_pred.size(0),-1, n_classes)\n",
    "    print(\"Output cls pred {} :\".format(index), cls_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, all predictions are concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_preds, cls_preds = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([8732, 4]) torch.Size([8732, 10])\n"
     ]
    }
   ],
   "source": [
    "for i, (loc_pred, cls_pred) in enumerate(zip(loc_preds, cls_preds)):\n",
    "    print(i, loc_pred.shape, cls_pred.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default boxes\n",
    "\n",
    "Default boxes are generated to \n",
    "\n",
    "- encode ground truth bounding boxes, see `SSDBoxCoder.encode`\n",
    "- decode predictions, see `SSDBoxCoder.decode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given models parameters\n",
    "steps = (8, 16, 32, 64, 100, 300)\n",
    "fm_sizes = (38, 19, 10, 5, 3, 1)\n",
    "box_sizes = (30, 60, 111, 162, 213, 264, 315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature map:  38\n",
      "      4.0 4.0 30\n",
      "      300.0 300.0 30\n",
      "Feature map:  19\n",
      "      8.0 8.0 60\n",
      "      296.0 296.0 60\n",
      "Feature map:  10\n",
      "      16.0 16.0 111\n",
      "      304.0 304.0 111\n",
      "Feature map:  5\n",
      "      32.0 32.0 162\n",
      "      288.0 288.0 162\n",
      "Feature map:  3\n",
      "      50.0 50.0 213\n",
      "      250.0 250.0 213\n",
      "Feature map:  1\n",
      "      150.0 150.0 264\n",
      "      150.0 150.0 264\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "for i, fm_size in enumerate(fm_sizes):\n",
    "    print(\"Feature map: \", fm_size)\n",
    "    hws = list(itertools.product(range(fm_size), repeat=2))\n",
    "    for h, w in [hws[0], hws[-1]]:\n",
    "        cx = (w + 0.5) * steps[i]\n",
    "        cy = (h + 0.5) * steps[i]\n",
    "        s = box_sizes[i]\n",
    "        print(\"     \", cx, cy, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   4.0000    4.0000   30.0000   30.0000\n",
       "   4.0000    4.0000   42.4264   42.4264\n",
       "   4.0000    4.0000   42.4264   21.2132\n",
       "                   â‹®                    \n",
       " 150.0000  150.0000  288.3748  288.3748\n",
       " 150.0000  150.0000  373.3524  186.6762\n",
       " 150.0000  150.0000  186.6762  373.3524\n",
       "[torch.FloatTensor of size 8732x4]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from customized_torchcv.models.ssd import SSDBoxCoder\n",
    "from customized_torchcv.utils.box import box_iou, change_box_order\n",
    "\n",
    "box_coder = SSDBoxCoder(model)\n",
    "box_coder._get_default_boxes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests on some example bounding boxes\n",
    "- small objects\n",
    "- large objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8732, 4]) torch.Size([8732])\n",
      "\n",
      "  10.0000   10.0000   11.0000   11.0000\n",
      "   1.0000    1.0000    5.0000    5.0000\n",
      " 150.0000  150.0000  155.0000  155.0000\n",
      "[torch.FloatTensor of size 3x4]\n",
      "\n",
      "\n",
      " 0.7000\n",
      " 0.7000\n",
      " 0.7000\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "objs = torch.Tensor([[1, 1, 5, 5], [10, 10, 11, 11], [150, 150, 155, 155]])\n",
    "labels = torch.LongTensor([1, 0, 2])\n",
    "\n",
    "loc_targets, cls_targets = box_coder.encode(objs, labels)\n",
    "print(loc_targets.shape, cls_targets.shape)\n",
    "\n",
    "cls_preds = torch.zeros(cls_targets.shape[0], 4)  # 1 + 3 classes\n",
    "cls_preds.scatter_(1, cls_targets.unsqueeze(1), 0.7)  # set class proba to 0.75\n",
    "boxes, labels, scores = box_coder.decode(loc_targets, cls_preds, score_thresh=0.0, nms_thresh=0.0)\n",
    "\n",
    "print(boxes)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -140.00718688964844 1.53206467628479\n",
      "1 -140.00718688964844 3.06412935256958\n",
      "2 -22.681140899658203 -8.341645240783691\n",
      "3 -22.681140899658203 -7.225927829742432\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(i, loc_targets[:, i].min(), loc_targets[:, i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_targets.min(), cls_targets.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_boxes = box_coder._get_default_boxes()  # xywh\n",
    "# default_boxes = change_box_order(default_boxes, 'xywh2xyxy')\n",
    "# ious = box_iou(default_boxes, objs)  # [#anchors, #obj]\n",
    "# ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8732, 4]) torch.Size([8732])\n",
      "\n",
      "   1.0000    1.0000   50.0000   50.0000\n",
      " 150.0000  150.0000  255.0000  255.0000\n",
      "[torch.FloatTensor of size 2x4]\n",
      "\n",
      "\n",
      " 0.7000\n",
      " 0.7000\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "objs = torch.Tensor([[1, 1, 50, 50], [150, 150, 255, 255]])\n",
    "labels = torch.LongTensor([0, 1])\n",
    "\n",
    "loc_targets, cls_targets = box_coder.encode(objs, labels)\n",
    "print(loc_targets.shape, cls_targets.shape)\n",
    "\n",
    "cls_preds = torch.zeros(cls_targets.shape[0], 3)  # 1 + 2 classes\n",
    "cls_preds.scatter_(1, cls_targets.unsqueeze(1), 0.7)  # set class proba to 0.75\n",
    "boxes, labels, scores = box_coder.decode(loc_targets, cls_preds, score_thresh=0.0, nms_thresh=0.0)\n",
    "\n",
    "print(boxes)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Loss function is a sum of Smooth L1 (location) and Cross-Entropy (classification) terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from customized_torchcv.loss.ssd_loss import SSDLoss\n",
    "\n",
    "loss_fn = SSDLoss(num_classes=model.num_classes)\n",
    "\n",
    "x = to_variable(torch.rand(1, 3, 300, 300))\n",
    "loc_preds, cls_preds = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8732, 10])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.7277434 , -0.34593922, -0.44672066,  0.12167928, -0.32203   ,\n",
       "         1.1578766 , -0.20052607,  0.52037036, -0.66037816,  0.11640968],\n",
       "       dtype=float32),\n",
       " array([ 0.22206742,  0.211182  ,  0.4276462 , -0.02398141], dtype=float32))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_preds[0, 0, :].data.numpy(), loc_preds[0, 0, :].data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8732])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objs = torch.Tensor([[1, 1, 5, 5], [10, 10, 11, 11], [150, 150, 155, 155]])\n",
    "labels = torch.LongTensor([1, 0, 2])\n",
    "loc_targets, cls_targets = box_coder.encode(objs, labels)\n",
    "loc_targets = loc_targets.unsqueeze(0)\n",
    "cls_targets = cls_targets.unsqueeze(0)\n",
    "cls_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([1, 8732, 4])\n"
     ]
    }
   ],
   "source": [
    "# positive targets: ground truth encoded boxes\n",
    "pos = cls_targets > 0\n",
    "print(pos.long().sum())\n",
    "\n",
    "# get the mask of positives\n",
    "mask = pos.unsqueeze(2).expand_as(loc_preds)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "loc_loss = F.smooth_l1_loss(loc_preds[mask], to_variable(loc_targets[mask]), size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 75.0979\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
