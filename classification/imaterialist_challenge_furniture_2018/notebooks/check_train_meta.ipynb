{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check meta-model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(0, Path(\".\").absolute().parent.as_posix())\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from common.dataset import FilesFromCsvDataset, TransformedDataset\n",
    "from common.meta import get_metafeatures, get_imsize_and_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.set(rc={'figure.figsize':(12, 10)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topk_df(df, k):\n",
    "    topk_values = np.argsort(df.values, axis=1)[:, -k:]\n",
    "    cols = [\"top_{}\".format(k - i) for i in range(k)]\n",
    "    topk_df = pd.DataFrame(topk_values, index=df.index, columns=cols)\n",
    "    return topk_df\n",
    "\n",
    "\n",
    "def get_metafeatures(prediction_files):\n",
    "    dfs = [pd.read_csv(f, index_col='id') for f in prediction_files]\n",
    "    for i, df in enumerate(dfs):\n",
    "        df.columns = [\"f{}_{}\".format(i, c) for c in df.columns]\n",
    "    meta_features = pd.concat([df for df in dfs], axis=1)\n",
    "    return meta_features\n",
    "\n",
    "\n",
    "def get_topk_metafeatures(prediction_files, k=5):\n",
    "    dfs = [pd.read_csv(f, index_col='id') for f in prediction_files]\n",
    "    dfs = [create_topk_df(df, k=k) for df in dfs]\n",
    "    for i, df in enumerate(dfs):\n",
    "        df.columns = [\"f{}_{}\".format(i, c) for c in df.columns]\n",
    "    meta_features = pd.concat([df for df in dfs], axis=1)\n",
    "    return meta_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features_path = Path(\"../output\")\n",
    "meta_features_list = [\n",
    "    meta_features_path / \"val_probas_inceptionresnetv2_350_resized_crop\" / \"20180428_1622\" / \"probas.csv\",\n",
    "    meta_features_path / \"val_probas_inceptionv4_350_resized_crop\" / \"20180428_1633\" / \"probas.csv\",\n",
    "    meta_features_path / \"val_probas_nasnetalarge_350_resized_crop\" / \"20180428_1654\" / \"probas.csv\",\n",
    "]\n",
    "# meta_features, dfs = get_topk_metafeatures(meta_features_list)\n",
    "meta_features = get_metafeatures(meta_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_features.loc[6, ['f0_c124', 'f0_c46', 'f1_c124', 'f1_c46']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_c0</th>\n",
       "      <th>f0_c1</th>\n",
       "      <th>f0_c2</th>\n",
       "      <th>f0_c3</th>\n",
       "      <th>f0_c4</th>\n",
       "      <th>f0_c5</th>\n",
       "      <th>f0_c6</th>\n",
       "      <th>f0_c7</th>\n",
       "      <th>f0_c8</th>\n",
       "      <th>f0_c9</th>\n",
       "      <th>...</th>\n",
       "      <th>f2_c118</th>\n",
       "      <th>f2_c119</th>\n",
       "      <th>f2_c120</th>\n",
       "      <th>f2_c121</th>\n",
       "      <th>f2_c122</th>\n",
       "      <th>f2_c123</th>\n",
       "      <th>f2_c124</th>\n",
       "      <th>f2_c125</th>\n",
       "      <th>f2_c126</th>\n",
       "      <th>f2_c127</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6302</th>\n",
       "      <td>1.373452e-11</td>\n",
       "      <td>1.510409e-10</td>\n",
       "      <td>1.260431e-12</td>\n",
       "      <td>4.921817e-12</td>\n",
       "      <td>1.109817e-10</td>\n",
       "      <td>2.236590e-12</td>\n",
       "      <td>5.073530e-11</td>\n",
       "      <td>2.922072e-12</td>\n",
       "      <td>2.468959e-12</td>\n",
       "      <td>5.981444e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>1.210329e-07</td>\n",
       "      <td>8.096192e-08</td>\n",
       "      <td>1.443976e-07</td>\n",
       "      <td>2.385026e-08</td>\n",
       "      <td>7.237251e-08</td>\n",
       "      <td>9.203653e-08</td>\n",
       "      <td>2.834459e-08</td>\n",
       "      <td>3.299012e-07</td>\n",
       "      <td>4.945206e-07</td>\n",
       "      <td>1.290316e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3349</th>\n",
       "      <td>6.708128e-07</td>\n",
       "      <td>5.170703e-07</td>\n",
       "      <td>1.846702e-07</td>\n",
       "      <td>4.019679e-07</td>\n",
       "      <td>2.481107e-07</td>\n",
       "      <td>3.374197e-07</td>\n",
       "      <td>2.376433e-06</td>\n",
       "      <td>9.245439e-07</td>\n",
       "      <td>4.313464e-03</td>\n",
       "      <td>3.201822e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>4.495173e-06</td>\n",
       "      <td>6.100723e-06</td>\n",
       "      <td>2.764779e-05</td>\n",
       "      <td>3.801815e-06</td>\n",
       "      <td>5.896711e-04</td>\n",
       "      <td>5.913841e-06</td>\n",
       "      <td>2.201900e-06</td>\n",
       "      <td>1.681049e-05</td>\n",
       "      <td>4.249565e-06</td>\n",
       "      <td>2.703849e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>1.588944e-07</td>\n",
       "      <td>3.460540e-06</td>\n",
       "      <td>1.504418e-05</td>\n",
       "      <td>3.487897e-06</td>\n",
       "      <td>3.223231e-08</td>\n",
       "      <td>1.037788e-06</td>\n",
       "      <td>5.816736e-07</td>\n",
       "      <td>1.317494e-08</td>\n",
       "      <td>5.965195e-07</td>\n",
       "      <td>3.312266e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>9.882467e-01</td>\n",
       "      <td>3.446758e-05</td>\n",
       "      <td>8.426102e-06</td>\n",
       "      <td>5.871575e-06</td>\n",
       "      <td>4.653336e-06</td>\n",
       "      <td>1.017057e-05</td>\n",
       "      <td>5.020957e-06</td>\n",
       "      <td>7.487863e-06</td>\n",
       "      <td>3.937877e-05</td>\n",
       "      <td>6.771946e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2677</th>\n",
       "      <td>5.284062e-11</td>\n",
       "      <td>1.166769e-08</td>\n",
       "      <td>2.623240e-11</td>\n",
       "      <td>6.023646e-10</td>\n",
       "      <td>3.024778e-07</td>\n",
       "      <td>3.700055e-10</td>\n",
       "      <td>2.080192e-10</td>\n",
       "      <td>8.870108e-11</td>\n",
       "      <td>1.285723e-10</td>\n",
       "      <td>1.011352e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.152033e-08</td>\n",
       "      <td>6.093003e-07</td>\n",
       "      <td>1.255923e-07</td>\n",
       "      <td>2.138172e-08</td>\n",
       "      <td>3.975764e-08</td>\n",
       "      <td>1.062255e-07</td>\n",
       "      <td>1.688745e-06</td>\n",
       "      <td>5.186873e-07</td>\n",
       "      <td>1.641713e-06</td>\n",
       "      <td>2.073703e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>1.060053e-04</td>\n",
       "      <td>2.394826e-07</td>\n",
       "      <td>4.066390e-06</td>\n",
       "      <td>1.615211e-05</td>\n",
       "      <td>1.085549e-07</td>\n",
       "      <td>6.640157e-05</td>\n",
       "      <td>1.039728e-06</td>\n",
       "      <td>5.748253e-08</td>\n",
       "      <td>1.454809e-06</td>\n",
       "      <td>2.930471e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>4.788781e-04</td>\n",
       "      <td>6.484089e-02</td>\n",
       "      <td>4.440381e-06</td>\n",
       "      <td>1.617641e-06</td>\n",
       "      <td>3.059961e-06</td>\n",
       "      <td>2.804265e-06</td>\n",
       "      <td>1.500740e-05</td>\n",
       "      <td>4.778179e-06</td>\n",
       "      <td>5.958621e-06</td>\n",
       "      <td>8.954229e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             f0_c0         f0_c1         f0_c2         f0_c3         f0_c4  \\\n",
       "id                                                                           \n",
       "6302  1.373452e-11  1.510409e-10  1.260431e-12  4.921817e-12  1.109817e-10   \n",
       "3349  6.708128e-07  5.170703e-07  1.846702e-07  4.019679e-07  2.481107e-07   \n",
       "484   1.588944e-07  3.460540e-06  1.504418e-05  3.487897e-06  3.223231e-08   \n",
       "2677  5.284062e-11  1.166769e-08  2.623240e-11  6.023646e-10  3.024778e-07   \n",
       "1517  1.060053e-04  2.394826e-07  4.066390e-06  1.615211e-05  1.085549e-07   \n",
       "\n",
       "             f0_c5         f0_c6         f0_c7         f0_c8         f0_c9  \\\n",
       "id                                                                           \n",
       "6302  2.236590e-12  5.073530e-11  2.922072e-12  2.468959e-12  5.981444e-12   \n",
       "3349  3.374197e-07  2.376433e-06  9.245439e-07  4.313464e-03  3.201822e-07   \n",
       "484   1.037788e-06  5.816736e-07  1.317494e-08  5.965195e-07  3.312266e-07   \n",
       "2677  3.700055e-10  2.080192e-10  8.870108e-11  1.285723e-10  1.011352e-10   \n",
       "1517  6.640157e-05  1.039728e-06  5.748253e-08  1.454809e-06  2.930471e-05   \n",
       "\n",
       "          ...            f2_c118       f2_c119       f2_c120       f2_c121  \\\n",
       "id        ...                                                                \n",
       "6302      ...       1.210329e-07  8.096192e-08  1.443976e-07  2.385026e-08   \n",
       "3349      ...       4.495173e-06  6.100723e-06  2.764779e-05  3.801815e-06   \n",
       "484       ...       9.882467e-01  3.446758e-05  8.426102e-06  5.871575e-06   \n",
       "2677      ...       2.152033e-08  6.093003e-07  1.255923e-07  2.138172e-08   \n",
       "1517      ...       4.788781e-04  6.484089e-02  4.440381e-06  1.617641e-06   \n",
       "\n",
       "           f2_c122       f2_c123       f2_c124       f2_c125       f2_c126  \\\n",
       "id                                                                           \n",
       "6302  7.237251e-08  9.203653e-08  2.834459e-08  3.299012e-07  4.945206e-07   \n",
       "3349  5.896711e-04  5.913841e-06  2.201900e-06  1.681049e-05  4.249565e-06   \n",
       "484   4.653336e-06  1.017057e-05  5.020957e-06  7.487863e-06  3.937877e-05   \n",
       "2677  3.975764e-08  1.062255e-07  1.688745e-06  5.186873e-07  1.641713e-06   \n",
       "1517  3.059961e-06  2.804265e-06  1.500740e-05  4.778179e-06  5.958621e-06   \n",
       "\n",
       "           f2_c127  \n",
       "id                  \n",
       "6302  1.290316e-07  \n",
       "3349  2.703849e-06  \n",
       "484   6.771946e-06  \n",
       "2677  2.073703e-07  \n",
       "1517  8.954229e-05  \n",
       "\n",
       "[5 rows x 384 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6300, 384)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FilesFromCsvDataset(\"../output/filtered_val_dataset.csv\")\n",
    "dataset = TransformedDataset(dataset,\n",
    "                             transforms=lambda x: (x, Image.open(x).size),\n",
    "                             target_transforms=lambda l: l - 1)\n",
    "df_imsize_targets = get_imsize_and_targets(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6252, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>target</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2923</th>\n",
       "      <td>800</td>\n",
       "      <td>27</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5059</th>\n",
       "      <td>500</td>\n",
       "      <td>62</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>800</td>\n",
       "      <td>92</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3893</th>\n",
       "      <td>320</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>800</td>\n",
       "      <td>69</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      height  target  width\n",
       "2923     800      27    800\n",
       "5059     500      62    500\n",
       "38       800      92    800\n",
       "3893     320       2    500\n",
       "2352     800      69    800"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_imsize_targets.shape)\n",
    "df_imsize_targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(df, col_name):\n",
    "    m1 = df[col_name].min()\n",
    "    m2 = df[col_name].max()\n",
    "    df.loc[:, col_name] = (df[col_name] - m1) / (m2 - m1 + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imsize_targets.loc[:, 'size'] = df_imsize_targets['width'] * df_imsize_targets['height']\n",
    "\n",
    "min_max_scale(df_imsize_targets, 'width')\n",
    "min_max_scale(df_imsize_targets, 'height')\n",
    "min_max_scale(df_imsize_targets, 'size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([meta_features, df_imsize_targets[['width', 'height', 'size']]], axis=1)\n",
    "X.dropna(inplace=True)\n",
    "y = df_imsize_targets.loc[X.index, 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6224, 387), (6224,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_c0</th>\n",
       "      <th>f0_c1</th>\n",
       "      <th>f0_c2</th>\n",
       "      <th>f0_c3</th>\n",
       "      <th>f0_c4</th>\n",
       "      <th>f0_c5</th>\n",
       "      <th>f0_c6</th>\n",
       "      <th>f0_c7</th>\n",
       "      <th>f0_c8</th>\n",
       "      <th>f0_c9</th>\n",
       "      <th>...</th>\n",
       "      <th>f2_c121</th>\n",
       "      <th>f2_c122</th>\n",
       "      <th>f2_c123</th>\n",
       "      <th>f2_c124</th>\n",
       "      <th>f2_c125</th>\n",
       "      <th>f2_c126</th>\n",
       "      <th>f2_c127</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.786763e-10</td>\n",
       "      <td>2.216359e-09</td>\n",
       "      <td>3.354208e-11</td>\n",
       "      <td>5.480898e-09</td>\n",
       "      <td>1.016345e-10</td>\n",
       "      <td>2.880471e-09</td>\n",
       "      <td>5.597683e-10</td>\n",
       "      <td>1.103374e-08</td>\n",
       "      <td>3.588944e-11</td>\n",
       "      <td>3.318691e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>4.248070e-08</td>\n",
       "      <td>1.542437e-07</td>\n",
       "      <td>1.546837e-07</td>\n",
       "      <td>2.476480e-06</td>\n",
       "      <td>1.771896e-07</td>\n",
       "      <td>3.419645e-07</td>\n",
       "      <td>3.491577e-08</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004338</td>\n",
       "      <td>0.000390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.454174e-06</td>\n",
       "      <td>4.968147e-09</td>\n",
       "      <td>1.728579e-04</td>\n",
       "      <td>1.411724e-04</td>\n",
       "      <td>9.133302e-09</td>\n",
       "      <td>1.432796e-06</td>\n",
       "      <td>1.634426e-08</td>\n",
       "      <td>7.527779e-06</td>\n",
       "      <td>1.150549e-08</td>\n",
       "      <td>1.903884e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>5.956465e-05</td>\n",
       "      <td>4.131234e-06</td>\n",
       "      <td>1.263950e-05</td>\n",
       "      <td>3.168242e-05</td>\n",
       "      <td>6.963891e-04</td>\n",
       "      <td>7.767742e-06</td>\n",
       "      <td>5.908821e-05</td>\n",
       "      <td>0.085129</td>\n",
       "      <td>0.088576</td>\n",
       "      <td>0.014782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.776277e-09</td>\n",
       "      <td>4.009195e-08</td>\n",
       "      <td>8.143656e-09</td>\n",
       "      <td>4.890069e-09</td>\n",
       "      <td>1.291318e-07</td>\n",
       "      <td>9.818061e-09</td>\n",
       "      <td>3.529312e-07</td>\n",
       "      <td>2.192131e-08</td>\n",
       "      <td>1.592583e-07</td>\n",
       "      <td>7.115647e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>1.951818e-07</td>\n",
       "      <td>3.182044e-07</td>\n",
       "      <td>5.872605e-05</td>\n",
       "      <td>2.711008e-07</td>\n",
       "      <td>2.351123e-06</td>\n",
       "      <td>1.549696e-07</td>\n",
       "      <td>7.264244e-07</td>\n",
       "      <td>0.067755</td>\n",
       "      <td>0.027657</td>\n",
       "      <td>0.005999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.465966e-08</td>\n",
       "      <td>9.050931e-09</td>\n",
       "      <td>4.340031e-09</td>\n",
       "      <td>1.269720e-03</td>\n",
       "      <td>9.529219e-09</td>\n",
       "      <td>4.225530e-09</td>\n",
       "      <td>1.333823e-08</td>\n",
       "      <td>5.796793e-04</td>\n",
       "      <td>2.795894e-09</td>\n",
       "      <td>1.915915e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>3.195381e-06</td>\n",
       "      <td>2.326123e-06</td>\n",
       "      <td>1.075147e-05</td>\n",
       "      <td>7.188110e-06</td>\n",
       "      <td>9.196233e-01</td>\n",
       "      <td>4.219503e-06</td>\n",
       "      <td>3.570874e-06</td>\n",
       "      <td>0.085129</td>\n",
       "      <td>0.043565</td>\n",
       "      <td>0.009204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.250429e-07</td>\n",
       "      <td>2.871629e-11</td>\n",
       "      <td>2.409671e-09</td>\n",
       "      <td>1.104301e-05</td>\n",
       "      <td>2.534062e-10</td>\n",
       "      <td>3.698222e-07</td>\n",
       "      <td>1.303015e-09</td>\n",
       "      <td>5.900128e-08</td>\n",
       "      <td>1.484111e-10</td>\n",
       "      <td>1.620004e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>5.987777e-09</td>\n",
       "      <td>2.220414e-08</td>\n",
       "      <td>3.893851e-08</td>\n",
       "      <td>1.038806e-08</td>\n",
       "      <td>1.222228e-08</td>\n",
       "      <td>9.131732e-09</td>\n",
       "      <td>4.092041e-08</td>\n",
       "      <td>0.093815</td>\n",
       "      <td>0.097614</td>\n",
       "      <td>0.017097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 387 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          f0_c0         f0_c1         f0_c2         f0_c3         f0_c4  \\\n",
       "1  1.786763e-10  2.216359e-09  3.354208e-11  5.480898e-09  1.016345e-10   \n",
       "2  1.454174e-06  4.968147e-09  1.728579e-04  1.411724e-04  9.133302e-09   \n",
       "3  7.776277e-09  4.009195e-08  8.143656e-09  4.890069e-09  1.291318e-07   \n",
       "4  9.465966e-08  9.050931e-09  4.340031e-09  1.269720e-03  9.529219e-09   \n",
       "5  1.250429e-07  2.871629e-11  2.409671e-09  1.104301e-05  2.534062e-10   \n",
       "\n",
       "          f0_c5         f0_c6         f0_c7         f0_c8         f0_c9  \\\n",
       "1  2.880471e-09  5.597683e-10  1.103374e-08  3.588944e-11  3.318691e-09   \n",
       "2  1.432796e-06  1.634426e-08  7.527779e-06  1.150549e-08  1.903884e-09   \n",
       "3  9.818061e-09  3.529312e-07  2.192131e-08  1.592583e-07  7.115647e-08   \n",
       "4  4.225530e-09  1.333823e-08  5.796793e-04  2.795894e-09  1.915915e-10   \n",
       "5  3.698222e-07  1.303015e-09  5.900128e-08  1.484111e-10  1.620004e-09   \n",
       "\n",
       "     ...          f2_c121       f2_c122       f2_c123       f2_c124  \\\n",
       "1    ...     4.248070e-08  1.542437e-07  1.546837e-07  2.476480e-06   \n",
       "2    ...     5.956465e-05  4.131234e-06  1.263950e-05  3.168242e-05   \n",
       "3    ...     1.951818e-07  3.182044e-07  5.872605e-05  2.711008e-07   \n",
       "4    ...     3.195381e-06  2.326123e-06  1.075147e-05  7.188110e-06   \n",
       "5    ...     5.987777e-09  2.220414e-08  3.893851e-08  1.038806e-08   \n",
       "\n",
       "        f2_c125       f2_c126       f2_c127     width    height      size  \n",
       "1  1.771896e-07  3.419645e-07  3.491577e-08  0.004170  0.004338  0.000390  \n",
       "2  6.963891e-04  7.767742e-06  5.908821e-05  0.085129  0.088576  0.014782  \n",
       "3  2.351123e-06  1.549696e-07  7.264244e-07  0.067755  0.027657  0.005999  \n",
       "4  9.196233e-01  4.219503e-06  3.570874e-06  0.085129  0.043565  0.009204  \n",
       "5  1.222228e-08  9.131732e-09  4.092041e-08  0.093815  0.097614  0.017097  \n",
       "\n",
       "[5 rows x 387 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 127, 1     37\n",
       " 2     62\n",
       " 3     32\n",
       " 4    125\n",
       " 5     17\n",
       " Name: target, dtype: int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.min(), y.max(), y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_features.loc[6302, :], df_imsize_targets.loc[6302, 'width'], y.loc[6302]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(pd.concat([X, y], axis=1).corr(), linewidths=.5);\n",
    "# plt.yticks(rotation=0);\n",
    "# plt.xticks(rotation=30);\n",
    "# sns.set(font_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassifed = {1: {'recall': 0.84, 'wrong_classes': [(87, 4)]},\n",
    " 3: {'recall': 0.5625, 'wrong_classes': [(2, 7), (28, 5)]},\n",
    " 14: {'recall': 0.3, 'wrong_classes': [(3, 4), (28, 5), (62, 8), (125, 6)]},\n",
    " 18: {'recall': 0.66, 'wrong_classes': [(127, 7)]},\n",
    " 21: {'recall': 0.7872340425531915, 'wrong_classes': [(16, 4)]},\n",
    " 22: {'recall': 0.7551020408163265, 'wrong_classes': [(62, 6)]},\n",
    " 26: {'recall': 0.6938775510204082, 'wrong_classes': [(111, 9)]},\n",
    " 27: {'recall': 0.8979591836734694, 'wrong_classes': [(23, 4)]},\n",
    " 30: {'recall': 0.7916666666666666, 'wrong_classes': [(69, 6)]},\n",
    " 34: {'recall': 0.7916666666666666, 'wrong_classes': [(12, 4), (69, 4)]},\n",
    " 38: {'recall': 0.68, 'wrong_classes': [(86, 11), (108, 5)]},\n",
    " 48: {'recall': 0.7346938775510204, 'wrong_classes': [(124, 5)]},\n",
    " 49: {'recall': 0.6530612244897959, 'wrong_classes': [(19, 4), (53, 12)]},\n",
    " 50: {'recall': 0.75, 'wrong_classes': [(52, 4)]},\n",
    " 53: {'recall': 0.7755102040816326, 'wrong_classes': [(19, 4)]},\n",
    " 57: {'recall': 0.8775510204081632, 'wrong_classes': [(2, 4)]},\n",
    " 58: {'recall': 0.86, 'wrong_classes': [(41, 4)]},\n",
    " 62: {'recall': 0.3, 'wrong_classes': [(14, 6), (22, 5), (25, 8), (28, 9)]},\n",
    " 65: {'recall': 0.48, 'wrong_classes': [(31, 5), (39, 7), (56, 6), (101, 4)]},\n",
    " 66: {'recall': 0.8541666666666666, 'wrong_classes': [(112, 5)]},\n",
    " 69: {'recall': 0.7959183673469388, 'wrong_classes': [(116, 4)]},\n",
    " 81: {'recall': 0.88, 'wrong_classes': [(126, 4)]},\n",
    " 86: {'recall': 0.9, 'wrong_classes': [(38, 4)]},\n",
    " 87: {'recall': 0.74, 'wrong_classes': [(1, 4), (53, 5)]},\n",
    " 96: {'recall': 0.8, 'wrong_classes': [(88, 4)]},\n",
    " 99: {'recall': 0.7959183673469388, 'wrong_classes': [(19, 7)]},\n",
    " 104: {'recall': 0.6875, 'wrong_classes': [(59, 12)]},\n",
    " 107: {'recall': 0.86, 'wrong_classes': [(4, 4)]},\n",
    " 108: {'recall': 0.875, 'wrong_classes': [(38, 6)]},\n",
    " 111: {'recall': 0.8, 'wrong_classes': [(26, 7)]},\n",
    " 112: {'recall': 0.9, 'wrong_classes': [(66, 4)]},\n",
    " 113: {'recall': 0.78, 'wrong_classes': [(81, 8)]},\n",
    " 114: {'recall': 0.8163265306122449, 'wrong_classes': [(120, 5)]},\n",
    " 123: {'recall': 0.6122448979591837, 'wrong_classes': [(64, 17)]},\n",
    " 126: {'recall': 0.8125, 'wrong_classes': [(81, 7)]},\n",
    " 127: {'recall': 0.8125, 'wrong_classes': [(18, 5)]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_probas(x_test):\n",
    "    mean_probas = np.zeros((len(x_test), n_classes))\n",
    "    for i in range(n):\n",
    "        mean_probas += x_test[:,i * n_classes:(i + 1) * n_classes]\n",
    "    return mean_probas\n",
    "\n",
    "\n",
    "def get_baseline_preds(x_test):\n",
    "    return np.argmax(get_baseline_probas(x_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN as meta-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=10)]: Done 250 out of 250 | elapsed:  9.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 84, 'leaf_size': 30, 'weights': 'distance'} -0.9530072283384319\n",
      "Recall: 0.8779092001748252 vs 0.8844287223193472\n",
      "Precision: 0.8868391724951927 vs 0.8939847561802985\n",
      "Accuracy: 0.87853470437018 vs 0.8849614395886889\n"
     ]
    }
   ],
   "source": [
    "n_classes = 128\n",
    "seed = 555\n",
    "n = len(meta_features_list)\n",
    "\n",
    "_X = X.values\n",
    "_y = y.values\n",
    "\n",
    "splt = StratifiedShuffleSplit(n_splits=7, test_size=0.25, random_state=seed)\n",
    "train_index, test_index = next(splt.split(_X, _y))\n",
    "\n",
    "_X_train = _X[train_index, :]\n",
    "_X_test = _X[test_index, :]\n",
    "_y_train = _y[train_index]\n",
    "_y_test = _y[test_index]\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "knn = KNeighborsClassifier()\n",
    "params = {\n",
    "    \"n_neighbors\": np.linspace(30, 100, dtype=np.int),\n",
    "    \"weights\": [\"distance\", ],\n",
    "    \"leaf_size\": [30, ]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(knn, params, scoring=\"neg_log_loss\", cv=cv, n_jobs=10, verbose=True)    \n",
    "gs.fit(_X_train, _y_train)\n",
    "print(gs.best_params_, gs.best_score_)\n",
    "\n",
    "_y_pred = gs.best_estimator_.predict(_X_test)\n",
    "_y_pred_base = get_baseline_preds(_X_test)\n",
    "\n",
    "print(\"Recall: {} vs {}\".format(recall_score(_y_test, _y_pred, average=\"macro\"), recall_score(_y_test, _y_pred_base, average=\"macro\")))\n",
    "print(\"Precision: {} vs {}\".format(precision_score(_y_test, _y_pred, average=\"macro\"), precision_score(_y_test, _y_pred_base, average=\"macro\")))\n",
    "print(\"Accuracy: {} vs {}\".format(accuracy_score(_y_test, _y_pred), accuracy_score(_y_test, _y_pred_base)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple DNN as meta-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Iteration 1, loss = 4.72803632\n",
      "Iteration 1, loss = 4.72852929\n",
      "Iteration 1, loss = 4.72473743\n",
      "Iteration 1, loss = 4.72728458\n",
      "Iteration 1, loss = 4.72852929\n",
      "Iteration 1, loss = 4.72854867\n",
      "Iteration 1, loss = 4.72728458\n",
      "Iteration 1, loss = 4.72473743\n",
      "Iteration 2, loss = 4.12045899\n",
      "Iteration 1, loss = 4.72803632\n",
      "Iteration 2, loss = 4.13299555\n",
      "Iteration 1, loss = 4.72854867\n",
      "Iteration 2, loss = 4.11111548\n",
      "Iteration 2, loss = 4.13299555\n",
      "Iteration 2, loss = 4.11784633\n",
      "Iteration 3, loss = 2.47481665\n",
      "Iteration 2, loss = 4.13324873\n",
      "Iteration 2, loss = 4.11784633\n",
      "Iteration 2, loss = 4.11111548\n",
      "Iteration 3, loss = 2.44644701\n",
      "Iteration 3, loss = 2.51197675\n",
      "Iteration 3, loss = 2.51197675\n",
      "Iteration 3, loss = 2.45432327\n",
      "Iteration 2, loss = 4.12045899\n",
      "Iteration 2, loss = 4.13324873\n",
      "Iteration 4, loss = 0.80317356\n",
      "Iteration 4, loss = 0.78412869\n",
      "Iteration 3, loss = 2.48845747\n",
      "Iteration 3, loss = 2.45432327\n",
      "Iteration 4, loss = 0.80375446\n",
      "Iteration 4, loss = 0.80375446\n",
      "Iteration 3, loss = 2.44644701\n",
      "Iteration 3, loss = 2.48845747\n",
      "Iteration 4, loss = 0.78351704\n",
      "Iteration 3, loss = 2.47481665\n",
      "Iteration 5, loss = 0.56601635\n",
      "Iteration 5, loss = 0.54817077\n",
      "Iteration 4, loss = 0.82155572\n",
      "Iteration 4, loss = 0.78351704\n",
      "Iteration 5, loss = 0.56150650\n",
      "Iteration 4, loss = 0.78412869\n",
      "Iteration 5, loss = 0.56150650\n",
      "Iteration 4, loss = 0.80317356\n",
      "Iteration 4, loss = 0.82155572\n",
      "Iteration 6, loss = 0.48761051\n",
      "Iteration 6, loss = 0.50709159\n",
      "Iteration 5, loss = 0.55582114\n",
      "Iteration 5, loss = 0.57442720\n",
      "Iteration 5, loss = 0.55582114\n",
      "Iteration 7, loss = 0.45351527\n",
      "Iteration 5, loss = 0.57442720\n",
      "Iteration 6, loss = 0.49462002\n",
      "Iteration 5, loss = 0.54817077\n",
      "Iteration 5, loss = 0.56601635\n",
      "Iteration 6, loss = 0.49462002\n",
      "Iteration 7, loss = 0.44651137\n",
      "Iteration 6, loss = 0.49036694\n",
      "Iteration 6, loss = 0.51537258\n",
      "Iteration 6, loss = 0.48761051\n",
      "Iteration 6, loss = 0.49036694\n",
      "Iteration 6, loss = 0.51537258\n",
      "Iteration 7, loss = 0.45341093\n",
      "Iteration 7, loss = 0.45341093\n",
      "Iteration 8, loss = 0.42851373\n",
      "Iteration 8, loss = 0.40985764\n",
      "Iteration 7, loss = 0.45120232\n",
      "Iteration 6, loss = 0.50709159\n",
      "Iteration 7, loss = 0.45120232\n",
      "Iteration 9, loss = 0.40432402\n",
      "Iteration 7, loss = 0.44651137\n",
      "Iteration 7, loss = 0.47442682\n",
      "Iteration 7, loss = 0.47442682\n",
      "Iteration 8, loss = 0.41809108\n",
      "Iteration 8, loss = 0.41809108\n",
      "Iteration 8, loss = 0.41471285\n",
      "Iteration 8, loss = 0.44206865\n",
      "Iteration 7, loss = 0.45351527\n",
      "Iteration 8, loss = 0.44206865\n",
      "Iteration 9, loss = 0.38851578\n",
      "Iteration 10, loss = 0.37988777\n",
      "Iteration 8, loss = 0.40985764\n",
      "Iteration 8, loss = 0.41471285\n",
      "Iteration 9, loss = 0.38818475\n",
      "Iteration 9, loss = 0.38705087\n",
      "Iteration 9, loss = 0.38818475\n",
      "Iteration 9, loss = 0.41631569\n",
      "Iteration 11, loss = 0.36149421\n",
      "Iteration 8, loss = 0.42851373\n",
      "Iteration 9, loss = 0.38851578\n",
      "Iteration 9, loss = 0.41631569\n",
      "Iteration 10, loss = 0.37176016\n",
      "Iteration 9, loss = 0.38705087\n",
      "Iteration 10, loss = 0.37103450\n",
      "Iteration 10, loss = 0.36877294\n",
      "Iteration 10, loss = 0.36877294\n",
      "Iteration 12, loss = 0.34875268\n",
      "Iteration 9, loss = 0.40432402\n",
      "Iteration 10, loss = 0.37176016\n",
      "Iteration 10, loss = 0.38890811\n",
      "Iteration 10, loss = 0.38890811\n",
      "Iteration 11, loss = 0.34961047\n",
      "Iteration 11, loss = 0.35658344\n",
      "Iteration 10, loss = 0.37103450\n",
      "Iteration 11, loss = 0.35658344\n",
      "Iteration 11, loss = 0.35108684\n",
      "Iteration 10, loss = 0.37988777\n",
      "Iteration 11, loss = 0.34961047\n",
      "Iteration 11, loss = 0.37403195\n",
      "Iteration 13, loss = 0.33481673\n",
      "Iteration 11, loss = 0.37403195\n",
      "Iteration 12, loss = 0.33250186\n",
      "Iteration 11, loss = 0.35108684\n",
      "Iteration 12, loss = 0.34217339\n",
      "Iteration 12, loss = 0.34217339\n",
      "Iteration 12, loss = 0.33846159\n",
      "Iteration 13, loss = 0.33494003\n",
      "Iteration 11, loss = 0.36149421\n",
      "Iteration 13, loss = 0.32890068\n",
      "Iteration 12, loss = 0.33250186\n",
      "Iteration 12, loss = 0.35198435\n",
      "Iteration 14, loss = 0.32114903\n",
      "Iteration 12, loss = 0.33846159\n",
      "Iteration 12, loss = 0.35198435\n",
      "Iteration 13, loss = 0.33494003\n",
      "Iteration 13, loss = 0.32263028\n",
      "Iteration 14, loss = 0.31719024\n",
      "Iteration 12, loss = 0.34875268\n",
      "Iteration 14, loss = 0.32087012\n",
      "Iteration 13, loss = 0.32263028\n",
      "Iteration 13, loss = 0.34309836\n",
      "Iteration 13, loss = 0.34309836\n",
      "Iteration 15, loss = 0.30962549\n",
      "Iteration 13, loss = 0.32890068\n",
      "Iteration 15, loss = 0.30438581\n",
      "Iteration 14, loss = 0.31719024\n",
      "Iteration 14, loss = 0.31311448\n",
      "Iteration 15, loss = 0.30428012\n",
      "Iteration 13, loss = 0.33481673\n",
      "Iteration 14, loss = 0.31311448\n",
      "Iteration 14, loss = 0.33166193\n",
      "Iteration 16, loss = 0.29415303\n",
      "Iteration 14, loss = 0.33166193\n",
      "Iteration 16, loss = 0.30056817\n",
      "Iteration 15, loss = 0.29614909\n",
      "Iteration 14, loss = 0.32087012\n",
      "Iteration 15, loss = 0.30438581\n",
      "Iteration 15, loss = 0.29614909\n",
      "Iteration 17, loss = 0.28476179\n",
      "Iteration 14, loss = 0.32114903\n",
      "Iteration 16, loss = 0.29591187\n",
      "Iteration 15, loss = 0.32305816\n",
      "Iteration 15, loss = 0.32305816\n",
      "Iteration 15, loss = 0.30428012\n",
      "Iteration 17, loss = 0.29634163\n",
      "Iteration 16, loss = 0.29288831\n",
      "Iteration 16, loss = 0.29415303\n",
      "Iteration 16, loss = 0.29288831\n",
      "Iteration 17, loss = 0.28474567\n",
      "Iteration 18, loss = 0.28009019\n",
      "Iteration 15, loss = 0.30962549\n",
      "Iteration 16, loss = 0.30870929\n",
      "Iteration 16, loss = 0.30870929\n",
      "Iteration 18, loss = 0.29332786\n",
      "Iteration 17, loss = 0.28476179\n",
      "Iteration 18, loss = 0.27340458\n",
      "Iteration 16, loss = 0.29591187\n",
      "Iteration 19, loss = 0.27174224\n",
      "Iteration 17, loss = 0.29179598\n",
      "Iteration 17, loss = 0.29179598\n",
      "Iteration 17, loss = 0.30325576\n",
      "Iteration 17, loss = 0.30325576\n",
      "Iteration 17, loss = 0.28474567\n",
      "Iteration 19, loss = 0.27181302\n",
      "Iteration 18, loss = 0.28009019\n",
      "Iteration 16, loss = 0.30056817\n",
      "Iteration 18, loss = 0.27340458\n",
      "Iteration 19, loss = 0.27787519\n",
      "Iteration 18, loss = 0.27952565\n",
      "Iteration 20, loss = 0.26720702\n",
      "Iteration 18, loss = 0.29799547\n",
      "Iteration 20, loss = 0.26433726\n",
      "Iteration 18, loss = 0.29799547\n",
      "Iteration 18, loss = 0.27952565\n",
      "Iteration 19, loss = 0.27181302\n",
      "Iteration 19, loss = 0.27174224\n",
      "Iteration 20, loss = 0.27135399\n",
      "Iteration 21, loss = 0.25865024\n",
      "Iteration 17, loss = 0.29634163\n",
      "Iteration 19, loss = 0.29375467\n",
      "Iteration 19, loss = 0.27794680\n",
      "Iteration 21, loss = 0.25970559\n",
      "Iteration 19, loss = 0.29375467\n",
      "Iteration 20, loss = 0.26433726\n",
      "Iteration 19, loss = 0.27794680\n",
      "Iteration 20, loss = 0.26720702\n",
      "Iteration 21, loss = 0.26611808\n",
      "Iteration 18, loss = 0.29332786\n",
      "Iteration 20, loss = 0.28483769\n",
      "Iteration 20, loss = 0.26528396\n",
      "Iteration 22, loss = 0.25606551\n",
      "Iteration 22, loss = 0.25238201\n",
      "Iteration 19, loss = 0.27787519\n",
      "Iteration 21, loss = 0.25970559\n",
      "Iteration 20, loss = 0.26528396\n",
      "Iteration 22, loss = 0.26193652\n",
      "Iteration 20, loss = 0.28483769\n",
      "Iteration 21, loss = 0.25865024\n",
      "Iteration 20, loss = 0.27135399\n",
      "Iteration 21, loss = 0.27461308\n",
      "Iteration 22, loss = 0.25238201\n",
      "Iteration 21, loss = 0.26037217\n",
      "Iteration 23, loss = 0.25853570\n",
      "Iteration 23, loss = 0.24507302\n",
      "Iteration 23, loss = 0.26235207\n",
      "Iteration 21, loss = 0.26037217\n",
      "Iteration 21, loss = 0.27461308\n",
      "Iteration 23, loss = 0.24507302\n",
      "Iteration 22, loss = 0.27406474\n",
      "Iteration 22, loss = 0.25606551\n",
      "Iteration 21, loss = 0.26611808\n",
      "Iteration 22, loss = 0.25407079\n",
      "Iteration 24, loss = 0.24924129\n",
      "Iteration 24, loss = 0.24421122\n",
      "Iteration 22, loss = 0.27406474\n",
      "Iteration 24, loss = 0.24421122\n",
      "Iteration 24, loss = 0.25019999\n",
      "Iteration 23, loss = 0.26958239\n",
      "Iteration 22, loss = 0.25407079\n",
      "Iteration 23, loss = 0.25853570\n",
      "Iteration 23, loss = 0.25719430\n",
      "Iteration 25, loss = 0.24580397\n",
      "Iteration 22, loss = 0.26193652\n",
      "Iteration 23, loss = 0.26958239\n",
      "Iteration 25, loss = 0.24803508\n",
      "Iteration 25, loss = 0.23492495\n",
      "Iteration 23, loss = 0.25719430\n",
      "Iteration 26, loss = 0.24527405\n",
      "Iteration 25, loss = 0.23492495\n",
      "Iteration 24, loss = 0.25999321\n",
      "Iteration 24, loss = 0.24924129\n",
      "Iteration 24, loss = 0.25999321\n",
      "Iteration 26, loss = 0.24600258\n",
      "Iteration 24, loss = 0.24799614\n",
      "Iteration 23, loss = 0.26235207\n",
      "Iteration 26, loss = 0.23514729\n",
      "Iteration 26, loss = 0.23514729\n",
      "Iteration 25, loss = 0.25528419\n",
      "Iteration 24, loss = 0.24799614\n",
      "Iteration 27, loss = 0.23254871\n",
      "Iteration 24, loss = 0.25019999\n",
      "Iteration 25, loss = 0.25528419\n",
      "Iteration 25, loss = 0.24644258\n",
      "Iteration 25, loss = 0.24580397\n",
      "Iteration 26, loss = 0.25668597\n",
      "Iteration 27, loss = 0.23877536\n",
      "Iteration 27, loss = 0.22791377\n",
      "Iteration 25, loss = 0.24644258\n",
      "Iteration 27, loss = 0.22791377\n",
      "Iteration 28, loss = 0.22628231\n",
      "Iteration 26, loss = 0.25668597\n",
      "Iteration 25, loss = 0.24803508\n",
      "Iteration 27, loss = 0.25453045\n",
      "Iteration 26, loss = 0.24434650\n",
      "Iteration 26, loss = 0.24527405\n",
      "Iteration 28, loss = 0.22724368\n",
      "Iteration 28, loss = 0.22724368\n",
      "Iteration 28, loss = 0.23827192\n",
      "Iteration 26, loss = 0.24434650\n",
      "Iteration 27, loss = 0.23518488\n",
      "Iteration 27, loss = 0.25453045\n",
      "Iteration 29, loss = 0.22842154\n",
      "Iteration 28, loss = 0.25651374\n",
      "Iteration 29, loss = 0.22373449\n",
      "Iteration 26, loss = 0.24600258\n",
      "Iteration 27, loss = 0.23254871\n",
      "Iteration 29, loss = 0.22842154\n",
      "Iteration 29, loss = 0.23884478\n",
      "Iteration 27, loss = 0.23518488\n",
      "Iteration 28, loss = 0.25651374\n",
      "Iteration 28, loss = 0.23430419\n",
      "Iteration 29, loss = 0.24972171\n",
      "Iteration 30, loss = 0.22433546\n",
      "Iteration 27, loss = 0.23877536\n",
      "Iteration 28, loss = 0.22628231\n",
      "Iteration 28, loss = 0.23430419\n",
      "Iteration 30, loss = 0.22223825\n",
      "Iteration 30, loss = 0.22433546\n",
      "Iteration 30, loss = 0.23800448\n",
      "Iteration 29, loss = 0.24972171\n",
      "Iteration 29, loss = 0.22373449\n",
      "Iteration 29, loss = 0.22774151\n",
      "Iteration 30, loss = 0.24380245\n",
      "Iteration 28, loss = 0.23827192\n",
      "Iteration 29, loss = 0.22774151\n",
      "Iteration 31, loss = 0.22113351\n",
      "Iteration 31, loss = 0.22113351\n",
      "Iteration 31, loss = 0.22078155\n",
      "Iteration 30, loss = 0.22223825\n",
      "Iteration 30, loss = 0.24380245\n",
      "Iteration 30, loss = 0.22354787\n",
      "Iteration 29, loss = 0.23884478\n",
      "Iteration 31, loss = 0.23485939\n",
      "Iteration 31, loss = 0.23602735\n",
      "Iteration 30, loss = 0.22354787\n",
      "Iteration 32, loss = 0.22087601\n",
      "Iteration 32, loss = 0.22087601\n",
      "Iteration 31, loss = 0.23602735\n",
      "Iteration 32, loss = 0.21637375\n",
      "Iteration 31, loss = 0.22078155\n",
      "Iteration 31, loss = 0.22073276\n",
      "Iteration 30, loss = 0.23800448\n",
      "Iteration 32, loss = 0.22472694\n",
      "Iteration 31, loss = 0.22073276\n",
      "Iteration 33, loss = 0.21458592\n",
      "Iteration 32, loss = 0.23136514\n",
      "Iteration 33, loss = 0.21458592\n",
      "Iteration 32, loss = 0.23136514\n",
      "Iteration 32, loss = 0.21998741\n",
      "Iteration 33, loss = 0.21652713\n",
      "Iteration 32, loss = 0.21637375\n",
      "Iteration 34, loss = 0.20990164\n",
      "Iteration 32, loss = 0.21998741\n",
      "Iteration 31, loss = 0.23485939\n",
      "Iteration 33, loss = 0.23231449\n",
      "Iteration 34, loss = 0.20990164\n",
      "Iteration 33, loss = 0.21884521\n",
      "Iteration 33, loss = 0.21947741\n",
      "Iteration 33, loss = 0.23231449\n",
      "Iteration 34, loss = 0.21318986\n",
      "Iteration 32, loss = 0.22472694\n",
      "Iteration 35, loss = 0.21235069\n",
      "Iteration 33, loss = 0.21652713\n",
      "Iteration 34, loss = 0.22078510\n",
      "Iteration 35, loss = 0.21235069\n",
      "Iteration 33, loss = 0.21884521\n",
      "Iteration 34, loss = 0.22230754\n",
      "Iteration 34, loss = 0.20916702\n",
      "Iteration 33, loss = 0.21947741\n",
      "Iteration 35, loss = 0.20872166\n",
      "Iteration 34, loss = 0.22078510\n",
      "Iteration 36, loss = 0.20307843\n",
      "Iteration 36, loss = 0.20307843\n",
      "Iteration 35, loss = 0.22730624\n",
      "Iteration 34, loss = 0.20916702\n",
      "Iteration 34, loss = 0.21318986\n",
      "Iteration 34, loss = 0.22230754\n",
      "Iteration 35, loss = 0.21459746\n",
      "Iteration 35, loss = 0.20465012\n",
      "Iteration 37, loss = 0.20270467\n",
      "Iteration 36, loss = 0.20794235\n",
      "Iteration 37, loss = 0.20270467\n",
      "Iteration 35, loss = 0.22730624\n",
      "Iteration 36, loss = 0.22128028\n",
      "Iteration 35, loss = 0.20872166\n",
      "Iteration 35, loss = 0.20465012\n",
      "Iteration 35, loss = 0.21459746\n",
      "Iteration 36, loss = 0.21057579\n",
      "Iteration 36, loss = 0.20989092\n",
      "Iteration 37, loss = 0.20644951\n",
      "Iteration 38, loss = 0.20845373\n",
      "Iteration 38, loss = 0.20845373\n",
      "Iteration 36, loss = 0.22128028\n",
      "Iteration 37, loss = 0.21953610\n",
      "Iteration 36, loss = 0.20794235\n",
      "Iteration 36, loss = 0.20989092\n",
      "Iteration 38, loss = 0.20452072\n",
      "Iteration 36, loss = 0.21057579\n",
      "Iteration 37, loss = 0.20883906\n",
      "Iteration 37, loss = 0.21953610\n",
      "Iteration 37, loss = 0.20852108\n",
      "Iteration 39, loss = 0.20237064\n",
      "Iteration 39, loss = 0.20237064\n",
      "Iteration 37, loss = 0.20852108\n",
      "Iteration 38, loss = 0.21496686\n",
      "Iteration 37, loss = 0.20644951\n",
      "Iteration 39, loss = 0.19855653\n",
      "Iteration 37, loss = 0.20883906\n",
      "Iteration 38, loss = 0.20452072\n",
      "Iteration 38, loss = 0.20579022\n",
      "Iteration 40, loss = 0.19748645\n",
      "Iteration 38, loss = 0.21496686\n",
      "Iteration 38, loss = 0.20026266\n",
      "Iteration 38, loss = 0.20026266\n",
      "Iteration 39, loss = 0.21151698\n",
      "Iteration 40, loss = 0.19748645\n",
      "Iteration 38, loss = 0.20579022\n",
      "Iteration 40, loss = 0.19717213\n",
      "Iteration 41, loss = 0.19232109\n",
      "Iteration 39, loss = 0.20635665\n",
      "Iteration 39, loss = 0.19855653\n",
      "Iteration 39, loss = 0.21151698\n",
      "Iteration 39, loss = 0.20196878\n",
      "Iteration 40, loss = 0.21005866\n",
      "Iteration 39, loss = 0.20196878\n",
      "Iteration 41, loss = 0.19232109\n",
      "Iteration 39, loss = 0.20635665\n",
      "Iteration 41, loss = 0.18651257\n",
      "Iteration 42, loss = 0.19742427\n",
      "Iteration 40, loss = 0.20164380\n",
      "Iteration 40, loss = 0.19717213\n",
      "Iteration 40, loss = 0.21005866\n",
      "Iteration 42, loss = 0.19742427\n",
      "Iteration 40, loss = 0.20143208\n",
      "Iteration 41, loss = 0.20768505\n",
      "Iteration 40, loss = 0.20143208\n",
      "Iteration 42, loss = 0.18755419\n",
      "Iteration 40, loss = 0.20164380\n",
      "Iteration 41, loss = 0.18651257\n",
      "Iteration 41, loss = 0.20430040\n",
      "Iteration 43, loss = 0.18649552\n",
      "Iteration 41, loss = 0.20768505\n",
      "Iteration 43, loss = 0.18649552\n",
      "Iteration 42, loss = 0.21161999\n",
      "Iteration 41, loss = 0.20018662\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.20018662\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 0.19234456\n",
      "Iteration 41, loss = 0.20430040\n",
      "Iteration 42, loss = 0.20590218\n",
      "Iteration 42, loss = 0.18755419\n",
      "Iteration 42, loss = 0.21161999\n",
      "Iteration 44, loss = 0.18406166\n",
      "Iteration 43, loss = 0.20626813\n",
      "Iteration 44, loss = 0.18406166\n",
      "Iteration 44, loss = 0.19313251\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 0.19647147\n",
      "Iteration 42, loss = 0.20590218\n",
      "Iteration 43, loss = 0.19234456\n",
      "Iteration 45, loss = 0.18198473\n",
      "Iteration 45, loss = 0.18198473\n",
      "Iteration 43, loss = 0.20626813\n",
      "Iteration 1, loss = 4.72591216\n",
      "Iteration 1, loss = 4.71918294\n",
      "Iteration 44, loss = 0.20984853\n",
      "Iteration 44, loss = 0.19514025\n",
      "Iteration 43, loss = 0.19647147\n",
      "Iteration 46, loss = 0.18403626\n",
      "Iteration 44, loss = 0.20984853\n",
      "Iteration 44, loss = 0.19313251\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 46, loss = 0.18403626\n",
      "Iteration 2, loss = 4.09892343\n",
      "Iteration 2, loss = 4.07306617\n",
      "Iteration 45, loss = 0.20620859\n",
      "Iteration 1, loss = 4.72355338\n",
      "Iteration 45, loss = 0.19751033\n",
      "Iteration 44, loss = 0.19514025\n",
      "Iteration 3, loss = 2.47270647\n",
      "Iteration 45, loss = 0.20620859\n",
      "Iteration 3, loss = 2.36889820\n",
      "Iteration 47, loss = 0.18816436\n",
      "Iteration 47, loss = 0.18816436\n",
      "Iteration 46, loss = 0.18974875\n",
      "Iteration 46, loss = 0.20349678\n",
      "Iteration 4, loss = 0.82450652\n",
      "Iteration 2, loss = 4.09225491\n",
      "Iteration 1, loss = 4.71983433\n",
      "Iteration 45, loss = 0.19751033\n",
      "Iteration 4, loss = 0.78521527\n",
      "Iteration 48, loss = 0.17886672\n",
      "Iteration 46, loss = 0.20349678\n",
      "Iteration 48, loss = 0.17886672\n",
      "Iteration 5, loss = 0.57339639\n",
      "Iteration 47, loss = 0.18624075\n",
      "Iteration 3, loss = 2.37400891\n",
      "Iteration 47, loss = 0.20366721\n",
      "Iteration 49, loss = 0.18144365\n",
      "Iteration 46, loss = 0.18974875\n",
      "Iteration 2, loss = 4.07857727\n",
      "Iteration 5, loss = 0.56787153\n",
      "Iteration 47, loss = 0.20366721\n",
      "Iteration 49, loss = 0.18144365\n",
      "Iteration 48, loss = 0.19205066\n",
      "Iteration 6, loss = 0.50134447\n",
      "Iteration 48, loss = 0.19815555\n",
      "Iteration 4, loss = 0.75412843\n",
      "Iteration 6, loss = 0.50794040\n",
      "Iteration 47, loss = 0.18624075\n",
      "Iteration 50, loss = 0.17915820\n",
      "Iteration 50, loss = 0.17915820\n",
      "Iteration 3, loss = 2.38114554\n",
      "Iteration 49, loss = 0.17879665\n",
      "Iteration 48, loss = 0.19815555\n",
      "Iteration 7, loss = 0.45385145\n",
      "Iteration 5, loss = 0.54458668\n",
      "Iteration 49, loss = 0.19532381\n",
      "Iteration 7, loss = 0.45451780\n",
      "Iteration 48, loss = 0.19205066\n",
      "Iteration 49, loss = 0.19532381\n",
      "Iteration 51, loss = 0.16663232\n",
      "Iteration 8, loss = 0.41918441\n",
      "Iteration 50, loss = 0.17824745\n",
      "Iteration 51, loss = 0.16663232\n",
      "Iteration 4, loss = 0.78069997\n",
      "Iteration 50, loss = 0.20089913\n",
      "Iteration 6, loss = 0.48444183\n",
      "Iteration 8, loss = 0.42630509\n",
      "Iteration 50, loss = 0.20089913\n",
      "Iteration 52, loss = 0.17128207\n",
      "Iteration 49, loss = 0.17879665\n",
      "Iteration 9, loss = 0.39560306\n",
      "Iteration 51, loss = 0.18537570\n",
      "Iteration 5, loss = 0.56910127\n",
      "Iteration 51, loss = 0.20182166\n",
      "Iteration 52, loss = 0.17128207\n",
      "Iteration 7, loss = 0.43760665\n",
      "Iteration 51, loss = 0.20182166\n",
      "Iteration 9, loss = 0.39290651\n",
      "Iteration 50, loss = 0.17824745\n",
      "Iteration 53, loss = 0.17480476\n",
      "Iteration 6, loss = 0.50168889\n",
      "Iteration 52, loss = 0.18573557\n",
      "Iteration 8, loss = 0.40300182\n",
      "Iteration 10, loss = 0.38161235\n",
      "Iteration 52, loss = 0.19829822\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 0.36461117\n",
      "Iteration 52, loss = 0.19829822\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 0.17480476\n",
      "Iteration 54, loss = 0.16956181\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 51, loss = 0.18537570\n",
      "Iteration 7, loss = 0.45019850\n",
      "Iteration 53, loss = 0.17931968\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 0.38259462\n",
      "Iteration 11, loss = 0.35894230\n",
      "Iteration 11, loss = 0.35484364\n",
      "Iteration 54, loss = 0.16956181\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.41903988\n",
      "Iteration 1, loss = 4.71437187\n",
      "Iteration 52, loss = 0.18573557\n",
      "Iteration 10, loss = 0.35961321\n",
      "Iteration 1, loss = 4.51470044\n",
      "Iteration 12, loss = 0.33840266\n",
      "Iteration 12, loss = 0.34033593\n",
      "Iteration 1, loss = 4.48763325\n",
      "Iteration 1, loss = 4.47560905\n",
      "Iteration 9, loss = 0.39890567\n",
      "Iteration 2, loss = 4.04230871\n",
      "Iteration 13, loss = 0.32413703\n",
      "Iteration 53, loss = 0.17931968\n",
      "Iteration 11, loss = 0.35411132\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.47167169\n",
      "Iteration 2, loss = 2.26201924\n",
      "Iteration 13, loss = 0.32212165\n",
      "Iteration 2, loss = 2.14094106\n",
      "Iteration 3, loss = 2.30502643\n",
      "Iteration 2, loss = 2.16617059\n",
      "Iteration 10, loss = 0.37431462\n",
      "Iteration 12, loss = 0.33601143\n",
      "Iteration 14, loss = 0.30642743\n",
      "Iteration 2, loss = 2.10128891\n",
      "Iteration 3, loss = 0.66770287\n",
      "Iteration 3, loss = 0.62381051\n",
      "Iteration 1, loss = 4.47932386\n",
      "Iteration 14, loss = 0.31226904\n",
      "Iteration 3, loss = 0.65797550\n",
      "Iteration 4, loss = 0.78239469\n",
      "Iteration 11, loss = 0.35684626\n",
      "Iteration 15, loss = 0.30452497\n",
      "Iteration 4, loss = 0.51640727\n",
      "Iteration 13, loss = 0.31463977\n",
      "Iteration 15, loss = 0.30116307\n",
      "Iteration 3, loss = 0.65133552\n",
      "Iteration 2, loss = 2.12529468\n",
      "Iteration 4, loss = 0.53951052\n",
      "Iteration 4, loss = 0.54724388\n",
      "Iteration 16, loss = 0.28685722\n",
      "Iteration 5, loss = 0.58195261\n",
      "Iteration 12, loss = 0.34087229\n",
      "Iteration 16, loss = 0.29042776\n",
      "Iteration 5, loss = 0.43746005\n",
      "Iteration 3, loss = 0.66593035\n",
      "Iteration 4, loss = 0.54107089\n",
      "Iteration 14, loss = 0.31467518\n",
      "Iteration 5, loss = 0.46132419\n",
      "Iteration 5, loss = 0.45308572\n",
      "Iteration 17, loss = 0.28321659\n",
      "Iteration 4, loss = 0.53144903\n",
      "Iteration 13, loss = 0.32479394\n",
      "Iteration 6, loss = 0.51622143\n",
      "Iteration 17, loss = 0.28861868\n",
      "Iteration 6, loss = 0.38957594\n",
      "Iteration 18, loss = 0.27167997\n",
      "Iteration 6, loss = 0.40168513\n",
      "Iteration 15, loss = 0.30014349\n",
      "Iteration 6, loss = 0.40956618\n",
      "Iteration 5, loss = 0.45658055\n",
      "Iteration 14, loss = 0.31611375\n",
      "Iteration 5, loss = 0.45868230\n",
      "Iteration 19, loss = 0.27403267\n",
      "Iteration 7, loss = 0.47748829\n",
      "Iteration 18, loss = 0.27903208\n",
      "Iteration 16, loss = 0.28538000\n",
      "Iteration 7, loss = 0.38068432\n",
      "Iteration 7, loss = 0.36571344\n",
      "Iteration 6, loss = 0.40590440\n",
      "Iteration 7, loss = 0.36936372\n",
      "Iteration 6, loss = 0.41171362\n",
      "Iteration 15, loss = 0.30470359\n",
      "Iteration 8, loss = 0.44012117\n",
      "Iteration 19, loss = 0.27481054\n",
      "Iteration 20, loss = 0.26738446\n",
      "Iteration 17, loss = 0.28787663\n",
      "Iteration 8, loss = 0.35944288\n",
      "Iteration 8, loss = 0.33549083\n",
      "Iteration 7, loss = 0.38412334\n",
      "Iteration 8, loss = 0.35427821\n",
      "Iteration 16, loss = 0.29855029\n",
      "Iteration 7, loss = 0.39875679\n",
      "Iteration 9, loss = 0.41201965\n",
      "Iteration 20, loss = 0.26141642\n",
      "Iteration 18, loss = 0.28208334\n",
      "Iteration 21, loss = 0.25990262\n",
      "Iteration 9, loss = 0.31699910\n",
      "Iteration 9, loss = 0.33182488\n",
      "Iteration 9, loss = 0.33637012\n",
      "Iteration 17, loss = 0.29083507\n",
      "Iteration 8, loss = 0.35941677\n",
      "Iteration 8, loss = 0.35300182\n",
      "Iteration 10, loss = 0.39274721\n",
      "Iteration 19, loss = 0.27071029\n",
      "Iteration 22, loss = 0.25605102\n",
      "Iteration 21, loss = 0.25523780\n",
      "Iteration 10, loss = 0.32246173\n",
      "Iteration 10, loss = 0.31548992\n",
      "Iteration 11, loss = 0.37678877\n",
      "Iteration 10, loss = 0.31190763\n",
      "Iteration 9, loss = 0.32434941\n",
      "Iteration 20, loss = 0.26789190\n",
      "Iteration 18, loss = 0.28601931\n",
      "Iteration 9, loss = 0.34451085\n",
      "Iteration 22, loss = 0.25714067\n",
      "Iteration 11, loss = 0.31686759\n",
      "Iteration 12, loss = 0.36324008\n",
      "Iteration 11, loss = 0.29909126\n",
      "Iteration 23, loss = 0.25263699\n",
      "Iteration 21, loss = 0.26199416\n",
      "Iteration 10, loss = 0.32727895\n",
      "Iteration 11, loss = 0.29606066\n",
      "Iteration 19, loss = 0.27904978\n",
      "Iteration 10, loss = 0.32434387\n",
      "Iteration 22, loss = 0.25274553\n",
      "Iteration 12, loss = 0.28832483\n",
      "Iteration 13, loss = 0.35092124\n",
      "Iteration 12, loss = 0.30098355\n",
      "Iteration 23, loss = 0.24797223\n",
      "Iteration 24, loss = 0.24479762\n",
      "Iteration 20, loss = 0.27220053\n",
      "Iteration 11, loss = 0.31879107\n",
      "Iteration 12, loss = 0.28819589\n",
      "Iteration 11, loss = 0.30780088\n",
      "Iteration 23, loss = 0.24344181\n",
      "Iteration 13, loss = 0.27952465\n",
      "Iteration 13, loss = 0.28491209\n",
      "Iteration 25, loss = 0.23911706\n",
      "Iteration 24, loss = 0.24684253\n",
      "Iteration 14, loss = 0.34177973\n",
      "Iteration 21, loss = 0.26936563\n",
      "Iteration 12, loss = 0.30021265\n",
      "Iteration 12, loss = 0.30359350\n",
      "Iteration 13, loss = 0.28128151\n",
      "Iteration 24, loss = 0.24308613\n",
      "Iteration 26, loss = 0.24120495\n",
      "Iteration 15, loss = 0.32260292\n",
      "Iteration 14, loss = 0.26675411\n",
      "Iteration 22, loss = 0.27098885\n",
      "Iteration 14, loss = 0.27338032\n",
      "Iteration 25, loss = 0.24335373\n",
      "Iteration 25, loss = 0.24582805\n",
      "Iteration 13, loss = 0.28392544\n",
      "Iteration 14, loss = 0.27821456\n",
      "Iteration 13, loss = 0.29880352\n",
      "Iteration 27, loss = 0.23027234\n",
      "Iteration 15, loss = 0.26843723\n",
      "Iteration 26, loss = 0.24235281\n",
      "Iteration 16, loss = 0.31517858\n",
      "Iteration 15, loss = 0.26883069\n",
      "Iteration 23, loss = 0.26129063\n",
      "Iteration 16, loss = 0.27377892\n",
      "Iteration 26, loss = 0.24567015\n",
      "Iteration 14, loss = 0.29124869\n",
      "Iteration 15, loss = 0.26505302\n",
      "Iteration 14, loss = 0.27710889\n",
      "Iteration 17, loss = 0.31193846\n",
      "Iteration 27, loss = 0.24460444\n",
      "Iteration 16, loss = 0.27112105\n",
      "Iteration 28, loss = 0.22954514\n",
      "Iteration 24, loss = 0.25631920\n",
      "Iteration 27, loss = 0.23195486\n",
      "Iteration 16, loss = 0.26461881\n",
      "Iteration 15, loss = 0.28331422\n",
      "Iteration 17, loss = 0.25612748\n",
      "Iteration 18, loss = 0.30416303\n",
      "Iteration 15, loss = 0.27337769\n",
      "Iteration 25, loss = 0.25311609\n",
      "Iteration 28, loss = 0.23463215\n",
      "Iteration 29, loss = 0.22012419\n",
      "Iteration 17, loss = 0.25006562\n",
      "Iteration 28, loss = 0.23419167\n",
      "Iteration 16, loss = 0.26937362\n",
      "Iteration 16, loss = 0.28269797\n",
      "Iteration 29, loss = 0.23099675\n",
      "Iteration 26, loss = 0.24012072\n",
      "Iteration 19, loss = 0.29463789\n",
      "Iteration 18, loss = 0.26213939\n",
      "Iteration 17, loss = 0.25078133\n",
      "Iteration 30, loss = 0.22118347\n",
      "Iteration 18, loss = 0.25582311\n",
      "Iteration 27, loss = 0.23734458\n",
      "Iteration 17, loss = 0.26568590\n",
      "Iteration 29, loss = 0.23050909\n",
      "Iteration 20, loss = 0.28965772\n",
      "Iteration 17, loss = 0.27292086\n",
      "Iteration 18, loss = 0.24445686\n",
      "Iteration 30, loss = 0.22181589\n",
      "Iteration 30, loss = 0.22757661\n",
      "Iteration 21, loss = 0.27969983\n",
      "Iteration 19, loss = 0.24155999\n",
      "Iteration 31, loss = 0.21929600\n",
      "Iteration 18, loss = 0.25809280\n",
      "Iteration 19, loss = 0.24584686\n",
      "Iteration 19, loss = 0.23553925\n",
      "Iteration 18, loss = 0.25240047\n",
      "Iteration 28, loss = 0.23113482\n",
      "Iteration 31, loss = 0.22158854\n",
      "Iteration 31, loss = 0.21787913\n",
      "Iteration 22, loss = 0.26839863\n",
      "Iteration 32, loss = 0.21716926\n",
      "Iteration 20, loss = 0.23603740\n",
      "Iteration 19, loss = 0.25507528\n",
      "Iteration 19, loss = 0.26497244\n",
      "Iteration 20, loss = 0.24180264\n",
      "Iteration 29, loss = 0.22558595\n",
      "Iteration 20, loss = 0.23637900\n",
      "Iteration 32, loss = 0.21558452\n",
      "Iteration 32, loss = 0.22241466\n",
      "Iteration 33, loss = 0.21815846\n",
      "Iteration 21, loss = 0.23499632\n",
      "Iteration 23, loss = 0.27103414\n",
      "Iteration 21, loss = 0.23858501\n",
      "Iteration 20, loss = 0.25973334\n",
      "Iteration 20, loss = 0.24873424\n",
      "Iteration 30, loss = 0.22755529\n",
      "Iteration 21, loss = 0.23520222\n",
      "Iteration 33, loss = 0.21494916\n",
      "Iteration 33, loss = 0.21615512\n",
      "Iteration 34, loss = 0.21568883\n",
      "Iteration 22, loss = 0.24000955\n",
      "Iteration 24, loss = 0.26711229\n",
      "Iteration 21, loss = 0.23546970\n",
      "Iteration 22, loss = 0.22987234\n",
      "Iteration 31, loss = 0.22504582\n",
      "Iteration 21, loss = 0.24936671\n",
      "Iteration 34, loss = 0.20766114\n",
      "Iteration 25, loss = 0.25803709\n",
      "Iteration 22, loss = 0.23188428\n",
      "Iteration 23, loss = 0.22556384\n",
      "Iteration 35, loss = 0.21350797\n",
      "Iteration 22, loss = 0.23506742\n",
      "Iteration 34, loss = 0.21784655\n",
      "Iteration 23, loss = 0.23397534\n",
      "Iteration 32, loss = 0.22366725\n",
      "Iteration 22, loss = 0.24275384\n",
      "Iteration 35, loss = 0.20908345\n",
      "Iteration 36, loss = 0.20752933\n",
      "Iteration 26, loss = 0.25733763\n",
      "Iteration 23, loss = 0.23600934\n",
      "Iteration 23, loss = 0.22763077\n",
      "Iteration 24, loss = 0.21985210\n",
      "Iteration 24, loss = 0.21345811\n",
      "Iteration 35, loss = 0.21529171\n",
      "Iteration 27, loss = 0.25466391\n",
      "Iteration 23, loss = 0.23502558\n",
      "Iteration 33, loss = 0.21662274\n",
      "Iteration 36, loss = 0.20631562\n",
      "Iteration 24, loss = 0.22391667\n",
      "Iteration 37, loss = 0.20268402\n",
      "Iteration 25, loss = 0.21457105\n",
      "Iteration 24, loss = 0.23375423\n",
      "Iteration 36, loss = 0.20357608\n",
      "Iteration 25, loss = 0.22601384\n",
      "Iteration 28, loss = 0.24587511\n",
      "Iteration 24, loss = 0.23328454\n",
      "Iteration 34, loss = 0.22364395\n",
      "Iteration 37, loss = 0.20405419\n",
      "Iteration 26, loss = 0.20367720\n",
      "Iteration 38, loss = 0.20349094\n",
      "Iteration 25, loss = 0.21990706\n",
      "Iteration 37, loss = 0.20420512\n",
      "Iteration 25, loss = 0.22031514\n",
      "Iteration 26, loss = 0.21315913\n",
      "Iteration 29, loss = 0.24526872\n",
      "Iteration 35, loss = 0.21700564\n",
      "Iteration 38, loss = 0.19965926\n",
      "Iteration 25, loss = 0.22929276\n",
      "Iteration 26, loss = 0.21635583\n",
      "Iteration 27, loss = 0.21645930\n",
      "Iteration 39, loss = 0.19688692\n",
      "Iteration 38, loss = 0.20124315\n",
      "Iteration 26, loss = 0.21785974\n",
      "Iteration 27, loss = 0.21329487\n",
      "Iteration 30, loss = 0.24279387\n",
      "Iteration 36, loss = 0.22065711\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 0.22960518\n",
      "Iteration 39, loss = 0.19810983\n",
      "Iteration 39, loss = 0.20340958\n",
      "Iteration 40, loss = 0.19441729\n",
      "Iteration 28, loss = 0.20632078\n",
      "Iteration 27, loss = 0.21083822\n",
      "Iteration 28, loss = 0.21081510\n",
      "Iteration 27, loss = 0.20367527\n",
      "Iteration 27, loss = 0.21965128\n",
      "Iteration 31, loss = 0.24054755\n",
      "Iteration 29, loss = 0.19861800\n",
      "Iteration 28, loss = 0.21787442\n",
      "Iteration 40, loss = 0.19797964\n",
      "Iteration 40, loss = 0.20301488\n",
      "Iteration 41, loss = 0.19216205\n",
      "Iteration 1, loss = 4.50473629\n",
      "Iteration 29, loss = 0.20617390\n",
      "Iteration 28, loss = 0.20372463\n",
      "Iteration 28, loss = 0.21585235\n",
      "Iteration 30, loss = 0.20236449\n",
      "Iteration 29, loss = 0.21144418\n",
      "Iteration 32, loss = 0.23619104\n",
      "Iteration 42, loss = 0.19122659\n",
      "Iteration 41, loss = 0.19675518\n",
      "Iteration 29, loss = 0.20248438\n",
      "Iteration 30, loss = 0.20269751\n",
      "Iteration 41, loss = 0.19408814\n",
      "Iteration 2, loss = 2.23594922\n",
      "Iteration 30, loss = 0.20405020\n",
      "Iteration 29, loss = 0.21412421\n",
      "Iteration 31, loss = 0.19426991\n",
      "Iteration 33, loss = 0.23170681\n",
      "Iteration 43, loss = 0.18890389\n",
      "Iteration 30, loss = 0.20905867\n",
      "Iteration 42, loss = 0.19309471\n",
      "Iteration 3, loss = 0.64638094\n",
      "Iteration 42, loss = 0.20171266\n",
      "Iteration 31, loss = 0.19101321\n",
      "Iteration 31, loss = 0.20344984\n",
      "Iteration 30, loss = 0.20680639\n",
      "Iteration 31, loss = 0.20124374\n",
      "Iteration 32, loss = 0.19741027\n",
      "Iteration 43, loss = 0.19049865\n",
      "Iteration 34, loss = 0.22851136\n",
      "Iteration 4, loss = 0.53385019\n",
      "Iteration 44, loss = 0.18595474\n",
      "Iteration 32, loss = 0.18934450\n",
      "Iteration 32, loss = 0.19800380\n",
      "Iteration 43, loss = 0.18997643\n",
      "Iteration 44, loss = 0.18836371\n",
      "Iteration 31, loss = 0.20904013\n",
      "Iteration 32, loss = 0.18909317\n",
      "Iteration 33, loss = 0.19211879\n",
      "Iteration 5, loss = 0.43944084\n",
      "Iteration 35, loss = 0.23258711\n",
      "Iteration 33, loss = 0.18333307\n",
      "Iteration 45, loss = 0.18119212\n",
      "Iteration 45, loss = 0.19109445\n",
      "Iteration 33, loss = 0.19880790\n",
      "Iteration 44, loss = 0.18241260\n",
      "Iteration 32, loss = 0.20290569\n",
      "Iteration 33, loss = 0.19408468\n",
      "Iteration 6, loss = 0.41084514\n",
      "Iteration 34, loss = 0.18279416\n",
      "Iteration 36, loss = 0.23375921\n",
      "Iteration 33, loss = 0.20813110\n",
      "Iteration 34, loss = 0.18834386\n",
      "Iteration 46, loss = 0.18119663\n",
      "Iteration 34, loss = 0.18876737\n",
      "Iteration 46, loss = 0.18738367\n",
      "Iteration 45, loss = 0.18953105\n",
      "Iteration 34, loss = 0.19466011\n",
      "Iteration 35, loss = 0.18539300\n",
      "Iteration 34, loss = 0.20124632\n",
      "Iteration 47, loss = 0.18585508\n",
      "Iteration 7, loss = 0.37257625\n",
      "Iteration 46, loss = 0.18452301\n",
      "Iteration 47, loss = 0.17953499\n",
      "Iteration 35, loss = 0.18666408\n",
      "Iteration 37, loss = 0.22576593\n",
      "Iteration 35, loss = 0.18266346\n",
      "Iteration 35, loss = 0.18796236\n",
      "Iteration 36, loss = 0.17945178\n",
      "Iteration 35, loss = 0.19736319\n",
      "Iteration 48, loss = 0.18000879\n",
      "Iteration 47, loss = 0.18372280\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 48, loss = 0.18125617\n",
      "Iteration 8, loss = 0.34643879\n",
      "Iteration 36, loss = 0.18568583\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 0.18899534\n",
      "Iteration 38, loss = 0.22580687\n",
      "Iteration 36, loss = 0.18648440\n",
      "Iteration 37, loss = 0.18692232\n",
      "Iteration 36, loss = 0.19951649\n",
      "Iteration 49, loss = 0.17715867\n",
      "Iteration 9, loss = 0.32765239\n",
      "Iteration 49, loss = 0.18105039\n",
      "Iteration 37, loss = 0.18635665\n",
      "Iteration 37, loss = 0.18002573\n",
      "Iteration 39, loss = 0.21782947\n",
      "Iteration 1, loss = 4.49085291\n",
      "Iteration 10, loss = 0.31133507\n",
      "Iteration 1, loss = 4.50532917\n",
      "Iteration 38, loss = 0.18210621\n",
      "Iteration 50, loss = 0.17696112\n",
      "Iteration 37, loss = 0.19766876\n",
      "Iteration 50, loss = 0.17513072\n",
      "Iteration 38, loss = 0.18708126\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 0.22146822\n",
      "Iteration 38, loss = 0.17693835\n",
      "Iteration 11, loss = 0.30777617\n",
      "Iteration 2, loss = 2.13623681\n",
      "Iteration 2, loss = 2.24451166\n",
      "Iteration 51, loss = 0.17889646\n",
      "Iteration 51, loss = 0.17415606\n",
      "Iteration 41, loss = 0.20790265\n",
      "Iteration 39, loss = 0.17810818\n",
      "Iteration 38, loss = 0.19263073\n",
      "Iteration 52, loss = 0.17700309\n",
      "Iteration 3, loss = 0.64144094\n",
      "Iteration 1, loss = 4.51571515\n",
      "Iteration 39, loss = 0.18648008\n",
      "Iteration 12, loss = 0.28854409\n",
      "Iteration 52, loss = 0.17656335\n",
      "Iteration 3, loss = 0.64512615\n",
      "Iteration 42, loss = 0.21024333\n",
      "Iteration 40, loss = 0.17009550\n",
      "Iteration 39, loss = 0.19609258\n",
      "Iteration 40, loss = 0.18467988\n",
      "Iteration 53, loss = 0.17350343\n",
      "Iteration 13, loss = 0.28991879\n",
      "Iteration 4, loss = 0.53826184\n",
      "Iteration 2, loss = 2.24979265\n",
      "Iteration 4, loss = 0.52384179\n",
      "Iteration 53, loss = 0.17580413\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 0.21784093\n",
      "Iteration 41, loss = 0.18084449\n",
      "Iteration 40, loss = 0.19212324\n",
      "Iteration 41, loss = 0.17561562\n",
      "Iteration 14, loss = 0.27732658\n",
      "Iteration 3, loss = 0.66708745\n",
      "Iteration 5, loss = 0.44268809\n",
      "Iteration 54, loss = 0.17369741\n",
      "Iteration 5, loss = 0.43898546\n",
      "Iteration 1, loss = 4.47972460\n",
      "Iteration 44, loss = 0.21598666\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 0.16331569\n",
      "Iteration 42, loss = 0.17606013\n",
      "Iteration 41, loss = 0.17870240\n",
      "Iteration 4, loss = 0.53707430\n",
      "Iteration 15, loss = 0.27166176\n",
      "Iteration 55, loss = 0.17094896\n",
      "Iteration 6, loss = 0.38560956\n",
      "Iteration 6, loss = 0.41005192\n",
      "Iteration 43, loss = 0.16853024\n",
      "Iteration 2, loss = 2.16231181\n",
      "Iteration 42, loss = 0.18368510\n",
      "Iteration 43, loss = 0.16264789\n",
      "Iteration 16, loss = 0.27005961\n",
      "Iteration 56, loss = 0.17218783\n",
      "Iteration 7, loss = 0.37914349\n",
      "Iteration 7, loss = 0.36421572\n",
      "Iteration 5, loss = 0.45378916\n",
      "Iteration 44, loss = 0.16548469\n",
      "Iteration 1, loss = 4.49121149\n",
      "Iteration 17, loss = 0.27172300\n",
      "Iteration 44, loss = 0.16776912\n",
      "Iteration 3, loss = 0.66313719\n",
      "Iteration 43, loss = 0.18386067\n",
      "Iteration 8, loss = 0.35574112\n",
      "Iteration 57, loss = 0.16246341\n",
      "Iteration 8, loss = 0.34399431\n",
      "Iteration 6, loss = 0.40897396\n",
      "Iteration 45, loss = 0.17061940\n",
      "Iteration 2, loss = 2.16584091\n",
      "Iteration 4, loss = 0.53906894\n",
      "Iteration 18, loss = 0.25499828\n",
      "Iteration 45, loss = 0.16288514\n",
      "Iteration 44, loss = 0.18070194\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 0.33612269\n",
      "Iteration 58, loss = 0.17130981\n",
      "Iteration 9, loss = 0.33647273\n",
      "Iteration 46, loss = 0.17182087\n",
      "Iteration 3, loss = 0.64916894\n",
      "Iteration 5, loss = 0.45887465\n",
      "Iteration 7, loss = 0.37987824\n",
      "Iteration 19, loss = 0.24349033\n",
      "Iteration 10, loss = 0.31977833\n",
      "Iteration 46, loss = 0.16248275\n",
      "Iteration 59, loss = 0.16573349\n",
      "Iteration 10, loss = 0.31637578\n",
      "Iteration 47, loss = 0.16936085\n",
      "Iteration 6, loss = 0.41719225\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.35291972\n",
      "Iteration 4, loss = 0.51305422\n",
      "Iteration 1, loss = 4.52657980\n",
      "Iteration 20, loss = 0.23721911\n",
      "Iteration 11, loss = 0.30609088\n",
      "Iteration 47, loss = 0.16944046\n",
      "Iteration 60, loss = 0.15742282\n",
      "Iteration 11, loss = 0.31018808\n",
      "Iteration 2, loss = 2.27897811\n",
      "Iteration 9, loss = 0.34746994\n",
      "Iteration 7, loss = 0.38455806\n",
      "Iteration 21, loss = 0.23283388\n",
      "Iteration 5, loss = 0.44706950\n",
      "Iteration 1, loss = 4.51405478\n",
      "Iteration 12, loss = 0.30459887\n",
      "Iteration 61, loss = 0.16035707\n",
      "Iteration 3, loss = 0.65647675\n",
      "Iteration 12, loss = 0.30047478\n",
      "Iteration 48, loss = 0.16141127\n",
      "Iteration 13, loss = 0.28733190\n",
      "Iteration 10, loss = 0.32306454\n",
      "Iteration 8, loss = 0.37116036\n",
      "Iteration 22, loss = 0.22773680\n",
      "Iteration 13, loss = 0.28652122\n",
      "Iteration 49, loss = 0.15516131\n",
      "Iteration 4, loss = 0.53473007\n",
      "Iteration 6, loss = 0.40655527\n",
      "Iteration 62, loss = 0.15987926\n",
      "Iteration 2, loss = 2.26208281\n",
      "Iteration 14, loss = 0.28126278\n",
      "Iteration 9, loss = 0.35606957\n",
      "Iteration 11, loss = 0.30330959\n",
      "Iteration 23, loss = 0.22091432\n",
      "Iteration 7, loss = 0.37734168\n",
      "Iteration 5, loss = 0.45630713\n",
      "Iteration 50, loss = 0.15544471\n",
      "Iteration 63, loss = 0.15900158\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 0.27941066\n",
      "Iteration 3, loss = 0.64064457\n",
      "Iteration 15, loss = 0.27581489\n",
      "Iteration 10, loss = 0.32989679\n",
      "Iteration 24, loss = 0.21660394\n",
      "Iteration 6, loss = 0.39881603\n",
      "Iteration 12, loss = 0.29740242\n",
      "Iteration 51, loss = 0.15646192\n",
      "Iteration 8, loss = 0.34792696\n",
      "Iteration 4, loss = 0.52203694\n",
      "Iteration 15, loss = 0.27746151\n",
      "Iteration 1, loss = 4.48134984\n",
      "Iteration 25, loss = 0.22617577\n",
      "Iteration 11, loss = 0.32012486\n",
      "Iteration 16, loss = 0.26857788\n",
      "Iteration 13, loss = 0.28520662\n",
      "Iteration 7, loss = 0.37467071\n",
      "Iteration 52, loss = 0.14720788\n",
      "Iteration 5, loss = 0.43631147\n",
      "Iteration 2, loss = 2.10379075\n",
      "Iteration 9, loss = 0.32657791\n",
      "Iteration 16, loss = 0.27339505\n",
      "Iteration 17, loss = 0.26078108\n",
      "Iteration 26, loss = 0.21917973\n",
      "Iteration 12, loss = 0.30298846\n",
      "Iteration 14, loss = 0.29011607\n",
      "Iteration 8, loss = 0.35083159\n",
      "Iteration 6, loss = 0.39984870\n",
      "Iteration 3, loss = 0.64626243\n",
      "Iteration 53, loss = 0.14943139\n",
      "Iteration 18, loss = 0.25407503\n",
      "Iteration 10, loss = 0.31486991\n",
      "Iteration 17, loss = 0.25846131\n",
      "Iteration 13, loss = 0.29913927\n",
      "Iteration 27, loss = 0.21028933\n",
      "Iteration 7, loss = 0.36555755\n",
      "Iteration 15, loss = 0.27220128\n",
      "Iteration 54, loss = 0.15618619\n",
      "Iteration 4, loss = 0.54434001\n",
      "Iteration 9, loss = 0.33122957\n",
      "Iteration 11, loss = 0.30269358\n",
      "Iteration 18, loss = 0.24809590\n",
      "Iteration 19, loss = 0.24514551\n",
      "Iteration 14, loss = 0.29640924\n",
      "Iteration 28, loss = 0.20728804\n",
      "Iteration 8, loss = 0.34067766\n",
      "Iteration 55, loss = 0.14814322\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 0.45199705\n",
      "Iteration 19, loss = 0.24912226\n",
      "Iteration 12, loss = 0.29128995\n",
      "Iteration 16, loss = 0.27214828\n",
      "Iteration 10, loss = 0.32741375\n",
      "Iteration 20, loss = 0.24723273\n",
      "Iteration 15, loss = 0.28349112\n",
      "Iteration 29, loss = 0.21097043\n",
      "Iteration 9, loss = 0.33033117\n",
      "Iteration 6, loss = 0.41349658\n",
      "Iteration 20, loss = 0.23695346\n",
      "Iteration 17, loss = 0.26432563\n",
      "Iteration 13, loss = 0.27856263\n",
      "Iteration 1, loss = 4.49749597\n",
      "Iteration 11, loss = 0.30563480\n",
      "Iteration 21, loss = 0.23169016\n",
      "Iteration 16, loss = 0.28300227\n",
      "Iteration 10, loss = 0.31331119\n",
      "Iteration 30, loss = 0.19599469\n",
      "Iteration 7, loss = 0.37720664\n",
      "Iteration 22, loss = 0.23457172\n",
      "Iteration 18, loss = 0.26595927\n",
      "Iteration 14, loss = 0.28177682\n",
      "Iteration 21, loss = 0.23290952\n",
      "Iteration 12, loss = 0.29195737\n",
      "Iteration 2, loss = 2.24052202\n",
      "Iteration 8, loss = 0.35290123\n",
      "Iteration 17, loss = 0.26533693\n",
      "Iteration 31, loss = 0.19986952\n",
      "Iteration 11, loss = 0.29659293\n",
      "Iteration 19, loss = 0.24807087\n",
      "Iteration 15, loss = 0.25979225\n",
      "Iteration 9, loss = 0.33524293\n",
      "Iteration 22, loss = 0.23492389\n",
      "Iteration 3, loss = 0.67480779\n",
      "Iteration 23, loss = 0.23523184\n",
      "Iteration 13, loss = 0.29134219\n",
      "Iteration 18, loss = 0.26661802\n",
      "Iteration 32, loss = 0.18912061\n",
      "Iteration 12, loss = 0.28950509\n",
      "Iteration 20, loss = 0.25319901\n",
      "Iteration 16, loss = 0.26642630\n",
      "Iteration 23, loss = 0.22274004\n",
      "Iteration 33, loss = 0.18998648\n",
      "Iteration 10, loss = 0.32601705\n",
      "Iteration 4, loss = 0.53493321\n",
      "Iteration 13, loss = 0.28657584\n",
      "Iteration 24, loss = 0.22559913\n",
      "Iteration 19, loss = 0.25947451\n",
      "Iteration 14, loss = 0.26662059\n",
      "Iteration 17, loss = 0.24940575\n",
      "Iteration 24, loss = 0.23021939\n",
      "Iteration 20, loss = 0.25614194\n",
      "Iteration 21, loss = 0.25441523\n",
      "Iteration 34, loss = 0.19172593\n",
      "Iteration 5, loss = 0.47301578\n",
      "Iteration 25, loss = 0.22138843\n",
      "Iteration 11, loss = 0.31167959\n",
      "Iteration 14, loss = 0.27895713\n",
      "Iteration 18, loss = 0.24332934\n",
      "Iteration 22, loss = 0.23613265\n",
      "Iteration 15, loss = 0.27034760\n",
      "Iteration 25, loss = 0.22341957\n",
      "Iteration 21, loss = 0.24763482\n",
      "Iteration 12, loss = 0.30038166\n",
      "Iteration 26, loss = 0.22028453\n",
      "Iteration 35, loss = 0.17936609\n",
      "Iteration 6, loss = 0.42191598\n",
      "Iteration 19, loss = 0.23852514\n",
      "Iteration 15, loss = 0.27227267\n",
      "Iteration 23, loss = 0.22908400\n",
      "Iteration 16, loss = 0.26427062\n",
      "Iteration 22, loss = 0.24080155\n",
      "Iteration 26, loss = 0.22134059\n",
      "Iteration 13, loss = 0.29094673\n",
      "Iteration 27, loss = 0.22111793\n",
      "Iteration 36, loss = 0.18315773\n",
      "Iteration 16, loss = 0.26176783\n",
      "Iteration 7, loss = 0.39303395\n",
      "Iteration 23, loss = 0.23415387\n",
      "Iteration 24, loss = 0.22347426\n",
      "Iteration 17, loss = 0.26317312\n",
      "Iteration 20, loss = 0.23514106\n",
      "Iteration 27, loss = 0.21684644\n",
      "Iteration 14, loss = 0.30421473\n",
      "Iteration 37, loss = 0.18871735\n",
      "Iteration 28, loss = 0.20673358\n",
      "Iteration 17, loss = 0.25335081\n",
      "Iteration 8, loss = 0.36873782\n",
      "Iteration 24, loss = 0.23087735\n",
      "Iteration 21, loss = 0.23428404\n",
      "Iteration 25, loss = 0.21809988\n",
      "Iteration 18, loss = 0.25206923\n",
      "Iteration 28, loss = 0.21253725\n",
      "Iteration 29, loss = 0.20848502\n",
      "Iteration 15, loss = 0.28706635\n",
      "Iteration 38, loss = 0.17305327\n",
      "Iteration 18, loss = 0.25276436\n",
      "Iteration 22, loss = 0.23312675\n",
      "Iteration 19, loss = 0.24152375\n",
      "Iteration 9, loss = 0.35152909\n",
      "Iteration 29, loss = 0.20252626\n",
      "Iteration 26, loss = 0.22581506\n",
      "Iteration 25, loss = 0.22799623\n",
      "Iteration 16, loss = 0.26645670\n",
      "Iteration 30, loss = 0.19778624\n",
      "Iteration 39, loss = 0.17595275\n",
      "Iteration 19, loss = 0.24916990\n",
      "Iteration 20, loss = 0.23765196\n",
      "Iteration 27, loss = 0.21313851\n",
      "Iteration 17, loss = 0.25857205\n",
      "Iteration 26, loss = 0.23858432\n",
      "Iteration 10, loss = 0.33746347\n",
      "Iteration 23, loss = 0.22820423\n",
      "Iteration 30, loss = 0.20602095\n",
      "Iteration 40, loss = 0.17222884\n",
      "Iteration 20, loss = 0.24521466\n",
      "Iteration 28, loss = 0.20987472\n",
      "Iteration 31, loss = 0.19394978\n",
      "Iteration 21, loss = 0.23120860\n",
      "Iteration 27, loss = 0.23669774\n",
      "Iteration 18, loss = 0.25871546\n",
      "Iteration 11, loss = 0.32929498\n",
      "Iteration 24, loss = 0.22967142\n",
      "Iteration 31, loss = 0.20457767\n",
      "Iteration 29, loss = 0.21395913\n",
      "Iteration 21, loss = 0.25129747\n",
      "Iteration 41, loss = 0.17533250\n",
      "Iteration 32, loss = 0.19649980\n",
      "Iteration 22, loss = 0.22588571\n",
      "Iteration 12, loss = 0.32294291\n",
      "Iteration 28, loss = 0.21705638\n",
      "Iteration 30, loss = 0.20402228\n",
      "Iteration 19, loss = 0.24632766\n",
      "Iteration 13, loss = 0.29809663\n",
      "Iteration 33, loss = 0.19361175\n",
      "Iteration 25, loss = 0.21822612\n",
      "Iteration 32, loss = 0.20115694\n",
      "Iteration 42, loss = 0.17191958\n",
      "Iteration 22, loss = 0.24057437\n",
      "Iteration 23, loss = 0.22439902\n",
      "Iteration 31, loss = 0.20216150\n",
      "Iteration 20, loss = 0.25164491\n",
      "Iteration 34, loss = 0.18524952\n",
      "Iteration 33, loss = 0.19971510\n",
      "Iteration 29, loss = 0.22489503\n",
      "Iteration 23, loss = 0.22751537\n",
      "Iteration 43, loss = 0.15892007\n",
      "Iteration 14, loss = 0.29387254\n",
      "Iteration 24, loss = 0.22248985\n",
      "Iteration 26, loss = 0.22368411\n",
      "Iteration 34, loss = 0.19331092\n",
      "Iteration 24, loss = 0.21860586\n",
      "Iteration 35, loss = 0.19323656\n",
      "Iteration 32, loss = 0.19832725\n",
      "Iteration 21, loss = 0.23338259\n",
      "Iteration 30, loss = 0.21407217\n",
      "Iteration 27, loss = 0.20836588\n",
      "Iteration 44, loss = 0.16957266\n",
      "Iteration 25, loss = 0.21958538\n",
      "Iteration 28, loss = 0.20651810\n",
      "Iteration 15, loss = 0.28262294\n",
      "Iteration 33, loss = 0.19144396\n",
      "Iteration 35, loss = 0.19027124\n",
      "Iteration 36, loss = 0.19064809\n",
      "Iteration 25, loss = 0.21497085\n",
      "Iteration 22, loss = 0.23173427\n",
      "Iteration 31, loss = 0.21663761\n",
      "Iteration 45, loss = 0.17059754\n",
      "Iteration 26, loss = 0.21374293\n",
      "Iteration 29, loss = 0.19129629\n",
      "Iteration 36, loss = 0.19030568\n",
      "Iteration 16, loss = 0.28361019\n",
      "Iteration 34, loss = 0.19132640\n",
      "Iteration 37, loss = 0.18597678\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.23881355\n",
      "Iteration 26, loss = 0.21290160\n",
      "Iteration 46, loss = 0.16173168\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 0.21156208\n",
      "Iteration 27, loss = 0.21266212\n",
      "Iteration 30, loss = 0.20038291\n",
      "Iteration 17, loss = 0.26754256\n",
      "Iteration 37, loss = 0.17784189\n",
      "Iteration 24, loss = 0.23022060\n",
      "Iteration 28, loss = 0.20547814\n",
      "Iteration 1, loss = 4.71726225\n",
      "Iteration 35, loss = 0.18932032\n",
      "Iteration 33, loss = 0.21341003\n",
      "Iteration 31, loss = 0.18893971\n",
      "Iteration 27, loss = 0.21509896\n",
      "Iteration 18, loss = 0.26819533\n",
      "Iteration 1, loss = 4.73102903\n",
      "Iteration 38, loss = 0.18240339\n",
      "Iteration 25, loss = 0.21966720\n",
      "Iteration 2, loss = 4.08490227\n",
      "Iteration 28, loss = 0.21818026\n",
      "Iteration 32, loss = 0.18928565\n",
      "Iteration 36, loss = 0.18843909\n",
      "Iteration 29, loss = 0.20461646\n",
      "Iteration 19, loss = 0.25382872\n",
      "Iteration 34, loss = 0.20774673\n",
      "Iteration 39, loss = 0.17919295\n",
      "Iteration 2, loss = 4.14213955\n",
      "Iteration 37, loss = 0.18636206\n",
      "Iteration 30, loss = 0.20354510\n",
      "Iteration 26, loss = 0.21341236\n",
      "Iteration 3, loss = 2.42966022\n",
      "Iteration 29, loss = 0.21133723\n",
      "Iteration 33, loss = 0.18781895\n",
      "Iteration 35, loss = 0.20207074\n",
      "Iteration 20, loss = 0.26313963\n",
      "Iteration 40, loss = 0.17626165\n",
      "Iteration 38, loss = 0.19309406\n",
      "Iteration 4, loss = 0.78885147\n",
      "Iteration 27, loss = 0.21882680\n",
      "Iteration 34, loss = 0.18540143\n",
      "Iteration 3, loss = 2.52933315\n",
      "Iteration 30, loss = 0.19732849\n",
      "Iteration 31, loss = 0.19962313\n",
      "Iteration 21, loss = 0.24589933\n",
      "Iteration 36, loss = 0.20212740\n",
      "Iteration 41, loss = 0.17799830\n",
      "Iteration 5, loss = 0.55673055\n",
      "Iteration 39, loss = 0.17947607\n",
      "Iteration 4, loss = 0.82667357\n",
      "Iteration 35, loss = 0.18679324\n",
      "Iteration 28, loss = 0.21496651\n",
      "Iteration 31, loss = 0.20481708\n",
      "Iteration 32, loss = 0.19613909\n",
      "Iteration 22, loss = 0.24936654\n",
      "Iteration 6, loss = 0.48421620\n",
      "Iteration 37, loss = 0.19627030\n",
      "Iteration 7, loss = 0.44563203\n",
      "Iteration 23, loss = 0.24080569\n",
      "Iteration 40, loss = 0.17339388\n",
      "Iteration 42, loss = 0.17303583\n",
      "Iteration 32, loss = 0.20492740\n",
      "Iteration 5, loss = 0.56779168\n",
      "Iteration 29, loss = 0.20388206\n",
      "Iteration 36, loss = 0.17983801\n",
      "Iteration 8, loss = 0.41618607\n",
      "Iteration 33, loss = 0.19324432\n",
      "Iteration 38, loss = 0.18943650\n",
      "Iteration 24, loss = 0.23540360\n",
      "Iteration 41, loss = 0.18293928\n",
      "Iteration 33, loss = 0.20033776\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 0.16815822\n",
      "Iteration 37, loss = 0.18332391\n",
      "Iteration 6, loss = 0.50588800\n",
      "Iteration 9, loss = 0.38699553\n",
      "Iteration 30, loss = 0.20309666\n",
      "Iteration 25, loss = 0.22954813\n",
      "Iteration 34, loss = 0.19389521\n",
      "Iteration 42, loss = 0.17007675\n",
      "Iteration 44, loss = 0.16457312\n",
      "Iteration 39, loss = 0.18684760\n",
      "Iteration 38, loss = 0.18680024\n",
      "Iteration 10, loss = 0.37657858\n",
      "Iteration 7, loss = 0.45918522\n",
      "Iteration 31, loss = 0.20331734\n",
      "Iteration 43, loss = 0.17109305\n",
      "Iteration 1, loss = 4.72016925\n",
      "Iteration 35, loss = 0.19089203\n",
      "Iteration 45, loss = 0.16667475\n",
      "Iteration 26, loss = 0.23241179\n",
      "Iteration 40, loss = 0.19258070\n",
      "Iteration 39, loss = 0.17879498\n",
      "Iteration 11, loss = 0.35338966\n",
      "Iteration 32, loss = 0.20252451\n",
      "Iteration 8, loss = 0.42881994\n",
      "Iteration 46, loss = 0.16392037\n",
      "Iteration 36, loss = 0.19259155\n",
      "Iteration 2, loss = 4.07870718\n",
      "Iteration 44, loss = 0.16976562\n",
      "Iteration 40, loss = 0.17630327\n",
      "Iteration 41, loss = 0.17875465\n",
      "Iteration 12, loss = 0.33930190\n",
      "Iteration 27, loss = 0.23112448\n",
      "Iteration 13, loss = 0.32066502\n",
      "Iteration 33, loss = 0.20277426\n",
      "Iteration 9, loss = 0.39211335\n",
      "Iteration 47, loss = 0.16217856\n",
      "Iteration 14, loss = 0.30885492\n",
      "Iteration 3, loss = 2.34845208\n",
      "Iteration 37, loss = 0.18637664\n",
      "Iteration 41, loss = 0.16897120\n",
      "Iteration 42, loss = 0.18735611\n",
      "Iteration 28, loss = 0.23426719\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 45, loss = 0.16841632\n",
      "Iteration 10, loss = 0.38119501\n",
      "Iteration 34, loss = 0.19480987\n",
      "Iteration 48, loss = 0.16126225\n",
      "Iteration 15, loss = 0.30380565\n",
      "Iteration 43, loss = 0.18648887\n",
      "Iteration 42, loss = 0.17022905\n",
      "Iteration 38, loss = 0.17698090\n",
      "Iteration 4, loss = 0.76651758\n",
      "Iteration 46, loss = 0.17365701\n",
      "Iteration 11, loss = 0.36075039\n",
      "Iteration 35, loss = 0.18392567\n",
      "Iteration 49, loss = 0.15804463\n",
      "Iteration 1, loss = 4.72955721\n",
      "Iteration 16, loss = 0.29167538\n",
      "Iteration 44, loss = 0.18272374\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 0.17389515\n",
      "Iteration 43, loss = 0.16556908\n",
      "Iteration 5, loss = 0.55547934\n",
      "Iteration 47, loss = 0.17324029\n",
      "Iteration 12, loss = 0.34233555\n",
      "Iteration 36, loss = 0.18149056\n",
      "Iteration 17, loss = 0.28004046\n",
      "Iteration 50, loss = 0.16071906\n",
      "Iteration 2, loss = 4.11270000\n",
      "Iteration 40, loss = 0.17395248\n",
      "Iteration 44, loss = 0.16338917\n",
      "Iteration 37, loss = 0.18052037\n",
      "Iteration 6, loss = 0.48865347\n",
      "Iteration 48, loss = 0.16304922\n",
      "Iteration 1, loss = 4.71566313\n",
      "Iteration 13, loss = 0.32514265\n",
      "Iteration 18, loss = 0.27818963\n",
      "Iteration 51, loss = 0.15392619\n",
      "Iteration 3, loss = 2.42558305\n",
      "Iteration 41, loss = 0.16446503\n",
      "Iteration 45, loss = 0.16500824\n",
      "Iteration 7, loss = 0.44484716\n",
      "Iteration 38, loss = 0.18778610\n",
      "Iteration 2, loss = 4.05702647\n",
      "Iteration 14, loss = 0.31296719\n",
      "Iteration 49, loss = 0.16206745\n",
      "Iteration 19, loss = 0.26918879\n",
      "Iteration 52, loss = 0.15324156\n",
      "Iteration 8, loss = 0.40966473\n",
      "Iteration 42, loss = 0.16809501\n",
      "Iteration 4, loss = 0.77632700\n",
      "Iteration 46, loss = 0.16641875\n",
      "Iteration 39, loss = 0.18482019\n",
      "Iteration 50, loss = 0.17160960\n",
      "Iteration 15, loss = 0.30469017\n",
      "Iteration 53, loss = 0.15270472\n",
      "Iteration 9, loss = 0.39199376\n",
      "Iteration 3, loss = 2.37656490\n",
      "Iteration 47, loss = 0.15988584\n",
      "Iteration 20, loss = 0.26677021\n",
      "Iteration 43, loss = 0.17465363\n",
      "Iteration 5, loss = 0.55486078\n",
      "Iteration 40, loss = 0.17174414\n",
      "Iteration 16, loss = 0.29757084\n",
      "Iteration 51, loss = 0.16413814\n",
      "Iteration 54, loss = 0.15029695\n",
      "Iteration 10, loss = 0.36346602\n",
      "Iteration 48, loss = 0.15740836\n",
      "Iteration 4, loss = 0.78805926\n",
      "Iteration 21, loss = 0.25057832\n",
      "Iteration 44, loss = 0.16554671\n",
      "Iteration 6, loss = 0.49199993\n",
      "Iteration 41, loss = 0.17190916\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 17, loss = 0.29041830\n",
      "Iteration 55, loss = 0.15299223\n",
      "Iteration 52, loss = 0.15761195\n",
      "Iteration 22, loss = 0.24824572\n",
      "Iteration 11, loss = 0.34644808\n",
      "Iteration 5, loss = 0.57398514\n",
      "Iteration 49, loss = 0.15248708\n",
      "Iteration 7, loss = 0.45372856\n",
      "Iteration 42, loss = 0.17192983\n",
      "Iteration 18, loss = 0.27886793\n",
      "Iteration 23, loss = 0.25040257\n",
      "Iteration 56, loss = 0.14213763\n",
      "Iteration 1, loss = 4.73252777\n",
      "Iteration 50, loss = 0.14567817\n",
      "Iteration 6, loss = 0.51492038\n",
      "Iteration 53, loss = 0.15315726\n",
      "Iteration 8, loss = 0.41384606\n",
      "Iteration 12, loss = 0.32828105\n",
      "Iteration 43, loss = 0.17258164\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 0.27342941\n",
      "Iteration 24, loss = 0.24307037\n",
      "Iteration 51, loss = 0.15112577\n",
      "Iteration 57, loss = 0.14771196\n",
      "Iteration 2, loss = 4.13019034\n",
      "Iteration 9, loss = 0.39084516\n",
      "Iteration 13, loss = 0.33028850\n",
      "Iteration 54, loss = 0.16170324\n",
      "Iteration 25, loss = 0.23190930\n",
      "Iteration 20, loss = 0.26899593\n",
      "Iteration 7, loss = 0.46898526\n",
      "Iteration 14, loss = 0.32108350\n",
      "Iteration 52, loss = 0.13989581\n",
      "Iteration 3, loss = 2.49034575\n",
      "Iteration 58, loss = 0.14373767\n",
      "Iteration 1, loss = 4.70017760\n",
      "Iteration 10, loss = 0.37190866\n",
      "Iteration 15, loss = 0.30523472\n",
      "Iteration 55, loss = 0.14824818\n",
      "Iteration 21, loss = 0.26140010\n",
      "Iteration 8, loss = 0.43444344\n",
      "Iteration 26, loss = 0.23477219\n",
      "Iteration 53, loss = 0.14230015\n",
      "Iteration 59, loss = 0.14206607\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.79404853\n",
      "Iteration 2, loss = 4.00599373\n",
      "Iteration 22, loss = 0.26005841\n",
      "Iteration 11, loss = 0.35823477\n",
      "Iteration 16, loss = 0.29081020\n",
      "Iteration 9, loss = 0.41140709\n",
      "Iteration 27, loss = 0.23686937\n",
      "Iteration 56, loss = 0.14810361\n",
      "Iteration 54, loss = 0.14263936\n",
      "Iteration 5, loss = 0.54516387\n",
      "Iteration 23, loss = 0.25313611\n",
      "Iteration 3, loss = 2.25209186\n",
      "Iteration 12, loss = 0.34392833\n",
      "Iteration 28, loss = 0.23265975\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 0.39117188\n",
      "Iteration 17, loss = 0.29264508\n",
      "Iteration 6, loss = 0.48131838\n",
      "Iteration 1, loss = 4.72081430\n",
      "Iteration 57, loss = 0.15772158\n",
      "Iteration 55, loss = 0.14100955\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 0.25112005\n",
      "Iteration 11, loss = 0.37878884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:  6.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.73923591\n",
      "Iteration 18, loss = 0.28017787\n",
      "Iteration 13, loss = 0.33148251\n",
      "Iteration 1, loss = 4.71921851\n",
      "Iteration 7, loss = 0.44600772\n",
      "Iteration 58, loss = 0.14175745\n",
      "Iteration 2, loss = 4.07996012\n",
      "Iteration 25, loss = 0.24312929\n",
      "Iteration 1, loss = 4.71760443\n",
      "Iteration 5, loss = 0.54853694\n",
      "Iteration 12, loss = 0.35989406\n",
      "Iteration 19, loss = 0.27736052\n",
      "Iteration 8, loss = 0.40882762\n",
      "Iteration 14, loss = 0.31785132\n",
      "Iteration 2, loss = 4.07091718\n",
      "Iteration 59, loss = 0.14748212\n",
      "Iteration 9, loss = 0.38206228\n",
      "Iteration 26, loss = 0.23850983\n",
      "Iteration 3, loss = 2.37485127\n",
      "Iteration 2, loss = 4.05373787\n",
      "Iteration 6, loss = 0.49510493\n",
      "Iteration 13, loss = 0.34580127\n",
      "Iteration 20, loss = 0.26455557\n",
      "Iteration 10, loss = 0.36660907\n",
      "Iteration 15, loss = 0.30718258\n",
      "Iteration 3, loss = 2.37323723\n",
      "Iteration 60, loss = 0.13974162\n",
      "Iteration 27, loss = 0.23686268\n",
      "Iteration 4, loss = 0.75947800\n",
      "Iteration 3, loss = 2.29628508\n",
      "Iteration 14, loss = 0.33117266\n",
      "Iteration 16, loss = 0.30262906\n",
      "Iteration 21, loss = 0.26178433\n",
      "Iteration 11, loss = 0.35586968\n",
      "Iteration 7, loss = 0.44082014\n",
      "Iteration 4, loss = 0.79574904\n",
      "Iteration 61, loss = 0.13681958\n",
      "Iteration 28, loss = 0.23866472\n",
      "Iteration 4, loss = 0.76295044\n",
      "Iteration 5, loss = 0.54911080\n",
      "Iteration 15, loss = 0.32193349\n",
      "Iteration 17, loss = 0.29312078\n",
      "Iteration 22, loss = 0.26844615\n",
      "Iteration 12, loss = 0.32849960\n",
      "Iteration 8, loss = 0.41461904\n",
      "Iteration 5, loss = 0.57345318\n",
      "Iteration 23, loss = 0.25498218\n",
      "Iteration 5, loss = 0.56921438\n",
      "Iteration 18, loss = 0.29178530\n",
      "Iteration 29, loss = 0.23196628\n",
      "Iteration 6, loss = 0.48282071\n",
      "Iteration 62, loss = 0.14244893\n",
      "Iteration 16, loss = 0.30917314\n",
      "Iteration 13, loss = 0.31281132\n",
      "Iteration 24, loss = 0.25166015\n",
      "Iteration 6, loss = 0.50969108\n",
      "Iteration 9, loss = 0.38920686\n",
      "Iteration 30, loss = 0.22949881\n",
      "Iteration 14, loss = 0.31194274\n",
      "Iteration 6, loss = 0.50705291\n",
      "Iteration 19, loss = 0.28784461\n",
      "Iteration 25, loss = 0.24760713\n",
      "Iteration 63, loss = 0.13882931\n",
      "Iteration 7, loss = 0.43860950\n",
      "Iteration 17, loss = 0.30643188\n",
      "Iteration 10, loss = 0.37255736\n",
      "Iteration 7, loss = 0.46688536\n",
      "Iteration 15, loss = 0.29772276\n",
      "Iteration 31, loss = 0.22385267\n",
      "Iteration 7, loss = 0.46174092\n",
      "Iteration 8, loss = 0.41856753\n",
      "Iteration 20, loss = 0.28287258\n",
      "Iteration 64, loss = 0.13495703\n",
      "Iteration 18, loss = 0.30179580\n",
      "Iteration 32, loss = 0.21410929\n",
      "Iteration 26, loss = 0.24166123\n",
      "Iteration 16, loss = 0.29856978\n",
      "Iteration 11, loss = 0.35285842\n",
      "Iteration 8, loss = 0.42754205\n",
      "Iteration 21, loss = 0.26922934\n",
      "Iteration 8, loss = 0.43250719\n",
      "Iteration 9, loss = 0.39340377\n",
      "Iteration 65, loss = 0.13627397\n",
      "Iteration 33, loss = 0.22058441\n",
      "Iteration 27, loss = 0.24833358\n",
      "Iteration 19, loss = 0.29044817\n",
      "Iteration 17, loss = 0.28256114\n",
      "Iteration 22, loss = 0.26418985\n",
      "Iteration 12, loss = 0.33185921\n",
      "Iteration 9, loss = 0.41285304\n",
      "Iteration 28, loss = 0.23438828\n",
      "Iteration 10, loss = 0.37311479\n",
      "Iteration 9, loss = 0.40169366\n",
      "Iteration 66, loss = 0.13087707\n",
      "Iteration 10, loss = 0.38211326\n",
      "Iteration 34, loss = 0.21517444\n",
      "Iteration 18, loss = 0.28332482\n",
      "Iteration 11, loss = 0.34790895\n",
      "Iteration 20, loss = 0.28566163\n",
      "Iteration 23, loss = 0.25770741\n",
      "Iteration 13, loss = 0.32481810\n",
      "Iteration 10, loss = 0.38047396\n",
      "Iteration 11, loss = 0.37187932\n",
      "Iteration 29, loss = 0.23142642\n",
      "Iteration 12, loss = 0.33708517\n",
      "Iteration 67, loss = 0.13837400\n",
      "Iteration 35, loss = 0.21612444\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 0.25324665\n",
      "Iteration 19, loss = 0.26533178\n",
      "Iteration 21, loss = 0.28221162\n",
      "Iteration 14, loss = 0.31257029\n",
      "Iteration 11, loss = 0.36141953\n",
      "Iteration 30, loss = 0.22545147\n",
      "Iteration 13, loss = 0.32707509\n",
      "Iteration 12, loss = 0.35991369\n",
      "Iteration 68, loss = 0.13366180\n",
      "Iteration 22, loss = 0.26812139\n",
      "Iteration 20, loss = 0.26327369\n",
      "Iteration 25, loss = 0.24650674\n",
      "Iteration 12, loss = 0.34971311\n",
      "Iteration 1, loss = 4.72025028\n",
      "Iteration 15, loss = 0.29512928\n",
      "Iteration 31, loss = 0.22755290\n",
      "Iteration 14, loss = 0.31398846\n",
      "Iteration 2, loss = 4.08238793\n",
      "Iteration 26, loss = 0.24026088\n",
      "Iteration 69, loss = 0.13641059\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.26746130\n",
      "Iteration 13, loss = 0.33935898\n",
      "Iteration 16, loss = 0.28213101\n",
      "Iteration 21, loss = 0.25281806\n",
      "Iteration 13, loss = 0.33809524\n",
      "Iteration 3, loss = 2.44003842\n",
      "Iteration 32, loss = 0.22823017\n",
      "Iteration 15, loss = 0.30219930\n",
      "Iteration 27, loss = 0.23988100\n",
      "Iteration 24, loss = 0.26445791\n",
      "Iteration 14, loss = 0.32516978\n",
      "Iteration 22, loss = 0.25535318\n",
      "Iteration 17, loss = 0.28063761\n",
      "Iteration 4, loss = 0.78958313\n",
      "Iteration 14, loss = 0.33098864\n",
      "Iteration 1, loss = 4.73126519\n",
      "Iteration 33, loss = 0.21756970\n",
      "Iteration 16, loss = 0.29181209\n",
      "Iteration 25, loss = 0.26251973\n",
      "Iteration 28, loss = 0.23326481\n",
      "Iteration 23, loss = 0.25194316\n",
      "Iteration 15, loss = 0.32140094\n",
      "Iteration 5, loss = 0.55099356\n",
      "Iteration 18, loss = 0.27609299\n",
      "Iteration 15, loss = 0.31499487\n",
      "Iteration 34, loss = 0.21576464\n",
      "Iteration 17, loss = 0.28392316\n",
      "Iteration 2, loss = 4.12328314\n",
      "Iteration 26, loss = 0.26590946\n",
      "Iteration 29, loss = 0.23430422\n",
      "Iteration 24, loss = 0.24713428\n",
      "Iteration 6, loss = 0.49871719\n",
      "Iteration 16, loss = 0.30668038\n",
      "Iteration 19, loss = 0.27202766\n",
      "Iteration 35, loss = 0.21248396\n",
      "Iteration 18, loss = 0.27930618\n",
      "Iteration 27, loss = 0.25653256\n",
      "Iteration 3, loss = 2.46839230\n",
      "Iteration 16, loss = 0.30431310\n",
      "Iteration 7, loss = 0.45333346\n",
      "Iteration 25, loss = 0.23981565\n",
      "Iteration 30, loss = 0.23122191\n",
      "Iteration 19, loss = 0.26866365\n",
      "Iteration 17, loss = 0.30328598\n",
      "Iteration 20, loss = 0.26059535\n",
      "Iteration 36, loss = 0.20973034\n",
      "Iteration 4, loss = 0.80973451\n",
      "Iteration 8, loss = 0.41532789\n",
      "Iteration 28, loss = 0.25472899\n",
      "Iteration 21, loss = 0.25909486\n",
      "Iteration 17, loss = 0.29393923\n",
      "Iteration 26, loss = 0.23146032\n",
      "Iteration 31, loss = 0.22728538\n",
      "Iteration 18, loss = 0.29420751\n",
      "Iteration 20, loss = 0.27315176\n",
      "Iteration 5, loss = 0.55423577\n",
      "Iteration 9, loss = 0.39067416\n",
      "Iteration 22, loss = 0.25584466\n",
      "Iteration 37, loss = 0.20735960\n",
      "Iteration 27, loss = 0.23342337\n",
      "Iteration 19, loss = 0.29232856\n",
      "Iteration 29, loss = 0.24999925\n",
      "Iteration 32, loss = 0.22603465\n",
      "Iteration 18, loss = 0.28995762\n",
      "Iteration 21, loss = 0.26476105\n",
      "Iteration 23, loss = 0.25798723\n",
      "Iteration 6, loss = 0.49406033\n",
      "Iteration 10, loss = 0.36941890\n",
      "Iteration 28, loss = 0.23074093\n",
      "Iteration 20, loss = 0.27746550\n",
      "Iteration 38, loss = 0.21291618\n",
      "Iteration 24, loss = 0.24847815\n",
      "Iteration 19, loss = 0.28680943\n",
      "Iteration 33, loss = 0.22013083\n",
      "Iteration 30, loss = 0.24512033\n",
      "Iteration 11, loss = 0.34994886\n",
      "Iteration 22, loss = 0.25889681\n",
      "Iteration 7, loss = 0.44646512\n",
      "Iteration 29, loss = 0.22432624\n",
      "Iteration 21, loss = 0.28028821\n",
      "Iteration 20, loss = 0.27648292\n",
      "Iteration 39, loss = 0.20880766\n",
      "Iteration 25, loss = 0.23817479\n",
      "Iteration 34, loss = 0.21911507\n",
      "Iteration 31, loss = 0.24047566\n",
      "Iteration 8, loss = 0.42130382\n",
      "Iteration 12, loss = 0.33663708\n",
      "Iteration 30, loss = 0.22349318\n",
      "Iteration 23, loss = 0.25581749\n",
      "Iteration 21, loss = 0.27052866\n",
      "Iteration 32, loss = 0.23698548\n",
      "Iteration 40, loss = 0.20801304\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.27866811\n",
      "Iteration 26, loss = 0.23465324\n",
      "Iteration 35, loss = 0.22262469\n",
      "Iteration 9, loss = 0.39325721\n",
      "Iteration 33, loss = 0.23469767\n",
      "Iteration 13, loss = 0.31743005\n",
      "Iteration 31, loss = 0.21355759\n",
      "Iteration 24, loss = 0.25113505\n",
      "Iteration 22, loss = 0.27151758\n",
      "Iteration 23, loss = 0.26901084\n",
      "Iteration 27, loss = 0.23965483\n",
      "Iteration 34, loss = 0.22952287\n",
      "Iteration 36, loss = 0.21435702\n",
      "Iteration 10, loss = 0.37619770\n",
      "Iteration 32, loss = 0.21882078\n",
      "Iteration 1, loss = 4.72229053\n",
      "Iteration 14, loss = 0.31441307\n",
      "Iteration 25, loss = 0.24887482\n",
      "Iteration 24, loss = 0.26651420\n",
      "Iteration 28, loss = 0.23374668\n",
      "Iteration 23, loss = 0.26633225\n",
      "Iteration 35, loss = 0.22268672\n",
      "Iteration 37, loss = 0.21181046\n",
      "Iteration 33, loss = 0.22192239\n",
      "Iteration 11, loss = 0.35279484\n",
      "Iteration 2, loss = 4.07900114\n",
      "Iteration 25, loss = 0.26300980\n",
      "Iteration 26, loss = 0.24564829\n",
      "Iteration 15, loss = 0.30017326\n",
      "Iteration 24, loss = 0.25659281\n",
      "Iteration 29, loss = 0.22379870\n",
      "Iteration 34, loss = 0.20972969\n",
      "Iteration 38, loss = 0.21236048\n",
      "Iteration 36, loss = 0.22192027\n",
      "Iteration 12, loss = 0.33905093\n",
      "Iteration 25, loss = 0.25601236\n",
      "Iteration 26, loss = 0.26051489\n",
      "Iteration 3, loss = 2.34013102\n",
      "Iteration 27, loss = 0.23625984\n",
      "Iteration 16, loss = 0.29442512\n",
      "Iteration 35, loss = 0.20651199\n",
      "Iteration 30, loss = 0.22441352\n",
      "Iteration 28, loss = 0.22999401\n",
      "Iteration 39, loss = 0.21112036\n",
      "Iteration 37, loss = 0.22651661\n",
      "Iteration 13, loss = 0.32706397\n",
      "Iteration 4, loss = 0.75296535\n",
      "Iteration 26, loss = 0.24061142\n",
      "Iteration 27, loss = 0.25282686\n",
      "Iteration 36, loss = 0.20903623\n",
      "Iteration 17, loss = 0.29083207\n",
      "Iteration 29, loss = 0.22661806\n",
      "Iteration 31, loss = 0.21905930\n",
      "Iteration 5, loss = 0.55244829\n",
      "Iteration 18, loss = 0.27779307\n",
      "Iteration 40, loss = 0.20416140\n",
      "Iteration 28, loss = 0.25292884\n",
      "Iteration 27, loss = 0.24518917\n",
      "Iteration 37, loss = 0.20337359\n",
      "Iteration 14, loss = 0.32081167\n",
      "Iteration 38, loss = 0.22489602\n",
      "Iteration 32, loss = 0.22164805\n",
      "Iteration 30, loss = 0.23252380\n",
      "Iteration 6, loss = 0.49160886\n",
      "Iteration 39, loss = 0.21387542\n",
      "Iteration 41, loss = 0.20512286\n",
      "Iteration 29, loss = 0.24291928\n",
      "Iteration 28, loss = 0.24477269\n",
      "Iteration 38, loss = 0.20328075\n",
      "Iteration 19, loss = 0.27419141\n",
      "Iteration 15, loss = 0.30696807\n",
      "Iteration 33, loss = 0.21703894\n",
      "Iteration 31, loss = 0.23072556\n",
      "Iteration 7, loss = 0.44528777\n",
      "Iteration 39, loss = 0.20174186\n",
      "Iteration 42, loss = 0.20697422\n",
      "Iteration 29, loss = 0.24053384\n",
      "Iteration 30, loss = 0.24909260\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 0.21490140\n",
      "Iteration 20, loss = 0.26770623\n",
      "Iteration 16, loss = 0.29779179\n",
      "Iteration 34, loss = 0.21573849\n",
      "Iteration 32, loss = 0.22528795\n",
      "Iteration 8, loss = 0.41364025\n",
      "Iteration 31, loss = 0.24174871\n",
      "Iteration 43, loss = 0.20004191\n",
      "Iteration 21, loss = 0.25727880\n",
      "Iteration 40, loss = 0.19948118\n",
      "Iteration 41, loss = 0.21790239\n",
      "Iteration 17, loss = 0.28779085\n",
      "Iteration 35, loss = 0.21033325\n",
      "Iteration 33, loss = 0.21919591\n",
      "Iteration 22, loss = 0.25437851\n",
      "Iteration 9, loss = 0.38746105\n",
      "Iteration 32, loss = 0.23381608\n",
      "Iteration 1, loss = 4.71851326\n",
      "Iteration 41, loss = 0.19716267\n",
      "Iteration 44, loss = 0.19865644\n",
      "Iteration 23, loss = 0.24905805\n",
      "Iteration 42, loss = 0.21923617\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 0.36437064\n",
      "Iteration 18, loss = 0.28760814\n",
      "Iteration 42, loss = 0.19303853\n",
      "Iteration 45, loss = 0.20074793\n",
      "Iteration 36, loss = 0.21334012\n",
      "Iteration 33, loss = 0.22982004\n",
      "Iteration 11, loss = 0.35267321\n",
      "Iteration 34, loss = 0.21522760\n",
      "Iteration 2, loss = 4.08314490\n",
      "Iteration 24, loss = 0.25213569\n",
      "Iteration 19, loss = 0.27756097\n",
      "Iteration 37, loss = 0.21052613\n",
      "Iteration 43, loss = 0.19007199\n",
      "Iteration 46, loss = 0.19469948\n",
      "Iteration 3, loss = 2.40641682\n",
      "Iteration 34, loss = 0.23452902\n",
      "Iteration 1, loss = 4.71903017\n",
      "Iteration 12, loss = 0.33728635\n",
      "Iteration 38, loss = 0.20608398\n",
      "Iteration 25, loss = 0.25199753\n",
      "Iteration 47, loss = 0.19217072\n",
      "Iteration 44, loss = 0.18703830\n",
      "Iteration 35, loss = 0.21455240\n",
      "Iteration 4, loss = 0.78184824\n",
      "Iteration 20, loss = 0.27002823\n",
      "Iteration 2, loss = 4.08480648\n",
      "Iteration 35, loss = 0.23218576\n",
      "Iteration 13, loss = 0.32936271\n",
      "Iteration 39, loss = 0.20140168\n",
      "Iteration 45, loss = 0.18487994\n",
      "Iteration 36, loss = 0.21270121\n",
      "Iteration 48, loss = 0.18690809\n",
      "Iteration 26, loss = 0.23921331\n",
      "Iteration 36, loss = 0.22357426\n",
      "Iteration 5, loss = 0.56009815\n",
      "Iteration 3, loss = 2.39935674\n",
      "Iteration 14, loss = 0.31065774\n",
      "Iteration 46, loss = 0.18182417\n",
      "Iteration 21, loss = 0.25907357\n",
      "Iteration 40, loss = 0.19919243\n",
      "Iteration 49, loss = 0.19207857\n",
      "Iteration 27, loss = 0.22972195\n",
      "Iteration 37, loss = 0.20969924\n",
      "Iteration 22, loss = 0.26030379\n",
      "Iteration 37, loss = 0.22520540\n",
      "Iteration 47, loss = 0.19146798\n",
      "Iteration 15, loss = 0.29906116\n",
      "Iteration 41, loss = 0.19837229\n",
      "Iteration 50, loss = 0.18899417\n",
      "Iteration 4, loss = 0.78279531\n",
      "Iteration 6, loss = 0.49891699\n",
      "Iteration 28, loss = 0.23030407\n",
      "Iteration 38, loss = 0.20927144\n",
      "Iteration 38, loss = 0.22021656\n",
      "Iteration 23, loss = 0.25399673\n",
      "Iteration 16, loss = 0.29546740\n",
      "Iteration 42, loss = 0.19134778\n",
      "Iteration 5, loss = 0.57662130\n",
      "Iteration 48, loss = 0.18549878\n",
      "Iteration 7, loss = 0.45509945\n",
      "Iteration 51, loss = 0.18626487\n",
      "Iteration 29, loss = 0.22780198\n",
      "Iteration 39, loss = 0.20823023\n",
      "Iteration 17, loss = 0.29101511\n",
      "Iteration 24, loss = 0.24789118\n",
      "Iteration 39, loss = 0.22027318\n",
      "Iteration 43, loss = 0.19412208\n",
      "Iteration 49, loss = 0.17886692\n",
      "Iteration 6, loss = 0.50494785\n",
      "Iteration 52, loss = 0.18365558\n",
      "Iteration 30, loss = 0.22378950\n",
      "Iteration 8, loss = 0.41892317\n",
      "Iteration 40, loss = 0.20348495\n",
      "Iteration 50, loss = 0.17641029\n",
      "Iteration 25, loss = 0.24187545\n",
      "Iteration 18, loss = 0.28408641\n",
      "Iteration 40, loss = 0.21267456\n",
      "Iteration 44, loss = 0.19438805\n",
      "Iteration 31, loss = 0.21922105\n",
      "Iteration 41, loss = 0.20104712\n",
      "Iteration 53, loss = 0.17492949\n",
      "Iteration 51, loss = 0.18070321\n",
      "Iteration 9, loss = 0.40053308\n",
      "Iteration 7, loss = 0.46009023\n",
      "Iteration 8, loss = 0.43159233\n",
      "Iteration 52, loss = 0.17228229\n",
      "Iteration 26, loss = 0.23692193\n",
      "Iteration 42, loss = 0.20051868\n",
      "Iteration 54, loss = 0.17339837\n",
      "Iteration 41, loss = 0.21475139\n",
      "Iteration 19, loss = 0.28012361\n",
      "Iteration 45, loss = 0.19042284\n",
      "Iteration 32, loss = 0.22185983\n",
      "Iteration 27, loss = 0.24162941\n",
      "Iteration 10, loss = 0.37971278\n",
      "Iteration 9, loss = 0.41096331\n",
      "Iteration 43, loss = 0.19808013\n",
      "Iteration 55, loss = 0.17469373\n",
      "Iteration 53, loss = 0.17485434\n",
      "Iteration 20, loss = 0.26594151\n",
      "Iteration 46, loss = 0.18689085\n",
      "Iteration 11, loss = 0.36520628\n",
      "Iteration 42, loss = 0.21003590\n",
      "Iteration 33, loss = 0.22101001\n",
      "Iteration 10, loss = 0.38814725\n",
      "Iteration 28, loss = 0.23266560\n",
      "Iteration 54, loss = 0.16957075\n",
      "Iteration 44, loss = 0.19725157\n",
      "Iteration 56, loss = 0.17822210\n",
      "Iteration 47, loss = 0.18413106\n",
      "Iteration 21, loss = 0.26057933\n",
      "Iteration 29, loss = 0.23010959\n",
      "Iteration 11, loss = 0.37127538\n",
      "Iteration 12, loss = 0.34531904\n",
      "Iteration 34, loss = 0.21270636\n",
      "Iteration 45, loss = 0.19189764\n",
      "Iteration 43, loss = 0.20489954\n",
      "Iteration 48, loss = 0.18321937\n",
      "Iteration 55, loss = 0.17076714\n",
      "Iteration 57, loss = 0.17369470\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.25477345\n",
      "Iteration 30, loss = 0.23053476\n",
      "Iteration 44, loss = 0.21209753\n",
      "Iteration 49, loss = 0.18000537\n",
      "Iteration 13, loss = 0.33048343\n",
      "Iteration 12, loss = 0.36223270\n",
      "Iteration 46, loss = 0.19515960\n",
      "Iteration 35, loss = 0.21568478\n",
      "Iteration 56, loss = 0.17364885\n",
      "Iteration 1, loss = 4.56505101\n",
      "Iteration 13, loss = 0.34159887\n",
      "Iteration 47, loss = 0.18888762\n",
      "Iteration 36, loss = 0.20948040\n",
      "Iteration 23, loss = 0.25728867\n",
      "Iteration 57, loss = 0.16812805\n",
      "Iteration 31, loss = 0.22504282\n",
      "Iteration 45, loss = 0.21556525\n",
      "Iteration 50, loss = 0.17630043\n",
      "Iteration 14, loss = 0.32170014\n",
      "Iteration 2, loss = 2.42887937\n",
      "Iteration 14, loss = 0.33282427\n",
      "Iteration 48, loss = 0.18198354\n",
      "Iteration 46, loss = 0.20682594\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 51, loss = 0.18686403\n",
      "Iteration 37, loss = 0.20687713\n",
      "Iteration 24, loss = 0.25466987\n",
      "Iteration 32, loss = 0.22026651\n",
      "Iteration 15, loss = 0.30834959\n",
      "Iteration 58, loss = 0.17490470\n",
      "Iteration 3, loss = 0.64904542\n",
      "Iteration 38, loss = 0.20088414\n",
      "Iteration 15, loss = 0.32258568\n",
      "Iteration 49, loss = 0.18139041\n",
      "Iteration 33, loss = 0.21335875\n",
      "Iteration 52, loss = 0.17733154\n",
      "Iteration 25, loss = 0.24058210\n",
      "Iteration 16, loss = 0.29792165\n",
      "Iteration 1, loss = 4.50729181\n",
      "Iteration 59, loss = 0.16886617\n",
      "Iteration 16, loss = 0.31131964\n",
      "Iteration 39, loss = 0.19781002\n",
      "Iteration 4, loss = 0.54048540\n",
      "Iteration 34, loss = 0.21382533\n",
      "Iteration 50, loss = 0.17972564\n",
      "Iteration 26, loss = 0.24270121\n",
      "Iteration 17, loss = 0.29606238\n",
      "Iteration 53, loss = 0.17377832\n",
      "Iteration 5, loss = 0.46143013\n",
      "Iteration 60, loss = 0.16213051\n",
      "Iteration 17, loss = 0.30753414\n",
      "Iteration 40, loss = 0.19469878\n",
      "Iteration 2, loss = 2.22452792\n",
      "Iteration 51, loss = 0.18084170\n",
      "Iteration 18, loss = 0.28482653\n",
      "Iteration 54, loss = 0.17276401\n",
      "Iteration 35, loss = 0.20872897\n",
      "Iteration 27, loss = 0.23465487\n",
      "Iteration 3, loss = 0.64678636\n",
      "Iteration 18, loss = 0.29272537\n",
      "Iteration 6, loss = 0.40735730\n",
      "Iteration 61, loss = 0.16226692\n",
      "Iteration 41, loss = 0.20267159\n",
      "Iteration 4, loss = 0.53842709\n",
      "Iteration 52, loss = 0.18157000\n",
      "Iteration 36, loss = 0.21351485\n",
      "Iteration 62, loss = 0.15939785\n",
      "Iteration 19, loss = 0.27711009\n",
      "Iteration 19, loss = 0.29051833\n",
      "Iteration 7, loss = 0.38113100\n",
      "Iteration 55, loss = 0.17381136\n",
      "Iteration 28, loss = 0.23441599\n",
      "Iteration 42, loss = 0.18899407\n",
      "Iteration 5, loss = 0.46275640\n",
      "Iteration 37, loss = 0.20539804\n",
      "Iteration 20, loss = 0.28570937\n",
      "Iteration 8, loss = 0.35675709\n",
      "Iteration 63, loss = 0.16283882\n",
      "Iteration 56, loss = 0.16615063\n",
      "Iteration 20, loss = 0.27603319\n",
      "Iteration 53, loss = 0.17984786\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 0.18757623\n",
      "Iteration 29, loss = 0.22964173\n",
      "Iteration 6, loss = 0.41314649\n",
      "Iteration 57, loss = 0.16491627\n",
      "Iteration 38, loss = 0.20632798\n",
      "Iteration 21, loss = 0.27789018\n",
      "Iteration 9, loss = 0.33301294\n",
      "Iteration 21, loss = 0.26477544\n",
      "Iteration 44, loss = 0.18456302\n",
      "Iteration 64, loss = 0.15612593\n",
      "Iteration 58, loss = 0.16609426\n",
      "Iteration 30, loss = 0.22568558\n",
      "Iteration 39, loss = 0.20511152\n",
      "Iteration 10, loss = 0.31212470\n",
      "Iteration 7, loss = 0.37093271\n",
      "Iteration 1, loss = 4.48783537\n",
      "Iteration 22, loss = 0.27158297\n",
      "Iteration 22, loss = 0.26684199\n",
      "Iteration 45, loss = 0.18253924\n",
      "Iteration 65, loss = 0.15288710\n",
      "Iteration 8, loss = 0.35005303\n",
      "Iteration 11, loss = 0.31991900\n",
      "Iteration 59, loss = 0.16768305\n",
      "Iteration 40, loss = 0.20429047\n",
      "Iteration 31, loss = 0.22370227\n",
      "Iteration 23, loss = 0.26962490\n",
      "Iteration 23, loss = 0.25470398\n",
      "Iteration 2, loss = 2.14601187\n",
      "Iteration 46, loss = 0.19090028\n",
      "Iteration 66, loss = 0.15220762\n",
      "Iteration 41, loss = 0.19871556\n",
      "Iteration 60, loss = 0.16587531\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 0.22586428\n",
      "Iteration 9, loss = 0.33857722\n",
      "Iteration 12, loss = 0.30106448\n",
      "Iteration 24, loss = 0.25249185\n",
      "Iteration 3, loss = 0.63092561\n",
      "Iteration 47, loss = 0.18957474\n",
      "Iteration 24, loss = 0.26096251\n",
      "Iteration 42, loss = 0.19635536\n",
      "Iteration 25, loss = 0.25158820\n",
      "Iteration 67, loss = 0.15762172\n",
      "Iteration 13, loss = 0.28456513\n",
      "Iteration 1, loss = 4.49558200\n",
      "Iteration 33, loss = 0.22067691\n",
      "Iteration 43, loss = 0.19260105\n",
      "Iteration 25, loss = 0.26139831\n",
      "Iteration 48, loss = 0.18816079\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 0.32435918\n",
      "Iteration 4, loss = 0.51519687\n",
      "Iteration 68, loss = 0.16100586\n",
      "Iteration 26, loss = 0.24588964\n",
      "Iteration 2, loss = 2.17970395\n",
      "Iteration 34, loss = 0.21664201\n",
      "Iteration 14, loss = 0.26713491\n",
      "Iteration 44, loss = 0.19261295\n",
      "Iteration 26, loss = 0.25260082\n",
      "Iteration 15, loss = 0.27057453\n",
      "Iteration 5, loss = 0.43412847\n",
      "Iteration 11, loss = 0.30845551\n",
      "Iteration 45, loss = 0.19013154\n",
      "Iteration 35, loss = 0.21471736\n",
      "Iteration 27, loss = 0.24118328\n",
      "Iteration 69, loss = 0.15212381\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.51820865\n",
      "Iteration 3, loss = 0.64300629\n",
      "Iteration 27, loss = 0.25187814\n",
      "Iteration 6, loss = 0.39256008\n",
      "Iteration 12, loss = 0.28399461\n",
      "Iteration 28, loss = 0.23660933\n",
      "Iteration 4, loss = 0.53522807\n",
      "Iteration 2, loss = 2.29550749\n",
      "Iteration 16, loss = 0.25633396\n",
      "Iteration 36, loss = 0.21557699\n",
      "Iteration 28, loss = 0.24762215\n",
      "Iteration 46, loss = 0.18632607\n",
      "Iteration 7, loss = 0.35917638\n",
      "Iteration 29, loss = 0.23569373\n",
      "Iteration 13, loss = 0.29777366\n",
      "Iteration 3, loss = 0.69358542\n",
      "Iteration 1, loss = 4.51428002\n",
      "Iteration 29, loss = 0.24674283\n",
      "Iteration 47, loss = 0.18179432\n",
      "Iteration 17, loss = 0.25446505\n",
      "Iteration 37, loss = 0.21339029\n",
      "Iteration 5, loss = 0.45401385\n",
      "Iteration 8, loss = 0.34695767\n",
      "Iteration 14, loss = 0.28369744\n",
      "Iteration 30, loss = 0.23519774\n",
      "Iteration 2, loss = 2.23977567\n",
      "Iteration 4, loss = 0.55730305\n",
      "Iteration 48, loss = 0.18274302\n",
      "Iteration 30, loss = 0.24274791\n",
      "Iteration 38, loss = 0.21373540\n",
      "Iteration 18, loss = 0.23744518\n",
      "Iteration 9, loss = 0.32555340\n",
      "Iteration 6, loss = 0.41356960\n",
      "Iteration 15, loss = 0.28302452\n",
      "Iteration 3, loss = 0.64866004\n",
      "Iteration 5, loss = 0.47329922\n",
      "Iteration 31, loss = 0.22902704\n",
      "Iteration 31, loss = 0.23598955\n",
      "Iteration 39, loss = 0.20530425\n",
      "Iteration 49, loss = 0.17792505\n",
      "Iteration 19, loss = 0.25108977\n",
      "Iteration 10, loss = 0.31235383\n",
      "Iteration 7, loss = 0.37350167\n",
      "Iteration 4, loss = 0.52158293\n",
      "Iteration 16, loss = 0.26652996\n",
      "Iteration 32, loss = 0.22372328\n",
      "Iteration 6, loss = 0.42081291\n",
      "Iteration 32, loss = 0.23249935\n",
      "Iteration 50, loss = 0.17742496\n",
      "Iteration 20, loss = 0.24642876\n",
      "Iteration 40, loss = 0.19818391\n",
      "Iteration 8, loss = 0.35852226\n",
      "Iteration 11, loss = 0.30536249\n",
      "Iteration 5, loss = 0.44509626\n",
      "Iteration 17, loss = 0.27263645\n",
      "Iteration 33, loss = 0.22331345\n",
      "Iteration 7, loss = 0.38991056\n",
      "Iteration 33, loss = 0.23433455\n",
      "Iteration 51, loss = 0.17476125\n",
      "Iteration 41, loss = 0.19786247\n",
      "Iteration 9, loss = 0.35636475\n",
      "Iteration 21, loss = 0.23332930\n",
      "Iteration 12, loss = 0.28867810\n",
      "Iteration 18, loss = 0.25403291\n",
      "Iteration 6, loss = 0.40337414\n",
      "Iteration 8, loss = 0.37314289\n",
      "Iteration 19, loss = 0.24307485\n",
      "Iteration 34, loss = 0.22672901\n",
      "Iteration 34, loss = 0.22759052\n",
      "Iteration 22, loss = 0.23018002\n",
      "Iteration 52, loss = 0.18186745\n",
      "Iteration 42, loss = 0.19595919\n",
      "Iteration 10, loss = 0.32439805\n",
      "Iteration 13, loss = 0.28876027\n",
      "Iteration 7, loss = 0.37151185\n",
      "Iteration 20, loss = 0.24496399\n",
      "Iteration 35, loss = 0.22019789\n",
      "Iteration 9, loss = 0.35758942\n",
      "Iteration 35, loss = 0.22486965\n",
      "Iteration 8, loss = 0.34510446\n",
      "Iteration 53, loss = 0.18665000\n",
      "Iteration 23, loss = 0.22824780\n",
      "Iteration 11, loss = 0.30453423\n",
      "Iteration 43, loss = 0.19438167\n",
      "Iteration 14, loss = 0.27159163\n",
      "Iteration 21, loss = 0.24265193\n",
      "Iteration 10, loss = 0.33466748\n",
      "Iteration 36, loss = 0.22503873\n",
      "Iteration 36, loss = 0.21350965\n",
      "Iteration 9, loss = 0.33515104\n",
      "Iteration 54, loss = 0.18268553\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 0.19185209\n",
      "Iteration 24, loss = 0.21995837\n",
      "Iteration 15, loss = 0.27812765\n",
      "Iteration 22, loss = 0.23506231\n",
      "Iteration 12, loss = 0.30113920\n",
      "Iteration 37, loss = 0.21025797\n",
      "Iteration 37, loss = 0.22685616\n",
      "Iteration 10, loss = 0.31766078\n",
      "Iteration 11, loss = 0.32464263\n",
      "Iteration 23, loss = 0.22924216\n",
      "Iteration 13, loss = 0.30249560\n",
      "Iteration 25, loss = 0.21728999\n",
      "Iteration 45, loss = 0.19285171\n",
      "Iteration 16, loss = 0.25935703\n",
      "Iteration 1, loss = 4.51073506\n",
      "Iteration 38, loss = 0.20998090\n",
      "Iteration 11, loss = 0.30306939\n",
      "Iteration 38, loss = 0.22404853\n",
      "Iteration 24, loss = 0.23078006\n",
      "Iteration 12, loss = 0.31132164\n",
      "Iteration 14, loss = 0.28531937\n",
      "Iteration 26, loss = 0.21232856\n",
      "Iteration 46, loss = 0.18727835\n",
      "Iteration 17, loss = 0.24687872\n",
      "Iteration 39, loss = 0.21126779\n",
      "Iteration 2, loss = 2.22117426\n",
      "Iteration 39, loss = 0.22085038\n",
      "Iteration 25, loss = 0.21457719\n",
      "Iteration 12, loss = 0.28751080\n",
      "Iteration 15, loss = 0.29285302\n",
      "Iteration 13, loss = 0.30112232\n",
      "Iteration 27, loss = 0.20447181\n",
      "Iteration 47, loss = 0.18539512\n",
      "Iteration 18, loss = 0.24105537\n",
      "Iteration 3, loss = 0.62970845\n",
      "Iteration 16, loss = 0.26838508\n",
      "Iteration 40, loss = 0.21429053\n",
      "Iteration 40, loss = 0.20062586\n",
      "Iteration 26, loss = 0.21855995\n",
      "Iteration 48, loss = 0.18759001\n",
      "Iteration 13, loss = 0.28311477\n",
      "Iteration 19, loss = 0.23744486\n",
      "Iteration 14, loss = 0.29729222\n",
      "Iteration 28, loss = 0.20827983\n",
      "Iteration 27, loss = 0.21383743\n",
      "Iteration 41, loss = 0.21134014\n",
      "Iteration 4, loss = 0.51991750\n",
      "Iteration 49, loss = 0.19165469\n",
      "Iteration 17, loss = 0.27247820\n",
      "Iteration 41, loss = 0.20272691\n",
      "Iteration 20, loss = 0.23836051\n",
      "Iteration 14, loss = 0.28285971\n",
      "Iteration 18, loss = 0.25657378\n",
      "Iteration 29, loss = 0.20141272\n",
      "Iteration 15, loss = 0.29948808\n",
      "Iteration 42, loss = 0.20120495\n",
      "Iteration 5, loss = 0.43884708\n",
      "Iteration 42, loss = 0.21719677\n",
      "Iteration 50, loss = 0.18388503\n",
      "Iteration 19, loss = 0.25345271\n",
      "Iteration 28, loss = 0.20788794\n",
      "Iteration 30, loss = 0.19953274\n",
      "Iteration 21, loss = 0.24224034\n",
      "Iteration 16, loss = 0.29093410\n",
      "Iteration 43, loss = 0.20600718\n",
      "Iteration 15, loss = 0.27730672\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 0.39735490\n",
      "Iteration 20, loss = 0.23912830\n",
      "Iteration 43, loss = 0.20486622\n",
      "Iteration 29, loss = 0.20893445\n",
      "Iteration 51, loss = 0.17844278\n",
      "Iteration 31, loss = 0.20085135\n",
      "Iteration 17, loss = 0.27344500\n",
      "Iteration 16, loss = 0.25279125\n",
      "Iteration 22, loss = 0.22578553\n",
      "Iteration 44, loss = 0.21120992\n",
      "Iteration 7, loss = 0.36911667\n",
      "Iteration 30, loss = 0.20632835\n",
      "Iteration 21, loss = 0.23454524\n",
      "Iteration 1, loss = 4.45942367\n",
      "Iteration 52, loss = 0.18054378\n",
      "Iteration 32, loss = 0.19463181\n",
      "Iteration 23, loss = 0.22828078\n",
      "Iteration 45, loss = 0.20323471\n",
      "Iteration 17, loss = 0.25938658\n",
      "Iteration 18, loss = 0.27471603\n",
      "Iteration 53, loss = 0.17554898\n",
      "Iteration 8, loss = 0.33619161\n",
      "Iteration 22, loss = 0.23624681\n",
      "Iteration 31, loss = 0.20243395\n",
      "Iteration 33, loss = 0.18457941\n",
      "Iteration 2, loss = 1.97995372\n",
      "Iteration 46, loss = 0.20520078\n",
      "Iteration 24, loss = 0.22026573\n",
      "Iteration 19, loss = 0.26626578\n",
      "Iteration 18, loss = 0.24623491\n",
      "Iteration 23, loss = 0.23257153\n",
      "Iteration 54, loss = 0.17746872\n",
      "Iteration 9, loss = 0.32644692\n",
      "Iteration 34, loss = 0.18625234\n",
      "Iteration 25, loss = 0.21952124\n",
      "Iteration 32, loss = 0.20130393\n",
      "Iteration 3, loss = 0.63735828\n",
      "Iteration 47, loss = 0.19920604\n",
      "Iteration 24, loss = 0.23234398\n",
      "Iteration 20, loss = 0.26643759\n",
      "Iteration 10, loss = 0.31590580\n",
      "Iteration 19, loss = 0.24351179\n",
      "Iteration 55, loss = 0.17664532\n",
      "Iteration 35, loss = 0.19811491\n",
      "Iteration 26, loss = 0.21260099\n",
      "Iteration 4, loss = 0.50659621\n",
      "Iteration 25, loss = 0.22548615\n",
      "Iteration 33, loss = 0.19064194\n",
      "Iteration 48, loss = 0.20037830\n",
      "Iteration 21, loss = 0.25976828\n",
      "Iteration 11, loss = 0.30190357\n",
      "Iteration 20, loss = 0.23217916\n",
      "Iteration 56, loss = 0.17397272\n",
      "Iteration 5, loss = 0.44491868\n",
      "Iteration 27, loss = 0.21392817\n",
      "Iteration 36, loss = 0.18860936\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 0.22203315\n",
      "Iteration 49, loss = 0.19825838\n",
      "Iteration 57, loss = 0.17692111\n",
      "Iteration 21, loss = 0.22601795\n",
      "Iteration 34, loss = 0.19641361\n",
      "Iteration 22, loss = 0.25154263\n",
      "Iteration 12, loss = 0.29467479\n",
      "Iteration 6, loss = 0.40075066\n",
      "Iteration 28, loss = 0.21405747\n",
      "Iteration 7, loss = 0.36559878\n",
      "Iteration 29, loss = 0.20960180\n",
      "Iteration 27, loss = 0.21864085\n",
      "Iteration 1, loss = 4.50094391\n",
      "Iteration 50, loss = 0.20426882\n",
      "Iteration 58, loss = 0.17377014\n",
      "Iteration 22, loss = 0.24142408\n",
      "Iteration 23, loss = 0.23965751\n",
      "Iteration 35, loss = 0.18739053\n",
      "Iteration 8, loss = 0.34742592\n",
      "Iteration 13, loss = 0.28257461\n",
      "Iteration 2, loss = 2.18404697\n",
      "Iteration 59, loss = 0.16671395\n",
      "Iteration 9, loss = 0.33118671\n",
      "Iteration 30, loss = 0.20549998\n",
      "Iteration 36, loss = 0.18231610\n",
      "Iteration 28, loss = 0.21910456\n",
      "Iteration 3, loss = 0.64636615\n",
      "Iteration 60, loss = 0.17185543\n",
      "Iteration 51, loss = 0.19993658\n",
      "Iteration 14, loss = 0.29631745\n",
      "Iteration 23, loss = 0.22986211\n",
      "Iteration 24, loss = 0.24358139\n",
      "Iteration 10, loss = 0.31128718\n",
      "Iteration 37, loss = 0.17823899\n",
      "Iteration 15, loss = 0.27272303\n",
      "Iteration 29, loss = 0.22085449\n",
      "Iteration 31, loss = 0.20631513\n",
      "Iteration 61, loss = 0.16910482\n",
      "Iteration 4, loss = 0.54146448\n",
      "Iteration 52, loss = 0.19580959\n",
      "Iteration 24, loss = 0.22524963\n",
      "Iteration 11, loss = 0.30987828\n",
      "Iteration 25, loss = 0.24103683\n",
      "Iteration 30, loss = 0.20718749\n",
      "Iteration 16, loss = 0.25413816\n",
      "Iteration 38, loss = 0.18291966\n",
      "Iteration 5, loss = 0.45193404\n",
      "Iteration 62, loss = 0.16813096\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 0.18626624\n",
      "Iteration 53, loss = 0.20244930\n",
      "Iteration 25, loss = 0.21783742\n",
      "Iteration 31, loss = 0.21166487\n",
      "Iteration 12, loss = 0.29782108\n",
      "Iteration 17, loss = 0.25120107\n",
      "Iteration 26, loss = 0.23507894\n",
      "Iteration 39, loss = 0.17773412\n",
      "Iteration 26, loss = 0.21739642\n",
      "Iteration 33, loss = 0.18972662\n",
      "Iteration 6, loss = 0.39353716\n",
      "Iteration 54, loss = 0.19309184\n",
      "Iteration 32, loss = 0.21268363\n",
      "Iteration 13, loss = 0.28933548\n",
      "Iteration 1, loss = 4.49291724\n",
      "Iteration 18, loss = 0.25302197\n",
      "Iteration 27, loss = 0.20898672\n",
      "Iteration 40, loss = 0.16927952\n",
      "Iteration 27, loss = 0.23365126\n",
      "Iteration 7, loss = 0.37922121\n",
      "Iteration 34, loss = 0.18809504\n",
      "Iteration 55, loss = 0.19235207\n",
      "Iteration 33, loss = 0.20780726\n",
      "Iteration 14, loss = 0.28173772\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.34589134\n",
      "Iteration 2, loss = 2.14096600\n",
      "Iteration 28, loss = 0.20702968\n",
      "Iteration 19, loss = 0.23541708\n",
      "Iteration 41, loss = 0.16928879\n",
      "Iteration 28, loss = 0.21591246\n",
      "Iteration 9, loss = 0.33637887\n",
      "Iteration 3, loss = 0.64420209\n",
      "Iteration 56, loss = 0.18840545\n",
      "Iteration 35, loss = 0.18087636\n",
      "Iteration 15, loss = 0.27802383\n",
      "Iteration 29, loss = 0.19807408\n",
      "Iteration 20, loss = 0.24145221\n",
      "Iteration 42, loss = 0.17113845\n",
      "Iteration 29, loss = 0.22573408\n",
      "Iteration 1, loss = 4.48852795\n",
      "Iteration 4, loss = 0.53795509\n",
      "Iteration 10, loss = 0.33895536\n",
      "Iteration 57, loss = 0.18828611\n",
      "Iteration 30, loss = 0.18638239\n",
      "Iteration 36, loss = 0.18397694\n",
      "Iteration 16, loss = 0.27213720\n",
      "Iteration 21, loss = 0.23730985\n",
      "Iteration 30, loss = 0.21940373\n",
      "Iteration 43, loss = 0.17691265\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 2.18556815\n",
      "Iteration 17, loss = 0.25230377\n",
      "Iteration 58, loss = 0.18544110\n",
      "Iteration 37, loss = 0.18211962\n",
      "Iteration 31, loss = 0.19305998\n",
      "Iteration 11, loss = 0.31405475\n",
      "Iteration 5, loss = 0.45482808\n",
      "Iteration 22, loss = 0.22787169\n",
      "Iteration 31, loss = 0.21561416\n",
      "Iteration 18, loss = 0.24594002\n",
      "Iteration 1, loss = 4.50584080\n",
      "Iteration 32, loss = 0.19036023\n",
      "Iteration 3, loss = 0.64870198\n",
      "Iteration 6, loss = 0.42415240\n",
      "Iteration 23, loss = 0.22879834\n",
      "Iteration 59, loss = 0.17928799\n",
      "Iteration 38, loss = 0.18589427\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 0.30089628\n",
      "Iteration 32, loss = 0.22077648\n",
      "Iteration 2, loss = 2.21767918\n",
      "Iteration 13, loss = 0.28924857\n",
      "Iteration 19, loss = 0.25664565\n",
      "Iteration 33, loss = 0.18973765\n",
      "Iteration 4, loss = 0.53953359\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 0.40483669\n",
      "Iteration 24, loss = 0.23795392\n",
      "Iteration 60, loss = 0.17681555\n",
      "Iteration 33, loss = 0.21662450\n",
      "Iteration 3, loss = 0.65159072\n",
      "Iteration 14, loss = 0.27212280\n",
      "Iteration 1, loss = 4.51231973\n",
      "Iteration 25, loss = 0.21760324\n",
      "Iteration 8, loss = 0.37874494\n",
      "Iteration 20, loss = 0.24675028\n",
      "Iteration 5, loss = 0.44486917\n",
      "Iteration 34, loss = 0.21324953\n",
      "Iteration 61, loss = 0.17170246\n",
      "Iteration 4, loss = 0.53090076\n",
      "Iteration 15, loss = 0.26203742\n",
      "Iteration 1, loss = 4.49649242\n",
      "Iteration 2, loss = 2.24127365\n",
      "Iteration 21, loss = 0.24372750\n",
      "Iteration 6, loss = 0.39599430\n",
      "Iteration 35, loss = 0.21069435\n",
      "Iteration 9, loss = 0.35420647\n",
      "Iteration 26, loss = 0.21440377\n",
      "Iteration 62, loss = 0.17158763\n",
      "Iteration 3, loss = 0.64250319\n",
      "Iteration 5, loss = 0.45038380\n",
      "Iteration 2, loss = 2.19007564\n",
      "Iteration 7, loss = 0.38115864\n",
      "Iteration 16, loss = 0.26638325\n",
      "Iteration 36, loss = 0.20322120\n",
      "Iteration 10, loss = 0.33176390\n",
      "Iteration 22, loss = 0.23412678\n",
      "Iteration 27, loss = 0.20879773\n",
      "Iteration 63, loss = 0.17062153\n",
      "Iteration 6, loss = 0.40582146\n",
      "Iteration 17, loss = 0.26838588\n",
      "Iteration 11, loss = 0.31638484\n",
      "Iteration 4, loss = 0.52036856\n",
      "Iteration 37, loss = 0.19764481\n",
      "Iteration 23, loss = 0.23177820\n",
      "Iteration 3, loss = 0.64119315\n",
      "Iteration 28, loss = 0.20724707\n",
      "Iteration 8, loss = 0.34817624\n",
      "Iteration 64, loss = 0.17513538\n",
      "Iteration 38, loss = 0.19719182\n",
      "Iteration 4, loss = 0.52474708\n",
      "Iteration 24, loss = 0.22262386\n",
      "Iteration 7, loss = 0.36761806\n",
      "Iteration 18, loss = 0.25317057\n",
      "Iteration 12, loss = 0.30838030\n",
      "Iteration 9, loss = 0.32615056\n",
      "Iteration 5, loss = 0.43927376\n",
      "Iteration 39, loss = 0.19580355\n",
      "Iteration 29, loss = 0.20307455\n",
      "Iteration 65, loss = 0.18038264\n",
      "Iteration 5, loss = 0.44680163\n",
      "Iteration 25, loss = 0.21984921\n",
      "Iteration 13, loss = 0.30148985\n",
      "Iteration 19, loss = 0.25618080\n",
      "Iteration 8, loss = 0.34448326\n",
      "Iteration 40, loss = 0.19774459\n",
      "Iteration 66, loss = 0.17094616\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 0.30170417\n",
      "Iteration 6, loss = 0.41373118\n",
      "Iteration 6, loss = 0.39981082\n",
      "Iteration 30, loss = 0.20382867\n",
      "Iteration 26, loss = 0.21717111\n",
      "Iteration 20, loss = 0.23558814\n",
      "Iteration 14, loss = 0.28892740\n",
      "Iteration 9, loss = 0.33168520\n",
      "Iteration 31, loss = 0.20175832\n",
      "Iteration 7, loss = 0.38495513\n",
      "Iteration 41, loss = 0.19836953\n",
      "Iteration 11, loss = 0.30823892\n",
      "Iteration 10, loss = 0.31627954\n",
      "Iteration 21, loss = 0.23073719\n",
      "Iteration 15, loss = 0.28748019\n",
      "Iteration 27, loss = 0.21582178\n",
      "Iteration 7, loss = 0.37185945\n",
      "Iteration 32, loss = 0.19007209\n",
      "Iteration 16, loss = 0.28733232\n",
      "Iteration 1, loss = 4.50462781\n",
      "Iteration 8, loss = 0.35668678\n",
      "Iteration 8, loss = 0.35678369\n",
      "Iteration 12, loss = 0.29473993\n",
      "Iteration 42, loss = 0.19209607\n",
      "Iteration 11, loss = 0.30932729\n",
      "Iteration 22, loss = 0.23238770\n",
      "Iteration 28, loss = 0.21300110\n",
      "Iteration 33, loss = 0.19325951\n",
      "Iteration 17, loss = 0.26886015\n",
      "Iteration 13, loss = 0.27678706\n",
      "Iteration 9, loss = 0.32909409\n",
      "Iteration 2, loss = 2.22505568\n",
      "Iteration 12, loss = 0.29345915\n",
      "Iteration 43, loss = 0.18262675\n",
      "Iteration 9, loss = 0.33267472\n",
      "Iteration 23, loss = 0.22535613\n",
      "Iteration 29, loss = 0.20663510\n",
      "Iteration 44, loss = 0.18109059\n",
      "Iteration 3, loss = 0.67036254\n",
      "Iteration 34, loss = 0.19061451\n",
      "Iteration 14, loss = 0.26402833\n",
      "Iteration 18, loss = 0.27151134\n",
      "Iteration 10, loss = 0.32688686\n",
      "Iteration 10, loss = 0.30691787\n",
      "Iteration 13, loss = 0.28288586\n",
      "Iteration 30, loss = 0.20562148\n",
      "Iteration 24, loss = 0.22720735\n",
      "Iteration 45, loss = 0.17746029\n",
      "Iteration 4, loss = 0.55502878\n",
      "Iteration 14, loss = 0.27064688\n",
      "Iteration 15, loss = 0.25480147\n",
      "Iteration 35, loss = 0.19208697\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.30808464\n",
      "Iteration 11, loss = 0.30905369\n",
      "Iteration 19, loss = 0.26741530\n",
      "Iteration 31, loss = 0.21227250\n",
      "Iteration 15, loss = 0.27240404\n",
      "Iteration 25, loss = 0.22117264\n",
      "Iteration 46, loss = 0.18221998\n",
      "Iteration 5, loss = 0.46362678\n",
      "Iteration 20, loss = 0.25386114\n",
      "Iteration 16, loss = 0.24643163\n",
      "Iteration 12, loss = 0.29699857\n",
      "Iteration 12, loss = 0.31105716\n",
      "Iteration 1, loss = 4.73702862\n",
      "Iteration 32, loss = 0.20202635\n",
      "Iteration 47, loss = 0.17552814\n",
      "Iteration 16, loss = 0.26569214\n",
      "Iteration 26, loss = 0.21795933\n",
      "Iteration 21, loss = 0.24801357\n",
      "Iteration 6, loss = 0.41857423\n",
      "Iteration 17, loss = 0.25262780\n",
      "Iteration 13, loss = 0.29927173\n",
      "Iteration 33, loss = 0.19645861\n",
      "Iteration 13, loss = 0.29935319\n",
      "Iteration 2, loss = 4.15863204\n",
      "Iteration 48, loss = 0.18510210\n",
      "Iteration 27, loss = 0.21885026\n",
      "Iteration 17, loss = 0.25082506\n",
      "Iteration 18, loss = 0.25229328\n",
      "Iteration 22, loss = 0.24586707\n",
      "Iteration 7, loss = 0.38943900\n",
      "Iteration 14, loss = 0.28286728\n",
      "Iteration 18, loss = 0.24449729\n",
      "Iteration 19, loss = 0.25280012\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 0.27680265\n",
      "Iteration 3, loss = 2.55033872\n",
      "Iteration 28, loss = 0.20801523\n",
      "Iteration 34, loss = 0.18959246\n",
      "Iteration 8, loss = 0.36513957\n",
      "Iteration 49, loss = 0.17694143\n",
      "Iteration 15, loss = 0.27372789\n",
      "Iteration 23, loss = 0.24655087\n",
      "Iteration 19, loss = 0.24424914\n",
      "Iteration 15, loss = 0.26995777\n",
      "Iteration 35, loss = 0.19028717\n",
      "Iteration 9, loss = 0.34861747\n",
      "Iteration 50, loss = 0.16953690\n",
      "Iteration 29, loss = 0.20637515\n",
      "Iteration 1, loss = 4.72764618\n",
      "Iteration 4, loss = 0.82340107\n",
      "Iteration 51, loss = 0.17501379\n",
      "Iteration 16, loss = 0.27220838\n",
      "Iteration 30, loss = 0.20422341\n",
      "Iteration 24, loss = 0.23943882\n",
      "Iteration 36, loss = 0.19292066\n",
      "Iteration 16, loss = 0.26223797\n",
      "Iteration 20, loss = 0.23907268\n",
      "Iteration 2, loss = 4.11220071\n",
      "Iteration 5, loss = 0.56462522\n",
      "Iteration 10, loss = 0.33664118\n",
      "Iteration 52, loss = 0.16741301\n",
      "Iteration 37, loss = 0.18832059\n",
      "Iteration 17, loss = 0.25631445\n",
      "Iteration 31, loss = 0.20235674\n",
      "Iteration 25, loss = 0.23537513\n",
      "Iteration 17, loss = 0.25419008\n",
      "Iteration 21, loss = 0.24034671\n",
      "Iteration 6, loss = 0.49644746\n",
      "Iteration 11, loss = 0.32644123\n",
      "Iteration 3, loss = 2.46032869\n",
      "Iteration 53, loss = 0.16838441\n",
      "Iteration 38, loss = 0.18320802\n",
      "Iteration 18, loss = 0.24524522\n",
      "Iteration 7, loss = 0.44920860\n",
      "Iteration 4, loss = 0.79988482\n",
      "Iteration 32, loss = 0.20605283\n",
      "Iteration 18, loss = 0.26476121\n",
      "Iteration 26, loss = 0.22982958\n",
      "Iteration 12, loss = 0.30782476\n",
      "Iteration 22, loss = 0.23880517\n",
      "Iteration 54, loss = 0.16467478\n",
      "Iteration 39, loss = 0.18019933\n",
      "Iteration 33, loss = 0.20588143\n",
      "Iteration 19, loss = 0.24549933\n",
      "Iteration 13, loss = 0.29985381\n",
      "Iteration 5, loss = 0.56558610\n",
      "Iteration 23, loss = 0.22392104\n",
      "Iteration 8, loss = 0.42070528\n",
      "Iteration 27, loss = 0.22652824\n",
      "Iteration 19, loss = 0.24705119\n",
      "Iteration 55, loss = 0.16652993\n",
      "Iteration 40, loss = 0.17373043\n",
      "Iteration 14, loss = 0.29292810\n",
      "Iteration 24, loss = 0.22511276\n",
      "Iteration 6, loss = 0.49830062\n",
      "Iteration 20, loss = 0.22737807\n",
      "Iteration 34, loss = 0.19287070\n",
      "Iteration 9, loss = 0.39943349\n",
      "Iteration 20, loss = 0.24310829\n",
      "Iteration 28, loss = 0.22053747\n",
      "Iteration 41, loss = 0.17916555\n",
      "Iteration 15, loss = 0.29278672\n",
      "Iteration 56, loss = 0.16692429\n",
      "Iteration 21, loss = 0.23862826\n",
      "Iteration 35, loss = 0.19565320\n",
      "Iteration 25, loss = 0.21649002\n",
      "Iteration 21, loss = 0.24613373\n",
      "Iteration 7, loss = 0.45732750\n",
      "Iteration 29, loss = 0.21982674\n",
      "Iteration 10, loss = 0.37977503\n",
      "Iteration 42, loss = 0.17551610\n",
      "Iteration 16, loss = 0.27868664\n",
      "Iteration 57, loss = 0.16386570\n",
      "Iteration 26, loss = 0.22174487\n",
      "Iteration 30, loss = 0.21866020\n",
      "Iteration 22, loss = 0.24291394\n",
      "Iteration 22, loss = 0.23241109\n",
      "Iteration 8, loss = 0.42267926\n",
      "Iteration 36, loss = 0.19731643\n",
      "Iteration 11, loss = 0.35728954\n",
      "Iteration 17, loss = 0.27054299\n",
      "Iteration 58, loss = 0.16182411\n",
      "Iteration 43, loss = 0.17122224\n",
      "Iteration 31, loss = 0.21456303\n",
      "Iteration 23, loss = 0.21835842\n",
      "Iteration 23, loss = 0.22698025\n",
      "Iteration 37, loss = 0.18736972\n",
      "Iteration 9, loss = 0.40113382\n",
      "Iteration 27, loss = 0.21678110\n",
      "Iteration 12, loss = 0.35222196\n",
      "Iteration 18, loss = 0.26460796\n",
      "Iteration 59, loss = 0.15565544\n",
      "Iteration 32, loss = 0.20934893\n",
      "Iteration 44, loss = 0.16892659\n",
      "Iteration 24, loss = 0.23347448\n",
      "Iteration 38, loss = 0.18092027\n",
      "Iteration 10, loss = 0.38361853\n",
      "Iteration 24, loss = 0.22653784\n",
      "Iteration 13, loss = 0.33213460\n",
      "Iteration 60, loss = 0.15705923\n",
      "Iteration 28, loss = 0.20864259\n",
      "Iteration 19, loss = 0.25874951\n",
      "Iteration 33, loss = 0.20540115\n",
      "Iteration 39, loss = 0.17911676\n",
      "Iteration 45, loss = 0.16913860\n",
      "Iteration 25, loss = 0.22702645\n",
      "Iteration 25, loss = 0.22237469\n",
      "Iteration 61, loss = 0.16261278\n",
      "Iteration 11, loss = 0.36140454\n",
      "Iteration 29, loss = 0.20366525\n",
      "Iteration 14, loss = 0.31714025\n",
      "Iteration 20, loss = 0.25962555\n",
      "Iteration 26, loss = 0.21617860\n",
      "Iteration 46, loss = 0.16784194\n",
      "Iteration 40, loss = 0.18197684\n",
      "Iteration 62, loss = 0.15269568\n",
      "Iteration 26, loss = 0.22484991\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 34, loss = 0.20391051\n",
      "Iteration 12, loss = 0.34683170\n",
      "Iteration 30, loss = 0.19943538\n",
      "Iteration 15, loss = 0.30734920\n",
      "Iteration 21, loss = 0.25225299\n",
      "Iteration 27, loss = 0.21931307\n",
      "Iteration 63, loss = 0.15606227\n",
      "Iteration 47, loss = 0.15816221\n",
      "Iteration 41, loss = 0.17092916\n",
      "Iteration 35, loss = 0.20080552\n",
      "Iteration 31, loss = 0.20087185\n",
      "Iteration 13, loss = 0.33140660\n",
      "Iteration 16, loss = 0.29623054\n",
      "Iteration 1, loss = 4.73327485\n",
      "Iteration 64, loss = 0.14527630\n",
      "Iteration 22, loss = 0.24607339\n",
      "Iteration 42, loss = 0.16998249\n",
      "Iteration 28, loss = 0.22976798\n",
      "Iteration 48, loss = 0.16098445\n",
      "Iteration 36, loss = 0.19546633\n",
      "Iteration 14, loss = 0.33389070\n",
      "Iteration 17, loss = 0.29296658\n",
      "Iteration 32, loss = 0.19665914\n",
      "Iteration 65, loss = 0.15013183\n",
      "Iteration 43, loss = 0.17286147\n",
      "Iteration 23, loss = 0.25553819\n",
      "Iteration 49, loss = 0.16045827\n",
      "Iteration 2, loss = 4.11695990\n",
      "Iteration 29, loss = 0.21155262\n",
      "Iteration 37, loss = 0.19455443\n",
      "Iteration 18, loss = 0.28673452\n",
      "Iteration 33, loss = 0.20149914\n",
      "Iteration 15, loss = 0.31495689\n",
      "Iteration 50, loss = 0.15831266\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 66, loss = 0.15571403\n",
      "Iteration 24, loss = 0.23737517\n",
      "Iteration 30, loss = 0.20565788\n",
      "Iteration 44, loss = 0.17203888\n",
      "Iteration 38, loss = 0.19551468\n",
      "Iteration 19, loss = 0.27645688\n",
      "Iteration 3, loss = 2.41610458\n",
      "Iteration 34, loss = 0.19267039\n",
      "Iteration 16, loss = 0.30415501\n",
      "Iteration 31, loss = 0.19631643\n",
      "Iteration 25, loss = 0.23923397\n",
      "Iteration 39, loss = 0.19101339\n",
      "Iteration 45, loss = 0.16917417\n",
      "Iteration 67, loss = 0.13824235\n",
      "Iteration 4, loss = 0.77908013\n",
      "Iteration 20, loss = 0.26875716\n",
      "Iteration 35, loss = 0.18643717\n",
      "Iteration 1, loss = 4.70426280\n",
      "Iteration 32, loss = 0.19906506\n",
      "Iteration 17, loss = 0.29018791\n",
      "Iteration 40, loss = 0.19125577\n",
      "Iteration 26, loss = 0.23684028\n",
      "Iteration 46, loss = 0.16150870\n",
      "Iteration 68, loss = 0.13433786\n",
      "Iteration 5, loss = 0.54423025\n",
      "Iteration 21, loss = 0.26079241\n",
      "Iteration 36, loss = 0.18573586\n",
      "Iteration 33, loss = 0.19699203\n",
      "Iteration 2, loss = 4.00086866\n",
      "Iteration 47, loss = 0.16009806\n",
      "Iteration 18, loss = 0.28827268\n",
      "Iteration 41, loss = 0.18771266\n",
      "Iteration 27, loss = 0.23178372\n",
      "Iteration 6, loss = 0.48360828\n",
      "Iteration 69, loss = 0.14379989\n",
      "Iteration 34, loss = 0.19953249\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 37, loss = 0.17895521\n",
      "Iteration 22, loss = 0.25857703\n",
      "Iteration 48, loss = 0.15767687\n",
      "Iteration 3, loss = 2.25190384\n",
      "Iteration 19, loss = 0.28186746\n",
      "Iteration 42, loss = 0.18957176\n",
      "Iteration 28, loss = 0.21931322\n",
      "Iteration 7, loss = 0.44853599\n",
      "Iteration 70, loss = 0.13606234\n",
      "Iteration 38, loss = 0.17519010\n",
      "Iteration 23, loss = 0.25243759\n",
      "Iteration 49, loss = 0.15797585\n",
      "Iteration 1, loss = 4.72045295\n",
      "Iteration 4, loss = 0.75599955\n",
      "Iteration 20, loss = 0.28685382\n",
      "Iteration 43, loss = 0.17985907\n",
      "Iteration 29, loss = 0.22629257\n",
      "Iteration 39, loss = 0.17358828\n",
      "Iteration 8, loss = 0.41731250\n",
      "Iteration 71, loss = 0.13805858\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 0.25199397\n",
      "Iteration 50, loss = 0.16131966\n",
      "Iteration 44, loss = 0.18114125\n",
      "Iteration 2, loss = 4.08410531\n",
      "Iteration 21, loss = 0.26786564\n",
      "Iteration 5, loss = 0.56546849\n",
      "Iteration 30, loss = 0.22274614\n",
      "Iteration 40, loss = 0.17682946\n",
      "Iteration 9, loss = 0.38732141\n",
      "Iteration 51, loss = 0.15882590\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 25, loss = 0.24841277\n",
      "Iteration 45, loss = 0.18010598\n",
      "Iteration 3, loss = 2.40390696\n",
      "Iteration 22, loss = 0.26589139\n",
      "Iteration 6, loss = 0.50237749\n",
      "Iteration 1, loss = 4.72813715\n",
      "Iteration 31, loss = 0.21683149\n",
      "Iteration 41, loss = 0.17667144\n",
      "Iteration 10, loss = 0.36963147\n",
      "Iteration 26, loss = 0.24636010\n",
      "Iteration 46, loss = 0.17825082\n",
      "Iteration 23, loss = 0.25757777\n",
      "Iteration 1, loss = 4.73270782\n",
      "Iteration 4, loss = 0.80969302\n",
      "Iteration 7, loss = 0.45678453\n",
      "Iteration 2, loss = 4.11454336\n",
      "Iteration 32, loss = 0.20986830\n",
      "Iteration 42, loss = 0.17431417\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.35787322\n",
      "Iteration 47, loss = 0.17557259\n",
      "Iteration 27, loss = 0.24163970\n",
      "Iteration 24, loss = 0.25825990\n",
      "Iteration 2, loss = 4.13573962\n",
      "Iteration 5, loss = 0.59589557\n",
      "Iteration 8, loss = 0.42669298\n",
      "Iteration 12, loss = 0.34148665\n",
      "Iteration 3, loss = 2.46801446\n",
      "Iteration 33, loss = 0.19944425\n",
      "Iteration 25, loss = 0.24743204\n",
      "Iteration 48, loss = 0.17142530\n",
      "Iteration 28, loss = 0.24196753\n",
      "Iteration 6, loss = 0.52730501\n",
      "Iteration 3, loss = 2.52047709\n",
      "Iteration 4, loss = 0.79907954\n",
      "Iteration 9, loss = 0.40079997\n",
      "Iteration 1, loss = 4.72175890\n",
      "Iteration 13, loss = 0.32497408\n",
      "Iteration 34, loss = 0.20385606\n",
      "Iteration 29, loss = 0.22689028\n",
      "Iteration 26, loss = 0.25138821\n",
      "Iteration 49, loss = 0.16536146\n",
      "Iteration 4, loss = 0.82308610\n",
      "Iteration 7, loss = 0.47819166\n",
      "Iteration 10, loss = 0.38653687\n",
      "Iteration 2, loss = 4.08863465\n",
      "Iteration 5, loss = 0.57140792\n",
      "Iteration 14, loss = 0.31643726\n",
      "Iteration 27, loss = 0.24282824\n",
      "Iteration 35, loss = 0.20634249\n",
      "Iteration 30, loss = 0.22671542\n",
      "Iteration 50, loss = 0.16252591\n",
      "Iteration 5, loss = 0.56952476\n",
      "Iteration 8, loss = 0.44768426\n",
      "Iteration 6, loss = 0.51235775\n",
      "Iteration 15, loss = 0.31003520\n",
      "Iteration 11, loss = 0.36613443\n",
      "Iteration 36, loss = 0.19761595\n",
      "Iteration 28, loss = 0.23876401\n",
      "Iteration 31, loss = 0.22406729\n",
      "Iteration 6, loss = 0.51028700\n",
      "Iteration 3, loss = 2.40597396\n",
      "Iteration 9, loss = 0.41955949\n",
      "Iteration 51, loss = 0.16285040\n",
      "Iteration 37, loss = 0.19988705\n",
      "Iteration 7, loss = 0.45766107\n",
      "Iteration 16, loss = 0.30065588\n",
      "Iteration 12, loss = 0.34897947\n",
      "Iteration 52, loss = 0.16448787\n",
      "Iteration 32, loss = 0.21807724\n",
      "Iteration 7, loss = 0.46065154\n",
      "Iteration 29, loss = 0.23443857\n",
      "Iteration 8, loss = 0.42467693\n",
      "Iteration 4, loss = 0.78267787\n",
      "Iteration 17, loss = 0.28622010\n",
      "Iteration 38, loss = 0.19670149\n",
      "Iteration 10, loss = 0.40328750\n",
      "Iteration 13, loss = 0.33747893\n",
      "Iteration 53, loss = 0.17320172\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.43622295\n",
      "Iteration 5, loss = 0.56149446\n",
      "Iteration 30, loss = 0.23214623\n",
      "Iteration 33, loss = 0.21461397\n",
      "Iteration 9, loss = 0.39859599\n",
      "Iteration 18, loss = 0.28862595\n",
      "Iteration 11, loss = 0.37368290\n",
      "Iteration 9, loss = 0.41178692\n",
      "Iteration 39, loss = 0.18747652\n",
      "Iteration 31, loss = 0.23286400\n",
      "Iteration 14, loss = 0.32056169\n",
      "Iteration 10, loss = 0.37900439\n",
      "Iteration 34, loss = 0.20902196\n",
      "Iteration 6, loss = 0.50465477\n",
      "Iteration 1, loss = 4.71374466\n",
      "Iteration 19, loss = 0.27685808\n",
      "Iteration 10, loss = 0.38038666\n",
      "Iteration 15, loss = 0.31375765\n",
      "Iteration 12, loss = 0.36610200\n",
      "Iteration 32, loss = 0.22497690\n",
      "Iteration 11, loss = 0.35988303\n",
      "Iteration 40, loss = 0.19287558\n",
      "Iteration 35, loss = 0.21543471\n",
      "Iteration 7, loss = 0.45569840\n",
      "Iteration 20, loss = 0.27757268\n",
      "Iteration 11, loss = 0.36734506\n",
      "Iteration 2, loss = 4.06323871\n",
      "Iteration 16, loss = 0.31182568\n",
      "Iteration 21, loss = 0.26495978\n",
      "Iteration 12, loss = 0.34241345\n",
      "Iteration 13, loss = 0.35458534\n",
      "Iteration 33, loss = 0.22823148\n",
      "Iteration 41, loss = 0.18891864\n",
      "Iteration 36, loss = 0.21172378\n",
      "Iteration 8, loss = 0.42448021\n",
      "Iteration 12, loss = 0.34967897\n",
      "Iteration 22, loss = 0.26719738\n",
      "Iteration 13, loss = 0.33485487\n",
      "Iteration 14, loss = 0.33441814\n",
      "Iteration 3, loss = 2.36964122\n",
      "Iteration 17, loss = 0.30046509\n",
      "Iteration 34, loss = 0.21790618\n",
      "Iteration 37, loss = 0.21540803\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 0.19476467\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 0.39577771\n",
      "Iteration 23, loss = 0.26334710\n",
      "Iteration 13, loss = 0.33934713\n",
      "Iteration 14, loss = 0.32121899\n",
      "Iteration 18, loss = 0.29590613\n",
      "Iteration 15, loss = 0.32707334\n",
      "Iteration 4, loss = 0.78898599\n",
      "Iteration 35, loss = 0.21725691\n",
      "Iteration 10, loss = 0.39132192\n",
      "Iteration 15, loss = 0.30982579\n",
      "Iteration 24, loss = 0.26507406\n",
      "Iteration 1, loss = 4.72828966\n",
      "Iteration 14, loss = 0.33132645\n",
      "Iteration 19, loss = 0.28057314\n",
      "Iteration 1, loss = 4.73343001\n",
      "Iteration 11, loss = 0.36100506\n",
      "Iteration 5, loss = 0.57386505\n",
      "Iteration 16, loss = 0.32149677\n",
      "Iteration 16, loss = 0.29669841\n",
      "Iteration 25, loss = 0.25104815\n",
      "Iteration 36, loss = 0.21791230\n",
      "Iteration 20, loss = 0.28580276\n",
      "Iteration 2, loss = 4.13338679\n",
      "Iteration 15, loss = 0.31103984\n",
      "Iteration 12, loss = 0.35589181\n",
      "Iteration 6, loss = 0.50185035\n",
      "Iteration 17, loss = 0.28976135\n",
      "Iteration 17, loss = 0.30761801\n",
      "Iteration 37, loss = 0.22045109\n",
      "Iteration 2, loss = 4.13806187\n",
      "Iteration 3, loss = 2.48701813\n",
      "Iteration 26, loss = 0.25232749\n",
      "Iteration 21, loss = 0.27226708\n",
      "Iteration 13, loss = 0.33733802\n",
      "Iteration 16, loss = 0.30449813\n",
      "Iteration 18, loss = 0.28641209\n",
      "Iteration 22, loss = 0.27311745\n",
      "Iteration 18, loss = 0.30069616\n",
      "Iteration 7, loss = 0.46425145\n",
      "Iteration 4, loss = 0.80958967\n",
      "Iteration 3, loss = 2.56632802\n",
      "Iteration 23, loss = 0.26243702\n",
      "Iteration 17, loss = 0.30184729\n",
      "Iteration 38, loss = 0.21329479\n",
      "Iteration 14, loss = 0.32418695\n",
      "Iteration 27, loss = 0.24294186\n",
      "Iteration 8, loss = 0.43259215\n",
      "Iteration 19, loss = 0.28572148\n",
      "Iteration 4, loss = 0.87059456\n",
      "Iteration 19, loss = 0.29905441\n",
      "Iteration 5, loss = 0.57226379\n",
      "Iteration 15, loss = 0.31231045\n",
      "Iteration 24, loss = 0.26469369\n",
      "Iteration 18, loss = 0.29240923\n",
      "Iteration 6, loss = 0.51211217\n",
      "Iteration 39, loss = 0.21346547\n",
      "Iteration 28, loss = 0.23517427\n",
      "Iteration 9, loss = 0.40088597\n",
      "Iteration 20, loss = 0.27810478\n",
      "Iteration 25, loss = 0.26277493\n",
      "Iteration 19, loss = 0.27798595\n",
      "Iteration 20, loss = 0.29559111\n",
      "Iteration 5, loss = 0.57124240\n",
      "Iteration 16, loss = 0.30599714\n",
      "Iteration 40, loss = 0.20513183\n",
      "Iteration 7, loss = 0.46288935\n",
      "Iteration 29, loss = 0.23806118\n",
      "Iteration 26, loss = 0.25619812\n",
      "Iteration 20, loss = 0.28422140\n",
      "Iteration 10, loss = 0.38620780\n",
      "Iteration 21, loss = 0.27199797\n",
      "Iteration 6, loss = 0.50054661\n",
      "Iteration 21, loss = 0.28700774\n",
      "Iteration 17, loss = 0.29395284\n",
      "Iteration 41, loss = 0.20497596\n",
      "Iteration 8, loss = 0.43001559\n",
      "Iteration 22, loss = 0.25938199\n",
      "Iteration 21, loss = 0.27655729\n",
      "Iteration 30, loss = 0.23515646\n",
      "Iteration 27, loss = 0.24993062\n",
      "Iteration 11, loss = 0.36851943\n",
      "Iteration 7, loss = 0.45858109\n",
      "Iteration 9, loss = 0.40677037\n",
      "Iteration 18, loss = 0.28643205\n",
      "Iteration 22, loss = 0.28130384\n",
      "Iteration 42, loss = 0.21060679\n",
      "Iteration 23, loss = 0.25859857\n",
      "Iteration 22, loss = 0.26958041\n",
      "Iteration 31, loss = 0.22972912\n",
      "Iteration 12, loss = 0.35263012\n",
      "Iteration 19, loss = 0.27954927\n",
      "Iteration 28, loss = 0.24593432\n",
      "Iteration 23, loss = 0.26997866\n",
      "Iteration 10, loss = 0.39321033\n",
      "Iteration 8, loss = 0.42861963\n",
      "Iteration 24, loss = 0.25654670\n",
      "Iteration 43, loss = 0.20910404\n",
      "Iteration 32, loss = 0.22751695\n",
      "Iteration 13, loss = 0.34347894\n",
      "Iteration 23, loss = 0.26177945\n",
      "Iteration 20, loss = 0.27825981\n",
      "Iteration 29, loss = 0.24481192\n",
      "Iteration 24, loss = 0.26651742\n",
      "Iteration 9, loss = 0.40537556\n",
      "Iteration 11, loss = 0.37308239\n",
      "Iteration 25, loss = 0.24671172\n",
      "Iteration 44, loss = 0.20023723\n",
      "Iteration 33, loss = 0.22330990\n",
      "Iteration 14, loss = 0.32242337\n",
      "Iteration 25, loss = 0.26781253\n",
      "Iteration 24, loss = 0.25694858\n",
      "Iteration 30, loss = 0.24391971\n",
      "Iteration 26, loss = 0.25220213\n",
      "Iteration 10, loss = 0.38381278\n",
      "Iteration 21, loss = 0.27526082\n",
      "Iteration 12, loss = 0.35752839\n",
      "Iteration 34, loss = 0.22401952\n",
      "Iteration 45, loss = 0.19577096\n",
      "Iteration 26, loss = 0.26084074\n",
      "Iteration 15, loss = 0.32233509\n",
      "Iteration 31, loss = 0.23321457\n",
      "Iteration 25, loss = 0.25365916\n",
      "Iteration 27, loss = 0.23886678\n",
      "Iteration 13, loss = 0.34531336\n",
      "Iteration 22, loss = 0.27124748\n",
      "Iteration 11, loss = 0.35575680\n",
      "Iteration 35, loss = 0.22135555\n",
      "Iteration 27, loss = 0.26724393\n",
      "Iteration 46, loss = 0.19364512\n",
      "Iteration 16, loss = 0.31441520\n",
      "Iteration 32, loss = 0.24061517\n",
      "Iteration 26, loss = 0.24956502\n",
      "Iteration 36, loss = 0.21868080\n",
      "Iteration 28, loss = 0.23283979\n",
      "Iteration 14, loss = 0.33005089\n",
      "Iteration 23, loss = 0.26493144\n",
      "Iteration 12, loss = 0.34781651\n",
      "Iteration 28, loss = 0.26501460\n",
      "Iteration 17, loss = 0.30774372\n",
      "Iteration 47, loss = 0.18989518\n",
      "Iteration 33, loss = 0.23209565\n",
      "Iteration 27, loss = 0.24543511\n",
      "Iteration 37, loss = 0.21746065\n",
      "Iteration 13, loss = 0.33505149\n",
      "Iteration 15, loss = 0.33116505\n",
      "Iteration 29, loss = 0.22950400\n",
      "Iteration 29, loss = 0.24916949\n",
      "Iteration 24, loss = 0.25817001\n",
      "Iteration 18, loss = 0.28868323\n",
      "Iteration 28, loss = 0.23751707\n",
      "Iteration 25, loss = 0.25318615\n",
      "Iteration 48, loss = 0.19200963\n",
      "Iteration 34, loss = 0.22585602\n",
      "Iteration 38, loss = 0.21236803\n",
      "Iteration 30, loss = 0.24350871\n",
      "Iteration 16, loss = 0.32037107\n",
      "Iteration 14, loss = 0.32171670\n",
      "Iteration 30, loss = 0.22637225\n",
      "Iteration 49, loss = 0.18326191\n",
      "Iteration 26, loss = 0.25123784\n",
      "Iteration 29, loss = 0.24225001\n",
      "Iteration 19, loss = 0.28262837\n",
      "Iteration 50, loss = 0.18557805\n",
      "Iteration 31, loss = 0.24310801\n",
      "Iteration 39, loss = 0.20602839\n",
      "Iteration 35, loss = 0.22267327\n",
      "Iteration 17, loss = 0.31686898\n",
      "Iteration 15, loss = 0.31497050\n",
      "Iteration 31, loss = 0.22362084\n",
      "Iteration 27, loss = 0.24342771\n",
      "Iteration 20, loss = 0.27579274\n",
      "Iteration 30, loss = 0.23618450\n",
      "Iteration 32, loss = 0.24816487\n",
      "Iteration 51, loss = 0.18592841\n",
      "Iteration 18, loss = 0.30678723\n",
      "Iteration 40, loss = 0.21060244\n",
      "Iteration 16, loss = 0.29852853\n",
      "Iteration 36, loss = 0.22465052\n",
      "Iteration 32, loss = 0.22741997\n",
      "Iteration 21, loss = 0.27892154\n",
      "Iteration 31, loss = 0.22623661\n",
      "Iteration 17, loss = 0.29070090\n",
      "Iteration 52, loss = 0.18943354\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 28, loss = 0.24921772\n",
      "Iteration 33, loss = 0.24089216\n",
      "Iteration 19, loss = 0.29151958\n",
      "Iteration 37, loss = 0.21761452\n",
      "Iteration 41, loss = 0.20486605\n",
      "Iteration 22, loss = 0.26508973\n",
      "Iteration 32, loss = 0.22617817\n",
      "Iteration 33, loss = 0.22255772\n",
      "Iteration 18, loss = 0.28665082\n",
      "Iteration 38, loss = 0.21531824\n",
      "Iteration 34, loss = 0.23667325\n",
      "Iteration 20, loss = 0.28461676\n",
      "Iteration 33, loss = 0.22996454\n",
      "Iteration 29, loss = 0.23607335\n",
      "Iteration 19, loss = 0.27203106\n",
      "Iteration 1, loss = 4.71338207\n",
      "Iteration 23, loss = 0.27379949\n",
      "Iteration 34, loss = 0.21727640\n",
      "Iteration 42, loss = 0.20322111\n",
      "Iteration 21, loss = 0.28524767\n",
      "Iteration 39, loss = 0.21293297\n",
      "Iteration 34, loss = 0.22367748\n",
      "Iteration 30, loss = 0.23412502\n",
      "Iteration 35, loss = 0.24177415\n",
      "Iteration 20, loss = 0.27951920\n",
      "Iteration 35, loss = 0.21566799\n",
      "Iteration 24, loss = 0.25666345\n",
      "Iteration 2, loss = 4.06186835\n",
      "Iteration 43, loss = 0.20103504\n",
      "Iteration 22, loss = 0.28318611\n",
      "Iteration 21, loss = 0.26900480\n",
      "Iteration 40, loss = 0.21088795\n",
      "Iteration 35, loss = 0.22119508\n",
      "Iteration 31, loss = 0.23528648\n",
      "Iteration 36, loss = 0.23294121\n",
      "Iteration 3, loss = 2.39219775\n",
      "Iteration 36, loss = 0.20916721\n",
      "Iteration 25, loss = 0.25506187\n",
      "Iteration 23, loss = 0.28057104\n",
      "Iteration 32, loss = 0.23061815\n",
      "Iteration 22, loss = 0.26067866\n",
      "Iteration 36, loss = 0.22006354\n",
      "Iteration 44, loss = 0.20374551\n",
      "Iteration 41, loss = 0.21335529\n",
      "Iteration 37, loss = 0.21285581\n",
      "Iteration 4, loss = 0.81017670\n",
      "Iteration 37, loss = 0.23280829\n",
      "Iteration 23, loss = 0.25051447\n",
      "Iteration 26, loss = 0.25507379\n",
      "Iteration 33, loss = 0.22538549\n",
      "Iteration 24, loss = 0.27381972\n",
      "Iteration 37, loss = 0.21523241\n",
      "Iteration 45, loss = 0.19631048\n",
      "Iteration 42, loss = 0.21153487\n",
      "Iteration 38, loss = 0.23540989\n",
      "Iteration 24, loss = 0.24683595\n",
      "Iteration 5, loss = 0.57685959\n",
      "Iteration 38, loss = 0.21155143\n",
      "Iteration 25, loss = 0.26110272\n",
      "Iteration 27, loss = 0.24455176\n",
      "Iteration 34, loss = 0.22130895\n",
      "Iteration 38, loss = 0.21616288\n",
      "Iteration 43, loss = 0.21030246\n",
      "Iteration 46, loss = 0.19669644\n",
      "Iteration 6, loss = 0.50491682\n",
      "Iteration 39, loss = 0.21210870\n",
      "Iteration 39, loss = 0.22718388\n",
      "Iteration 25, loss = 0.25052369\n",
      "Iteration 39, loss = 0.20195191\n",
      "Iteration 26, loss = 0.26162871\n",
      "Iteration 44, loss = 0.20943330\n",
      "Iteration 35, loss = 0.22054044\n",
      "Iteration 28, loss = 0.24817460\n",
      "Iteration 47, loss = 0.19408428\n",
      "Iteration 40, loss = 0.19699037\n",
      "Iteration 26, loss = 0.25032835\n",
      "Iteration 7, loss = 0.46502928\n",
      "Iteration 40, loss = 0.21534131\n",
      "Iteration 40, loss = 0.22300701\n",
      "Iteration 36, loss = 0.21362908\n",
      "Iteration 27, loss = 0.25910575\n",
      "Iteration 29, loss = 0.24093012\n",
      "Iteration 41, loss = 0.20354521\n",
      "Iteration 45, loss = 0.20346070\n",
      "Iteration 48, loss = 0.19193153\n",
      "Iteration 27, loss = 0.24582745\n",
      "Iteration 8, loss = 0.42930775\n",
      "Iteration 41, loss = 0.20697989\n",
      "Iteration 41, loss = 0.21827304\n",
      "Iteration 37, loss = 0.21611823\n",
      "Iteration 42, loss = 0.19755095\n",
      "Iteration 46, loss = 0.20537874\n",
      "Iteration 30, loss = 0.23981048\n",
      "Iteration 28, loss = 0.25507388\n",
      "Iteration 49, loss = 0.18583240\n",
      "Iteration 28, loss = 0.23567512\n",
      "Iteration 9, loss = 0.39942976\n",
      "Iteration 42, loss = 0.20498158\n",
      "Iteration 42, loss = 0.22028725\n",
      "Iteration 38, loss = 0.22125356\n",
      "Iteration 50, loss = 0.19268375\n",
      "Iteration 31, loss = 0.24016420\n",
      "Iteration 47, loss = 0.19608869\n",
      "Iteration 43, loss = 0.19896003\n",
      "Iteration 29, loss = 0.24951709\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 0.37900978\n",
      "Iteration 29, loss = 0.23423277\n",
      "Iteration 43, loss = 0.22072034\n",
      "Iteration 51, loss = 0.19027504\n",
      "Iteration 39, loss = 0.21856280\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 0.20169797\n",
      "Iteration 30, loss = 0.24192842\n",
      "Iteration 48, loss = 0.19833061\n",
      "Iteration 32, loss = 0.23256458\n",
      "Iteration 52, loss = 0.18344675\n",
      "Iteration 30, loss = 0.22890052\n",
      "Iteration 44, loss = 0.21970640\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.36873019\n",
      "Iteration 1, loss = 4.71175913\n",
      "Iteration 44, loss = 0.20415308\n",
      "Iteration 1, loss = 4.69915231\n",
      "Iteration 31, loss = 0.24324694\n",
      "Iteration 33, loss = 0.22951273\n",
      "Iteration 49, loss = 0.19751458\n",
      "Iteration 12, loss = 0.34652053\n",
      "Iteration 31, loss = 0.22484860\n",
      "Iteration 53, loss = 0.19007984\n",
      "Iteration 2, loss = 3.99843896\n",
      "Iteration 45, loss = 0.19335461\n",
      "Iteration 32, loss = 0.24540126\n",
      "Iteration 2, loss = 4.03550159\n",
      "Iteration 50, loss = 0.18875651\n",
      "Iteration 34, loss = 0.22142455\n",
      "Iteration 1, loss = 4.70069094\n",
      "Iteration 13, loss = 0.33559520\n",
      "Iteration 54, loss = 0.18656625\n",
      "Iteration 3, loss = 2.27630934\n",
      "Iteration 32, loss = 0.22329139\n",
      "Iteration 46, loss = 0.19478138\n",
      "Iteration 33, loss = 0.23584468\n",
      "Iteration 51, loss = 0.19448587\n",
      "Iteration 3, loss = 2.28398991\n",
      "Iteration 2, loss = 3.99960772\n",
      "Iteration 14, loss = 0.32370225\n",
      "Iteration 35, loss = 0.22462593\n",
      "Iteration 4, loss = 0.75733461\n",
      "Iteration 55, loss = 0.18314938\n",
      "Iteration 47, loss = 0.19608965\n",
      "Iteration 34, loss = 0.23327627\n",
      "Iteration 33, loss = 0.22242719\n",
      "Iteration 52, loss = 0.18962008\n",
      "Iteration 4, loss = 0.75190974\n",
      "Iteration 56, loss = 0.18631408\n",
      "Iteration 15, loss = 0.31693041\n",
      "Iteration 3, loss = 2.25991011\n",
      "Iteration 36, loss = 0.22460200\n",
      "Iteration 48, loss = 0.19032169\n",
      "Iteration 34, loss = 0.21621048\n",
      "Iteration 5, loss = 0.54896826\n",
      "Iteration 35, loss = 0.23657931\n",
      "Iteration 53, loss = 0.18353110\n",
      "Iteration 57, loss = 0.17927735\n",
      "Iteration 5, loss = 0.55451893\n",
      "Iteration 16, loss = 0.30166457\n",
      "Iteration 4, loss = 0.77758833\n",
      "Iteration 6, loss = 0.49908263\n",
      "Iteration 37, loss = 0.21548592\n",
      "Iteration 49, loss = 0.19310518\n",
      "Iteration 35, loss = 0.21823125\n",
      "Iteration 36, loss = 0.23301692\n",
      "Iteration 58, loss = 0.17984248\n",
      "Iteration 54, loss = 0.18517274\n",
      "Iteration 17, loss = 0.29104049\n",
      "Iteration 6, loss = 0.49607455\n",
      "Iteration 7, loss = 0.45081439\n",
      "Iteration 5, loss = 0.58044938\n",
      "Iteration 38, loss = 0.22148233\n",
      "Iteration 50, loss = 0.18880610\n",
      "Iteration 59, loss = 0.17860617\n",
      "Iteration 37, loss = 0.23174411\n",
      "Iteration 36, loss = 0.21804013\n",
      "Iteration 18, loss = 0.28868810\n",
      "Iteration 55, loss = 0.18574693\n",
      "Iteration 8, loss = 0.41317775\n",
      "Iteration 7, loss = 0.45231954\n",
      "Iteration 6, loss = 0.52672113\n",
      "Iteration 39, loss = 0.21245386\n",
      "Iteration 60, loss = 0.17579783\n",
      "Iteration 19, loss = 0.27900356\n",
      "Iteration 51, loss = 0.18548842\n",
      "Iteration 38, loss = 0.22260757\n",
      "Iteration 37, loss = 0.21399720\n",
      "Iteration 56, loss = 0.18553579\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 0.39066961\n",
      "Iteration 8, loss = 0.43000610\n",
      "Iteration 61, loss = 0.17483512\n",
      "Iteration 40, loss = 0.21194581\n",
      "Iteration 7, loss = 0.48379524\n",
      "Iteration 20, loss = 0.27473526\n",
      "Iteration 52, loss = 0.18534339\n",
      "Iteration 39, loss = 0.23209110\n",
      "Iteration 10, loss = 0.37119335\n",
      "Iteration 38, loss = 0.20991513\n",
      "Iteration 62, loss = 0.17309299\n",
      "Iteration 8, loss = 0.43897547\n",
      "Iteration 9, loss = 0.40137860\n",
      "Iteration 21, loss = 0.27284370\n",
      "Iteration 1, loss = 4.51589598\n",
      "Iteration 41, loss = 0.21301849\n",
      "Iteration 53, loss = 0.18764464\n",
      "Iteration 40, loss = 0.22776156\n",
      "Iteration 11, loss = 0.35247270\n",
      "Iteration 9, loss = 0.41965769\n",
      "Iteration 63, loss = 0.17507800\n",
      "Iteration 39, loss = 0.20720341\n",
      "Iteration 22, loss = 0.27522984\n",
      "Iteration 2, loss = 2.24472435\n",
      "Iteration 10, loss = 0.37609278\n",
      "Iteration 42, loss = 0.20545469\n",
      "Iteration 41, loss = 0.21670070\n",
      "Iteration 54, loss = 0.18595983\n",
      "Iteration 10, loss = 0.40183284\n",
      "Iteration 12, loss = 0.33626561\n",
      "Iteration 40, loss = 0.20214914\n",
      "Iteration 23, loss = 0.26562754\n",
      "Iteration 64, loss = 0.16708466\n",
      "Iteration 43, loss = 0.20818111\n",
      "Iteration 3, loss = 0.63899318\n",
      "Iteration 11, loss = 0.35700599\n",
      "Iteration 42, loss = 0.21725940\n",
      "Iteration 55, loss = 0.17928523\n",
      "Iteration 11, loss = 0.37832584\n",
      "Iteration 24, loss = 0.25431011\n",
      "Iteration 13, loss = 0.32745865\n",
      "Iteration 41, loss = 0.20286382\n",
      "Iteration 65, loss = 0.16173619\n",
      "Iteration 44, loss = 0.20312835\n",
      "Iteration 12, loss = 0.34604182\n",
      "Iteration 4, loss = 0.52625604\n",
      "Iteration 56, loss = 0.17537072\n",
      "Iteration 43, loss = 0.21183313\n",
      "Iteration 12, loss = 0.36131504\n",
      "Iteration 25, loss = 0.25057521\n",
      "Iteration 14, loss = 0.31569272\n",
      "Iteration 66, loss = 0.16184147\n",
      "Iteration 42, loss = 0.19919100\n",
      "Iteration 13, loss = 0.34310248\n",
      "Iteration 57, loss = 0.17545333\n",
      "Iteration 45, loss = 0.20854914\n",
      "Iteration 5, loss = 0.45282887\n",
      "Iteration 44, loss = 0.21632793\n",
      "Iteration 15, loss = 0.30838724\n",
      "Iteration 67, loss = 0.16813606\n",
      "Iteration 13, loss = 0.35439235\n",
      "Iteration 43, loss = 0.20159127\n",
      "Iteration 26, loss = 0.24888503\n",
      "Iteration 14, loss = 0.32612842\n",
      "Iteration 58, loss = 0.17538792\n",
      "Iteration 45, loss = 0.21038469\n",
      "Iteration 46, loss = 0.20010188\n",
      "Iteration 6, loss = 0.39336073\n",
      "Iteration 16, loss = 0.30333152\n",
      "Iteration 14, loss = 0.33961769\n",
      "Iteration 68, loss = 0.16726644\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 0.19654791\n",
      "Iteration 59, loss = 0.18036784\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 0.31528985\n",
      "Iteration 27, loss = 0.25018159\n",
      "Iteration 46, loss = 0.20770426\n",
      "Iteration 47, loss = 0.20370305\n",
      "Iteration 7, loss = 0.37912612\n",
      "Iteration 17, loss = 0.29288472\n",
      "Iteration 28, loss = 0.24324192\n",
      "Iteration 45, loss = 0.20239929\n",
      "Iteration 15, loss = 0.33013654\n",
      "Iteration 16, loss = 0.30370071\n",
      "Iteration 18, loss = 0.28799891\n",
      "Iteration 47, loss = 0.20956195\n",
      "Iteration 48, loss = 0.19925733\n",
      "Iteration 1, loss = 4.49618895\n",
      "Iteration 8, loss = 0.35611494\n",
      "Iteration 1, loss = 4.49397410\n",
      "Iteration 29, loss = 0.23876089\n",
      "Iteration 16, loss = 0.32088611\n",
      "Iteration 19, loss = 0.28121831\n",
      "Iteration 17, loss = 0.29490648\n",
      "Iteration 46, loss = 0.20320784\n",
      "Iteration 48, loss = 0.21188530\n",
      "Iteration 49, loss = 0.19076962\n",
      "Iteration 2, loss = 2.15126361\n",
      "Iteration 2, loss = 2.13839346\n",
      "Iteration 9, loss = 0.32902140\n",
      "Iteration 49, loss = 0.20866718\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 0.27110392\n",
      "Iteration 30, loss = 0.23333827\n",
      "Iteration 18, loss = 0.29255269\n",
      "Iteration 47, loss = 0.19342041\n",
      "Iteration 17, loss = 0.31111809\n",
      "Iteration 50, loss = 0.19114326\n",
      "Iteration 3, loss = 0.65615511\n",
      "Iteration 3, loss = 0.63644332\n",
      "Iteration 10, loss = 0.32803637\n",
      "Iteration 21, loss = 0.27021738\n",
      "Iteration 19, loss = 0.28227732\n",
      "Iteration 18, loss = 0.31569397\n",
      "Iteration 31, loss = 0.23427142\n",
      "Iteration 1, loss = 4.51048879\n",
      "Iteration 51, loss = 0.19830680\n",
      "Iteration 48, loss = 0.19256921\n",
      "Iteration 4, loss = 0.52955252\n",
      "Iteration 4, loss = 0.52094645\n",
      "Iteration 11, loss = 0.30694773\n",
      "Iteration 20, loss = 0.28240961\n",
      "Iteration 32, loss = 0.22900497\n",
      "Iteration 22, loss = 0.26084059\n",
      "Iteration 52, loss = 0.19558931\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 2.23957406\n",
      "Iteration 19, loss = 0.30351166\n",
      "Iteration 49, loss = 0.19541009\n",
      "Iteration 5, loss = 0.44275836\n",
      "Iteration 5, loss = 0.43337965\n",
      "Iteration 33, loss = 0.22459437\n",
      "Iteration 21, loss = 0.26830295\n",
      "Iteration 12, loss = 0.29762475\n",
      "Iteration 23, loss = 0.25761482\n",
      "Iteration 3, loss = 0.66794480\n",
      "Iteration 20, loss = 0.29971762\n",
      "Iteration 6, loss = 0.40494354\n",
      "Iteration 50, loss = 0.18851269\n",
      "Iteration 34, loss = 0.22221874\n",
      "Iteration 13, loss = 0.29089990\n",
      "Iteration 1, loss = 4.54020350\n",
      "Iteration 6, loss = 0.40651992\n",
      "Iteration 24, loss = 0.25766510\n",
      "Iteration 22, loss = 0.26894446\n",
      "Iteration 4, loss = 0.54082167\n",
      "Iteration 21, loss = 0.28614287\n",
      "Iteration 7, loss = 0.38297652\n",
      "Iteration 35, loss = 0.22035279\n",
      "Iteration 51, loss = 0.18849600\n",
      "Iteration 14, loss = 0.27368952\n",
      "Iteration 2, loss = 2.40270487\n",
      "Iteration 23, loss = 0.27033680\n",
      "Iteration 22, loss = 0.28321373\n",
      "Iteration 7, loss = 0.36339929\n",
      "Iteration 5, loss = 0.46445655\n",
      "Iteration 25, loss = 0.24517571\n",
      "Iteration 8, loss = 0.36944527\n",
      "Iteration 52, loss = 0.18927346\n",
      "Iteration 36, loss = 0.21973891\n",
      "Iteration 15, loss = 0.26990482\n",
      "Iteration 24, loss = 0.25903183\n",
      "Iteration 3, loss = 0.68577155\n",
      "Iteration 8, loss = 0.34855299\n",
      "Iteration 26, loss = 0.24599023\n",
      "Iteration 9, loss = 0.33989497\n",
      "Iteration 23, loss = 0.27545428\n",
      "Iteration 6, loss = 0.40862583\n",
      "Iteration 53, loss = 0.18938187\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 37, loss = 0.22536177\n",
      "Iteration 25, loss = 0.25712746\n",
      "Iteration 16, loss = 0.27312086\n",
      "Iteration 4, loss = 0.55902250\n",
      "Iteration 9, loss = 0.33090327\n",
      "Iteration 27, loss = 0.24347250\n",
      "Iteration 24, loss = 0.27156138\n",
      "Iteration 10, loss = 0.32442779\n",
      "Iteration 7, loss = 0.38495002\n",
      "Iteration 17, loss = 0.26356180\n",
      "Iteration 38, loss = 0.21616345\n",
      "Iteration 5, loss = 0.46994316\n",
      "Iteration 26, loss = 0.25735886\n",
      "Iteration 10, loss = 0.31862449\n",
      "Iteration 1, loss = 4.54265774\n",
      "Iteration 8, loss = 0.35843828\n",
      "Iteration 28, loss = 0.23899960\n",
      "Iteration 18, loss = 0.25210833\n",
      "Iteration 25, loss = 0.26479931\n",
      "Iteration 11, loss = 0.31246356\n",
      "Iteration 6, loss = 0.42762543\n",
      "Iteration 39, loss = 0.22112141\n",
      "Iteration 27, loss = 0.25138141\n",
      "Iteration 11, loss = 0.31912115\n",
      "Iteration 2, loss = 2.39383631\n",
      "Iteration 9, loss = 0.33831744\n",
      "Iteration 40, loss = 0.21603407\n",
      "Iteration 29, loss = 0.23589696\n",
      "Iteration 7, loss = 0.40236441\n",
      "Iteration 19, loss = 0.25363154\n",
      "Iteration 12, loss = 0.29801747\n",
      "Iteration 3, loss = 0.65675177\n",
      "Iteration 26, loss = 0.26973284\n",
      "Iteration 28, loss = 0.24075164\n",
      "Iteration 10, loss = 0.31813297\n",
      "Iteration 12, loss = 0.29935965\n",
      "Iteration 20, loss = 0.24515088\n",
      "Iteration 8, loss = 0.38304460\n",
      "Iteration 41, loss = 0.21258889\n",
      "Iteration 30, loss = 0.23128650\n",
      "Iteration 4, loss = 0.54225126\n",
      "Iteration 13, loss = 0.29090822\n",
      "Iteration 29, loss = 0.24038155\n",
      "Iteration 13, loss = 0.29166121\n",
      "Iteration 11, loss = 0.31207946\n",
      "Iteration 27, loss = 0.26441639\n",
      "Iteration 9, loss = 0.35875232\n",
      "Iteration 21, loss = 0.23643125\n",
      "Iteration 42, loss = 0.21047587\n",
      "Iteration 31, loss = 0.23014470\n",
      "Iteration 14, loss = 0.28315787\n",
      "Iteration 30, loss = 0.23564978\n",
      "Iteration 5, loss = 0.44145729\n",
      "Iteration 10, loss = 0.33711155\n",
      "Iteration 12, loss = 0.30901724\n",
      "Iteration 14, loss = 0.28468982\n",
      "Iteration 28, loss = 0.25322914\n",
      "Iteration 43, loss = 0.20277182\n",
      "Iteration 22, loss = 0.23950679\n",
      "Iteration 32, loss = 0.22946787\n",
      "Iteration 13, loss = 0.29977150\n",
      "Iteration 15, loss = 0.27060436\n",
      "Iteration 11, loss = 0.32701649\n",
      "Iteration 15, loss = 0.27501851\n",
      "Iteration 31, loss = 0.24256301\n",
      "Iteration 44, loss = 0.20048942\n",
      "Iteration 23, loss = 0.22978019\n",
      "Iteration 29, loss = 0.25675251\n",
      "Iteration 6, loss = 0.40009045\n",
      "Iteration 33, loss = 0.22281395\n",
      "Iteration 16, loss = 0.28142028\n",
      "Iteration 14, loss = 0.29427320\n",
      "Iteration 24, loss = 0.22608091\n",
      "Iteration 12, loss = 0.32492206\n",
      "Iteration 16, loss = 0.26856707\n",
      "Iteration 45, loss = 0.20353235\n",
      "Iteration 32, loss = 0.23211566\n",
      "Iteration 30, loss = 0.25283154\n",
      "Iteration 7, loss = 0.36960851\n",
      "Iteration 34, loss = 0.22588781\n",
      "Iteration 17, loss = 0.26803524\n",
      "Iteration 25, loss = 0.21570014\n",
      "Iteration 46, loss = 0.19798727\n",
      "Iteration 13, loss = 0.31864377\n",
      "Iteration 15, loss = 0.27465096\n",
      "Iteration 17, loss = 0.26685341\n",
      "Iteration 33, loss = 0.22673130\n",
      "Iteration 47, loss = 0.19845675\n",
      "Iteration 31, loss = 0.24374594\n",
      "Iteration 8, loss = 0.34685624\n",
      "Iteration 35, loss = 0.22287027\n",
      "Iteration 18, loss = 0.25316045\n",
      "Iteration 16, loss = 0.27149184\n",
      "Iteration 14, loss = 0.30702273\n",
      "Iteration 18, loss = 0.25616728\n",
      "Iteration 36, loss = 0.22535050\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 0.21156900\n",
      "Iteration 34, loss = 0.22673866\n",
      "Iteration 48, loss = 0.18858432\n",
      "Iteration 32, loss = 0.24359868\n",
      "Iteration 9, loss = 0.32052789\n",
      "Iteration 19, loss = 0.26500285\n",
      "Iteration 17, loss = 0.26724944\n",
      "Iteration 15, loss = 0.29380051\n",
      "Iteration 19, loss = 0.25536089\n",
      "Iteration 35, loss = 0.22422982\n",
      "Iteration 49, loss = 0.19363194\n",
      "Iteration 27, loss = 0.21237692\n",
      "Iteration 33, loss = 0.24129725\n",
      "Iteration 10, loss = 0.31235676\n",
      "Iteration 20, loss = 0.25008496\n",
      "Iteration 1, loss = 4.51890071\n",
      "Iteration 18, loss = 0.26446747\n",
      "Iteration 20, loss = 0.23834172\n",
      "Iteration 16, loss = 0.28214116\n",
      "Iteration 28, loss = 0.20648713\n",
      "Iteration 36, loss = 0.22057718\n",
      "Iteration 34, loss = 0.23720919\n",
      "Iteration 50, loss = 0.19043070\n",
      "Iteration 21, loss = 0.24502268\n",
      "Iteration 2, loss = 2.29122177\n",
      "Iteration 21, loss = 0.23885663\n",
      "Iteration 11, loss = 0.30642425\n",
      "Iteration 19, loss = 0.26581391\n",
      "Iteration 17, loss = 0.27818399\n",
      "Iteration 37, loss = 0.22365962\n",
      "Iteration 29, loss = 0.20747297\n",
      "Iteration 3, loss = 0.67064549\n",
      "Iteration 35, loss = 0.23498950\n",
      "Iteration 22, loss = 0.23062681\n",
      "Iteration 51, loss = 0.18030066\n",
      "Iteration 22, loss = 0.23668485\n",
      "Iteration 12, loss = 0.29056409\n",
      "Iteration 20, loss = 0.24384428\n",
      "Iteration 38, loss = 0.21556418\n",
      "Iteration 18, loss = 0.27696540\n",
      "Iteration 30, loss = 0.20169127\n",
      "Iteration 23, loss = 0.23784889\n",
      "Iteration 23, loss = 0.22987147\n",
      "Iteration 52, loss = 0.18339145\n",
      "Iteration 36, loss = 0.23206537\n",
      "Iteration 4, loss = 0.55064110\n",
      "Iteration 13, loss = 0.29567033\n",
      "Iteration 21, loss = 0.24710053\n",
      "Iteration 37, loss = 0.22798288\n",
      "Iteration 31, loss = 0.20712488\n",
      "Iteration 39, loss = 0.21748806\n",
      "Iteration 24, loss = 0.23301669\n",
      "Iteration 22, loss = 0.24716813\n",
      "Iteration 53, loss = 0.18586360\n",
      "Iteration 14, loss = 0.27178578\n",
      "Iteration 24, loss = 0.22582357\n",
      "Iteration 5, loss = 0.46076697\n",
      "Iteration 19, loss = 0.27713637\n",
      "Iteration 32, loss = 0.19998503\n",
      "Iteration 54, loss = 0.18676041\n",
      "Iteration 40, loss = 0.21212040\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 0.26606115\n",
      "Iteration 23, loss = 0.23901036\n",
      "Iteration 38, loss = 0.22284036\n",
      "Iteration 25, loss = 0.22153365\n",
      "Iteration 25, loss = 0.22441391\n",
      "Iteration 6, loss = 0.41276054\n",
      "Iteration 20, loss = 0.27007725\n",
      "Iteration 41, loss = 0.21141383\n",
      "Iteration 26, loss = 0.21752733\n",
      "Iteration 16, loss = 0.26189534\n",
      "Iteration 39, loss = 0.22628868\n",
      "Iteration 24, loss = 0.22763733\n",
      "Iteration 26, loss = 0.23080225\n",
      "Iteration 33, loss = 0.19227531\n",
      "Iteration 1, loss = 4.52075247\n",
      "Iteration 7, loss = 0.37592245\n",
      "Iteration 21, loss = 0.25670748\n",
      "Iteration 42, loss = 0.21409056\n",
      "Iteration 2, loss = 2.21634733\n",
      "Iteration 27, loss = 0.21880150\n",
      "Iteration 40, loss = 0.22279055\n",
      "Iteration 25, loss = 0.23614230\n",
      "Iteration 27, loss = 0.21718561\n",
      "Iteration 34, loss = 0.19810525\n",
      "Iteration 17, loss = 0.25433432\n",
      "Iteration 22, loss = 0.26044902\n",
      "Iteration 8, loss = 0.35291825\n",
      "Iteration 28, loss = 0.22001794\n",
      "Iteration 43, loss = 0.21504283\n",
      "Iteration 3, loss = 0.62042731\n",
      "Iteration 26, loss = 0.22521009\n",
      "Iteration 35, loss = 0.19371775\n",
      "Iteration 41, loss = 0.21974680\n",
      "Iteration 28, loss = 0.21392564\n",
      "Iteration 23, loss = 0.25615511\n",
      "Iteration 18, loss = 0.24567875\n",
      "Iteration 29, loss = 0.22013923\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.51606973\n",
      "Iteration 36, loss = 0.19901287\n",
      "Iteration 29, loss = 0.21308607\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 0.33573637\n",
      "Iteration 44, loss = 0.20757219\n",
      "Iteration 27, loss = 0.23469734\n",
      "Iteration 42, loss = 0.22387975\n",
      "Iteration 19, loss = 0.24840500\n",
      "Iteration 24, loss = 0.24018167\n",
      "Iteration 30, loss = 0.20911772\n",
      "Iteration 5, loss = 0.44049088\n",
      "Iteration 45, loss = 0.19771742\n",
      "Iteration 28, loss = 0.21848914\n",
      "Iteration 10, loss = 0.31788945\n",
      "Iteration 1, loss = 4.50003421\n",
      "Iteration 46, loss = 0.20468255\n",
      "Iteration 31, loss = 0.20357682\n",
      "Iteration 1, loss = 4.53705233\n",
      "Iteration 43, loss = 0.21522707\n",
      "Iteration 20, loss = 0.24623783\n",
      "Iteration 25, loss = 0.24290099\n",
      "Iteration 6, loss = 0.40258221\n",
      "Iteration 11, loss = 0.31109165\n",
      "Iteration 47, loss = 0.19924424\n",
      "Iteration 26, loss = 0.23593602\n",
      "Iteration 32, loss = 0.20278690\n",
      "Iteration 29, loss = 0.22308950\n",
      "Iteration 2, loss = 2.19318907\n",
      "Iteration 48, loss = 0.19619139\n",
      "Iteration 44, loss = 0.22580940\n",
      "Iteration 21, loss = 0.23510870\n",
      "Iteration 7, loss = 0.36548488\n",
      "Iteration 12, loss = 0.30592709\n",
      "Iteration 27, loss = 0.24588090\n",
      "Iteration 30, loss = 0.21675822\n",
      "Iteration 2, loss = 2.37685213\n",
      "Iteration 3, loss = 0.66883032\n",
      "Iteration 33, loss = 0.18999351\n",
      "Iteration 49, loss = 0.20246660\n",
      "Iteration 45, loss = 0.21898887\n",
      "Iteration 22, loss = 0.22212816\n",
      "Iteration 13, loss = 0.29956630\n",
      "Iteration 8, loss = 0.35126228\n",
      "Iteration 28, loss = 0.23354486\n",
      "Iteration 31, loss = 0.20650674\n",
      "Iteration 23, loss = 0.22319499\n",
      "Iteration 3, loss = 0.67651304\n",
      "Iteration 34, loss = 0.19515665\n",
      "Iteration 4, loss = 0.54383395\n",
      "Iteration 50, loss = 0.19675590\n",
      "Iteration 46, loss = 0.21841305\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 0.29166350\n",
      "Iteration 9, loss = 0.33326211\n",
      "Iteration 29, loss = 0.23661233\n",
      "Iteration 32, loss = 0.19947396\n",
      "Iteration 24, loss = 0.22396013\n",
      "Iteration 4, loss = 0.56104768\n",
      "Iteration 35, loss = 0.19390832\n",
      "Iteration 5, loss = 0.45092240\n",
      "Iteration 51, loss = 0.19417215\n",
      "Iteration 15, loss = 0.27931051\n",
      "Iteration 10, loss = 0.32148232\n",
      "Iteration 30, loss = 0.22077768\n",
      "Iteration 25, loss = 0.22805721\n",
      "Iteration 33, loss = 0.19415342\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 0.19101369\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.50439737\n",
      "Iteration 52, loss = 0.19006061\n",
      "Iteration 16, loss = 0.26494935\n",
      "Iteration 6, loss = 0.40463563\n",
      "Iteration 5, loss = 0.46383744\n",
      "Iteration 11, loss = 0.30304589\n",
      "Iteration 31, loss = 0.22184347\n",
      "Iteration 53, loss = 0.19138674\n",
      "Iteration 6, loss = 0.42150767\n",
      "Iteration 34, loss = 0.20471752\n",
      "Iteration 2, loss = 2.20461947\n",
      "Iteration 17, loss = 0.26260683\n",
      "Iteration 7, loss = 0.37359673\n",
      "Iteration 12, loss = 0.29176835\n",
      "Iteration 3, loss = 0.65193622\n",
      "Iteration 18, loss = 0.25335600\n",
      "Iteration 1, loss = 4.54158004\n",
      "Iteration 1, loss = 4.51057744\n",
      "Iteration 35, loss = 0.19040134\n",
      "Iteration 8, loss = 0.35676231\n",
      "Iteration 7, loss = 0.40376334\n",
      "Iteration 32, loss = 0.21833890\n",
      "Iteration 54, loss = 0.18574191\n",
      "Iteration 13, loss = 0.28942029\n",
      "Iteration 36, loss = 0.18833442\n",
      "Iteration 19, loss = 0.26089074\n",
      "Iteration 2, loss = 2.29517885\n",
      "Iteration 4, loss = 0.53055273\n",
      "Iteration 2, loss = 2.24061297\n",
      "Iteration 8, loss = 0.37314205\n",
      "Iteration 9, loss = 0.33540201\n",
      "Iteration 33, loss = 0.22005138\n",
      "Iteration 55, loss = 0.18928621\n",
      "Iteration 37, loss = 0.19152817\n",
      "Iteration 14, loss = 0.28117861\n",
      "Iteration 5, loss = 0.44570629\n",
      "Iteration 20, loss = 0.25866088\n",
      "Iteration 3, loss = 0.64288194\n",
      "Iteration 15, loss = 0.27992736\n",
      "Iteration 3, loss = 0.65107477\n",
      "Iteration 38, loss = 0.18989766\n",
      "Iteration 9, loss = 0.35012787\n",
      "Iteration 34, loss = 0.21266481\n",
      "Iteration 10, loss = 0.32966630\n",
      "Iteration 56, loss = 0.18259880\n",
      "Iteration 21, loss = 0.24680335\n",
      "Iteration 6, loss = 0.40906019\n",
      "Iteration 4, loss = 0.53802638\n",
      "Iteration 16, loss = 0.27061185\n",
      "Iteration 39, loss = 0.19409594\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.31217462\n",
      "Iteration 4, loss = 0.53615426\n",
      "Iteration 35, loss = 0.21346900\n",
      "Iteration 57, loss = 0.18414536\n",
      "Iteration 10, loss = 0.34451834\n",
      "Iteration 22, loss = 0.23624850\n",
      "Iteration 17, loss = 0.25168631\n",
      "Iteration 5, loss = 0.45996281\n",
      "Iteration 7, loss = 0.37464785\n",
      "Iteration 12, loss = 0.30302350\n",
      "Iteration 36, loss = 0.20607499\n",
      "Iteration 11, loss = 0.33323354\n",
      "Iteration 5, loss = 0.44025741\n",
      "Iteration 58, loss = 0.17705582\n",
      "Iteration 1, loss = 4.49933068\n",
      "Iteration 23, loss = 0.22961311\n",
      "Iteration 6, loss = 0.41033438\n",
      "Iteration 18, loss = 0.26809029\n",
      "Iteration 8, loss = 0.34630964\n",
      "Iteration 6, loss = 0.39421537\n",
      "Iteration 12, loss = 0.32610814\n",
      "Iteration 13, loss = 0.29905702\n",
      "Iteration 37, loss = 0.19850749\n",
      "Iteration 59, loss = 0.18238122\n",
      "Iteration 2, loss = 2.18690828\n",
      "Iteration 24, loss = 0.22731471\n",
      "Iteration 19, loss = 0.26186901\n",
      "Iteration 7, loss = 0.39064239\n",
      "Iteration 9, loss = 0.32920537\n",
      "Iteration 7, loss = 0.36668023\n",
      "Iteration 13, loss = 0.31307669\n",
      "Iteration 14, loss = 0.28332878\n",
      "Iteration 60, loss = 0.17933171\n",
      "Iteration 38, loss = 0.19506359\n",
      "Iteration 8, loss = 0.35486894\n",
      "Iteration 3, loss = 0.66036279\n",
      "Iteration 15, loss = 0.28058471\n",
      "Iteration 10, loss = 0.32592999\n",
      "Iteration 8, loss = 0.34428629\n",
      "Iteration 20, loss = 0.26281710\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 25, loss = 0.22188898\n",
      "Iteration 61, loss = 0.17711495\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 0.29949973\n",
      "Iteration 9, loss = 0.33827279\n",
      "Iteration 39, loss = 0.20419467\n",
      "Iteration 4, loss = 0.54899852\n",
      "Iteration 16, loss = 0.27502693\n",
      "Iteration 11, loss = 0.30756696\n",
      "Iteration 9, loss = 0.33613862\n",
      "Iteration 15, loss = 0.29575513\n",
      "Iteration 26, loss = 0.21516766\n",
      "Iteration 10, loss = 0.31722440\n",
      "Iteration 40, loss = 0.20195927\n",
      "Iteration 17, loss = 0.26105498\n",
      "Iteration 1, loss = 4.54124496\n",
      "Iteration 5, loss = 0.46264081\n",
      "Iteration 12, loss = 0.28277783\n",
      "Iteration 27, loss = 0.21283606\n",
      "Iteration 16, loss = 0.29049697\n",
      "Iteration 10, loss = 0.32083426\n",
      "Iteration 18, loss = 0.25900372\n",
      "Iteration 28, loss = 0.22374661\n",
      "Iteration 11, loss = 0.30914538\n",
      "Iteration 41, loss = 0.20201133\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 2.36482957\n",
      "Iteration 17, loss = 0.28555788\n",
      "Iteration 11, loss = 0.31808448\n",
      "Iteration 13, loss = 0.28229772\n",
      "Iteration 29, loss = 0.22200835\n",
      "Iteration 6, loss = 0.42778053\n",
      "Iteration 12, loss = 0.29571720\n",
      "Iteration 3, loss = 0.67748217\n",
      "Iteration 19, loss = 0.25162313\n",
      "Iteration 18, loss = 0.27291112\n",
      "Iteration 12, loss = 0.29575082\n",
      "Iteration 30, loss = 0.22139299\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 0.28407273\n",
      "Iteration 7, loss = 0.38954688\n",
      "Iteration 13, loss = 0.28597832\n",
      "Iteration 20, loss = 0.24929235\n",
      "Iteration 13, loss = 0.29093119\n",
      "Iteration 4, loss = 0.54834086\n",
      "Iteration 19, loss = 0.27463178\n",
      "Iteration 15, loss = 0.26848388\n",
      "Iteration 21, loss = 0.25090474\n",
      "Iteration 14, loss = 0.27681728\n",
      "Iteration 14, loss = 0.28648857\n",
      "Iteration 5, loss = 0.46864324\n",
      "Iteration 20, loss = 0.26937444\n",
      "Iteration 8, loss = 0.36588479\n",
      "Iteration 22, loss = 0.23856728\n",
      "Iteration 16, loss = 0.26420385\n",
      "Iteration 15, loss = 0.27664882\n",
      "Iteration 6, loss = 0.42291527\n",
      "Iteration 15, loss = 0.27787868\n",
      "Iteration 21, loss = 0.26265218\n",
      "Iteration 17, loss = 0.25086712\n",
      "Iteration 9, loss = 0.33900955\n",
      "Iteration 7, loss = 0.40153388\n",
      "Iteration 16, loss = 0.27096136\n",
      "Iteration 23, loss = 0.23880743\n",
      "Iteration 22, loss = 0.25837704\n",
      "Iteration 16, loss = 0.27344871\n",
      "Iteration 18, loss = 0.26153022\n",
      "Iteration 17, loss = 0.26182059\n",
      "Iteration 10, loss = 0.33031688\n",
      "Iteration 8, loss = 0.37161519\n",
      "Iteration 24, loss = 0.23269679\n",
      "Iteration 23, loss = 0.25664479\n",
      "Iteration 17, loss = 0.26053091\n",
      "Iteration 18, loss = 0.25175252\n",
      "Iteration 25, loss = 0.22919567\n",
      "Iteration 19, loss = 0.25105162\n",
      "Iteration 11, loss = 0.31482922\n",
      "Iteration 24, loss = 0.25092966\n",
      "Iteration 9, loss = 0.35333666\n",
      "Iteration 18, loss = 0.25529267\n",
      "Iteration 26, loss = 0.22665476\n",
      "Iteration 19, loss = 0.25327655\n",
      "Iteration 12, loss = 0.30712553\n",
      "Iteration 10, loss = 0.33358533\n",
      "Iteration 19, loss = 0.26170308\n",
      "Iteration 27, loss = 0.22945316\n",
      "Iteration 25, loss = 0.24135335\n",
      "Iteration 20, loss = 0.24721871\n",
      "Iteration 13, loss = 0.30253029\n",
      "Iteration 20, loss = 0.24380853\n",
      "Iteration 20, loss = 0.25055265\n",
      "Iteration 28, loss = 0.21809806\n",
      "Iteration 26, loss = 0.23710865\n",
      "Iteration 11, loss = 0.32535843\n",
      "Iteration 14, loss = 0.28241416\n",
      "Iteration 21, loss = 0.24196418\n",
      "Iteration 21, loss = 0.24324264\n",
      "Iteration 21, loss = 0.23788638\n",
      "Iteration 27, loss = 0.23290098\n",
      "Iteration 12, loss = 0.32254043\n",
      "Iteration 22, loss = 0.23978608\n",
      "Iteration 15, loss = 0.27486168\n",
      "Iteration 29, loss = 0.21943265\n",
      "Iteration 28, loss = 0.22906907\n",
      "Iteration 22, loss = 0.23881086\n",
      "Iteration 22, loss = 0.22759648\n",
      "Iteration 16, loss = 0.27479661\n",
      "Iteration 23, loss = 0.23889974\n",
      "Iteration 30, loss = 0.22035538\n",
      "Iteration 29, loss = 0.23128953\n",
      "Iteration 23, loss = 0.23899081\n",
      "Iteration 30, loss = 0.22250508\n",
      "Iteration 13, loss = 0.31472885\n",
      "Iteration 23, loss = 0.22531737\n",
      "Iteration 31, loss = 0.20737560\n",
      "Iteration 17, loss = 0.27807238\n",
      "Iteration 24, loss = 0.23293843\n",
      "Iteration 24, loss = 0.23124881\n",
      "Iteration 31, loss = 0.21783165\n",
      "Iteration 25, loss = 0.22649429\n",
      "Iteration 14, loss = 0.31085110\n",
      "Iteration 18, loss = 0.26104297\n",
      "Iteration 32, loss = 0.21559801\n",
      "Iteration 24, loss = 0.22418556\n",
      "Iteration 25, loss = 0.23359932\n",
      "Iteration 32, loss = 0.21844918\n",
      "Iteration 19, loss = 0.26338355\n",
      "Iteration 33, loss = 0.21588237\n",
      "Iteration 26, loss = 0.22097727\n",
      "Iteration 25, loss = 0.21343535\n",
      "Iteration 33, loss = 0.20452460\n",
      "Iteration 15, loss = 0.27864422\n",
      "Iteration 26, loss = 0.23267764\n",
      "Iteration 20, loss = 0.24605063\n",
      "Iteration 34, loss = 0.21120832\n",
      "Iteration 27, loss = 0.21839903\n",
      "Iteration 34, loss = 0.19976810\n",
      "Iteration 16, loss = 0.27610044\n",
      "Iteration 26, loss = 0.22252203\n",
      "Iteration 21, loss = 0.24897883\n",
      "Iteration 27, loss = 0.21296847\n",
      "Iteration 35, loss = 0.19677455\n",
      "Iteration 35, loss = 0.21700597\n",
      "Iteration 28, loss = 0.21504010\n",
      "Iteration 17, loss = 0.28071679\n",
      "Iteration 28, loss = 0.22440230\n",
      "Iteration 27, loss = 0.21737841\n",
      "Iteration 36, loss = 0.19466072\n",
      "Iteration 36, loss = 0.20896500\n",
      "Iteration 22, loss = 0.24153683\n",
      "Iteration 29, loss = 0.21332375\n",
      "Iteration 18, loss = 0.27517852\n",
      "Iteration 29, loss = 0.20797234\n",
      "Iteration 28, loss = 0.21362698\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 37, loss = 0.20075446\n",
      "Iteration 37, loss = 0.20066979\n",
      "Iteration 23, loss = 0.23768771\n",
      "Iteration 30, loss = 0.20253181\n",
      "Iteration 38, loss = 0.20580546\n",
      "Iteration 30, loss = 0.20504569\n",
      "Iteration 19, loss = 0.26153733\n",
      "Iteration 38, loss = 0.19715258\n",
      "Iteration 24, loss = 0.23315499\n",
      "Iteration 31, loss = 0.20686320\n",
      "Iteration 31, loss = 0.20430630\n",
      "Iteration 39, loss = 0.19427349\n",
      "Iteration 39, loss = 0.19264014\n",
      "Iteration 20, loss = 0.26975090\n",
      "Iteration 25, loss = 0.22989018\n",
      "Iteration 40, loss = 0.20374885\n",
      "Iteration 32, loss = 0.21127988\n",
      "Iteration 32, loss = 0.20175705\n",
      "Iteration 40, loss = 0.19487255\n",
      "Iteration 21, loss = 0.25789548\n",
      "Iteration 26, loss = 0.22187067\n",
      "Iteration 41, loss = 0.19035208\n",
      "Iteration 41, loss = 0.19811080\n",
      "Iteration 33, loss = 0.19422734\n",
      "Iteration 33, loss = 0.20830869\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.25551025\n",
      "Iteration 27, loss = 0.21685154\n",
      "Iteration 42, loss = 0.20256097\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 0.17632663\n",
      "Iteration 34, loss = 0.19872982\n",
      "Iteration 23, loss = 0.24152480\n",
      "Iteration 43, loss = 0.17819434\n",
      "Iteration 28, loss = 0.21925205\n",
      "Iteration 35, loss = 0.19069911\n",
      "Iteration 44, loss = 0.17355956\n",
      "Iteration 29, loss = 0.21524023\n",
      "Iteration 24, loss = 0.23257111\n",
      "Iteration 36, loss = 0.19325127\n",
      "Iteration 45, loss = 0.17139581\n",
      "Iteration 30, loss = 0.22235994\n",
      "Iteration 25, loss = 0.24265910\n",
      "Iteration 37, loss = 0.18878630\n",
      "Iteration 46, loss = 0.18217755\n",
      "Iteration 26, loss = 0.23742482\n",
      "Iteration 47, loss = 0.18531222\n",
      "Iteration 31, loss = 0.21285493\n",
      "Iteration 38, loss = 0.19060957\n",
      "Iteration 27, loss = 0.22912017\n",
      "Iteration 32, loss = 0.21781416\n",
      "Iteration 48, loss = 0.17553314\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 0.18332900\n",
      "Iteration 28, loss = 0.21635598\n",
      "Iteration 33, loss = 0.21365477\n",
      "Iteration 40, loss = 0.18743804\n",
      "Iteration 29, loss = 0.23154100\n",
      "Iteration 41, loss = 0.18701521\n",
      "Iteration 34, loss = 0.20174948\n",
      "Iteration 30, loss = 0.21414735\n",
      "Iteration 35, loss = 0.19296230\n",
      "Iteration 42, loss = 0.18493241\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.21560400\n",
      "Iteration 36, loss = 0.20035535\n",
      "Iteration 32, loss = 0.21870079\n",
      "Iteration 37, loss = 0.19984641\n",
      "Iteration 33, loss = 0.22020487\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 38, loss = 0.18935062\n",
      "Iteration 39, loss = 0.18342679\n",
      "Iteration 40, loss = 0.18350533\n",
      "Iteration 41, loss = 0.18698641\n",
      "Iteration 42, loss = 0.18656357\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  90 out of  90 | elapsed: 16.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.67175335\n",
      "Iteration 2, loss = 3.61843903\n",
      "Iteration 3, loss = 1.27312681\n",
      "Iteration 4, loss = 0.60294369\n",
      "Iteration 5, loss = 0.52022269\n",
      "Iteration 6, loss = 0.47502952\n",
      "Iteration 7, loss = 0.44272620\n",
      "Iteration 8, loss = 0.41973587\n",
      "Iteration 9, loss = 0.39266318\n",
      "Iteration 10, loss = 0.37337673\n",
      "Iteration 11, loss = 0.36271513\n",
      "Iteration 12, loss = 0.34903326\n",
      "Iteration 13, loss = 0.33445396\n",
      "Iteration 14, loss = 0.32753173\n",
      "Iteration 15, loss = 0.31009964\n",
      "Iteration 16, loss = 0.30667695\n",
      "Iteration 17, loss = 0.30126351\n",
      "Iteration 18, loss = 0.29546536\n",
      "Iteration 19, loss = 0.28748862\n",
      "Iteration 20, loss = 0.27810214\n",
      "Iteration 21, loss = 0.27311613\n",
      "Iteration 22, loss = 0.27559595\n",
      "Iteration 23, loss = 0.26393864\n",
      "Iteration 24, loss = 0.26660487\n",
      "Iteration 25, loss = 0.25911998\n",
      "Iteration 26, loss = 0.25351181\n",
      "Iteration 27, loss = 0.25158542\n",
      "Iteration 28, loss = 0.25267593\n",
      "Iteration 29, loss = 0.24000300\n",
      "Iteration 30, loss = 0.24395557\n",
      "Iteration 31, loss = 0.23943075\n",
      "Iteration 32, loss = 0.23242474\n",
      "Iteration 33, loss = 0.22486731\n",
      "Iteration 34, loss = 0.22894748\n",
      "Iteration 35, loss = 0.22269798\n",
      "Iteration 36, loss = 0.22031705\n",
      "Iteration 37, loss = 0.22086092\n",
      "Iteration 38, loss = 0.22029966\n",
      "Iteration 39, loss = 0.21756204\n",
      "Iteration 40, loss = 0.21256618\n",
      "Iteration 41, loss = 0.21280701\n",
      "Iteration 42, loss = 0.20402890\n",
      "Iteration 43, loss = 0.20863003\n",
      "Iteration 44, loss = 0.20672819\n",
      "Iteration 45, loss = 0.20078832\n",
      "Iteration 46, loss = 0.19725985\n",
      "Iteration 47, loss = 0.19802631\n",
      "Iteration 48, loss = 0.19633057\n",
      "Iteration 49, loss = 0.19481972\n",
      "Iteration 50, loss = 0.19556506\n",
      "Iteration 51, loss = 0.19247597\n",
      "Iteration 52, loss = 0.18818107\n",
      "Iteration 53, loss = 0.18717924\n",
      "Iteration 54, loss = 0.19354726\n",
      "Iteration 55, loss = 0.19189939\n",
      "Iteration 56, loss = 0.18626747\n",
      "Iteration 57, loss = 0.18272296\n",
      "Iteration 58, loss = 0.17796206\n",
      "Iteration 59, loss = 0.17595555\n",
      "Iteration 60, loss = 0.17137810\n",
      "Iteration 61, loss = 0.17549897\n",
      "Iteration 62, loss = 0.17291240\n",
      "Iteration 63, loss = 0.17769823\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "{'learning_rate_init': 0.001, 'alpha': 0.0001, 'hidden_layer_sizes': (256, 256, 128), 'learning_rate': 'adaptive', 'max_iter': 100} -0.7468337295074581\n",
      "Recall: 0.8753551136363635 vs 0.8844287223193472\n",
      "Precision: 0.884523645080195 vs 0.8939847561802985\n",
      "Accuracy: 0.8759640102827764 vs 0.8849614395886889\n"
     ]
    }
   ],
   "source": [
    "n_classes = 128\n",
    "seed = 555\n",
    "n = len(meta_features_list)\n",
    "\n",
    "_X = X.values\n",
    "_y = y.values\n",
    "\n",
    "splt = StratifiedShuffleSplit(n_splits=7, test_size=0.25, random_state=seed)\n",
    "train_index, test_index = next(splt.split(_X, _y))\n",
    "\n",
    "_X_train = _X[train_index, :]\n",
    "_X_test = _X[test_index, :]\n",
    "_y_train = _y[train_index]\n",
    "_y_test = _y[test_index]\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "clf = MLPClassifier(early_stopping=False, verbose=False)\n",
    "params = {\n",
    "    \"hidden_layer_sizes\": [(n_classes * (n - 1), n_classes * (n - 1), n_classes), ],\n",
    "    \"alpha\": [0.0001, ],\n",
    "    \"learning_rate_init\":[0.001, ],\n",
    "    \"learning_rate\": ['adaptive', ],\n",
    "    \"max_iter\": [100, ]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(clf, params, scoring=\"neg_log_loss\", cv=cv, n_jobs=10, verbose=True)    \n",
    "gs.fit(_X_train, _y_train)\n",
    "print(gs.best_params_, gs.best_score_)\n",
    "\n",
    "_y_pred = gs.best_estimator_.predict(_X_test)\n",
    "_y_pred_base = get_baseline_preds(_X_test)\n",
    "\n",
    "print(\"Recall: {} vs {}\".format(recall_score(_y_test, _y_pred, average=\"macro\"), recall_score(_y_test, _y_pred_base, average=\"macro\")))\n",
    "print(\"Precision: {} vs {}\".format(precision_score(_y_test, _y_pred, average=\"macro\"), precision_score(_y_test, _y_pred_base, average=\"macro\")))\n",
    "print(\"Accuracy: {} vs {}\".format(accuracy_score(_y_test, _y_pred), accuracy_score(_y_test, _y_pred_base)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{'learning_rate_init': 0.001, 'alpha': 0.0001, 'hidden_layer_sizes': (256, 256, 128), 'learning_rate': 'adaptive', 'max_iter': 100} -0.7468337295074581\n",
    "Recall: 0.8753551136363635 vs 0.8844287223193472\n",
    "Precision: 0.884523645080195 vs 0.8939847561802985\n",
    "Accuracy: 0.8759640102827764 vs 0.8849614395886889\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1556,), (1556,))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_y_test.shape, _y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "_y_probas = gs.best_estimator_.predict_proba(_X_test)\n",
    "_y_probas_base = get_baseline_probas(_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.7740352953625593 vs 0.4062003888590844\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "print(\"Log loss: {} vs {}\".format(log_loss(_y_test, _y_probas), log_loss(_y_test, _y_probas_base)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CatBoostClassifier as meta-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cat\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.0367380\ttest: 0.0372643\tbest: 0.0372643 (0)\ttotal: 13s\tremaining: 43m 6s\n",
      "1:\tlearn: 0.0639797\ttest: 0.0637668\tbest: 0.0637668 (1)\ttotal: 26s\tremaining: 42m 51s\n",
      "2:\tlearn: 0.0943911\ttest: 0.0920468\tbest: 0.0920468 (2)\ttotal: 39.8s\tremaining: 43m 35s\n",
      "3:\tlearn: 0.1215066\ttest: 0.1174779\tbest: 0.1174779 (3)\ttotal: 53.2s\tremaining: 43m 25s\n",
      "4:\tlearn: 0.1477262\ttest: 0.1447554\tbest: 0.1447554 (4)\ttotal: 1m 6s\tremaining: 43m 3s\n",
      "5:\tlearn: 0.1684678\ttest: 0.1643389\tbest: 0.1643389 (5)\ttotal: 1m 19s\tremaining: 42m 46s\n",
      "6:\tlearn: 0.1915412\ttest: 0.1890836\tbest: 0.1890836 (6)\ttotal: 1m 32s\tremaining: 42m 27s\n",
      "7:\tlearn: 0.2162585\ttest: 0.2130992\tbest: 0.2130992 (7)\ttotal: 1m 45s\tremaining: 42m 12s\n",
      "8:\tlearn: 0.2394889\ttest: 0.2366966\tbest: 0.2366966 (8)\ttotal: 1m 58s\tremaining: 41m 54s\n",
      "9:\tlearn: 0.2607898\ttest: 0.2558099\tbest: 0.2558099 (9)\ttotal: 2m 11s\tremaining: 41m 46s\n",
      "10:\tlearn: 0.2761848\ttest: 0.2710958\tbest: 0.2710958 (10)\ttotal: 2m 25s\tremaining: 41m 34s\n",
      "11:\tlearn: 0.2945096\ttest: 0.2890398\tbest: 0.2890398 (11)\ttotal: 2m 38s\tremaining: 41m 22s\n",
      "12:\tlearn: 0.3098748\ttest: 0.3028710\tbest: 0.3028710 (12)\ttotal: 2m 51s\tremaining: 41m 13s\n",
      "13:\tlearn: 0.3270105\ttest: 0.3193902\tbest: 0.3193902 (13)\ttotal: 3m 5s\tremaining: 41m\n",
      "14:\tlearn: 0.3412073\ttest: 0.3335850\tbest: 0.3335850 (14)\ttotal: 3m 18s\tremaining: 40m 51s\n",
      "15:\tlearn: 0.3519301\ttest: 0.3430131\tbest: 0.3430131 (15)\ttotal: 3m 31s\tremaining: 40m 37s\n",
      "16:\tlearn: 0.3650758\ttest: 0.3575972\tbest: 0.3575972 (16)\ttotal: 3m 45s\tremaining: 40m 23s\n",
      "17:\tlearn: 0.3749250\ttest: 0.3674638\tbest: 0.3674638 (17)\ttotal: 3m 58s\tremaining: 40m 11s\n",
      "18:\tlearn: 0.3788806\ttest: 0.3702777\tbest: 0.3702777 (18)\ttotal: 4m 11s\tremaining: 39m 59s\n",
      "19:\tlearn: 0.3969407\ttest: 0.3864469\tbest: 0.3864469 (19)\ttotal: 4m 25s\tremaining: 39m 50s\n",
      "20:\tlearn: 0.4168000\ttest: 0.4051633\tbest: 0.4051633 (20)\ttotal: 4m 39s\tremaining: 39m 39s\n",
      "21:\tlearn: 0.4297331\ttest: 0.4170392\tbest: 0.4170392 (21)\ttotal: 4m 52s\tremaining: 39m 27s\n",
      "22:\tlearn: 0.4441988\ttest: 0.4322707\tbest: 0.4322707 (22)\ttotal: 5m 5s\tremaining: 39m 13s\n",
      "23:\tlearn: 0.4547564\ttest: 0.4426364\tbest: 0.4426364 (23)\ttotal: 5m 18s\tremaining: 38m 57s\n",
      "24:\tlearn: 0.4643796\ttest: 0.4508763\tbest: 0.4508763 (24)\ttotal: 5m 31s\tremaining: 38m 40s\n",
      "25:\tlearn: 0.4729841\ttest: 0.4602823\tbest: 0.4602823 (25)\ttotal: 5m 44s\tremaining: 38m 26s\n",
      "26:\tlearn: 0.4858983\ttest: 0.4704922\tbest: 0.4704922 (26)\ttotal: 5m 58s\tremaining: 38m 14s\n",
      "27:\tlearn: 0.4951267\ttest: 0.4781795\tbest: 0.4781795 (27)\ttotal: 6m 11s\tremaining: 38m\n",
      "28:\tlearn: 0.5052080\ttest: 0.4861333\tbest: 0.4861333 (28)\ttotal: 6m 24s\tremaining: 37m 48s\n",
      "29:\tlearn: 0.5132295\ttest: 0.4955061\tbest: 0.4955061 (29)\ttotal: 6m 38s\tremaining: 37m 36s\n",
      "30:\tlearn: 0.5174941\ttest: 0.5011067\tbest: 0.5011067 (30)\ttotal: 6m 51s\tremaining: 37m 22s\n",
      "31:\tlearn: 0.5288191\ttest: 0.5138214\tbest: 0.5138214 (31)\ttotal: 7m 4s\tremaining: 37m 6s\n",
      "32:\tlearn: 0.5381525\ttest: 0.5221347\tbest: 0.5221347 (32)\ttotal: 7m 17s\tremaining: 36m 53s\n",
      "33:\tlearn: 0.5452975\ttest: 0.5286406\tbest: 0.5286406 (33)\ttotal: 7m 31s\tremaining: 36m 43s\n",
      "34:\tlearn: 0.5492696\ttest: 0.5322450\tbest: 0.5322450 (34)\ttotal: 7m 44s\tremaining: 36m 28s\n",
      "35:\tlearn: 0.5573648\ttest: 0.5400284\tbest: 0.5400284 (35)\ttotal: 7m 57s\tremaining: 36m 13s\n",
      "36:\tlearn: 0.5642309\ttest: 0.5469337\tbest: 0.5469337 (36)\ttotal: 8m 9s\tremaining: 35m 56s\n",
      "37:\tlearn: 0.5646511\ttest: 0.5484676\tbest: 0.5484676 (37)\ttotal: 8m 22s\tremaining: 35m 42s\n",
      "38:\tlearn: 0.5710717\ttest: 0.5544745\tbest: 0.5544745 (38)\ttotal: 8m 36s\tremaining: 35m 30s\n",
      "39:\tlearn: 0.5774128\ttest: 0.5591961\tbest: 0.5591961 (39)\ttotal: 8m 49s\tremaining: 35m 16s\n",
      "40:\tlearn: 0.5807125\ttest: 0.5622255\tbest: 0.5622255 (40)\ttotal: 9m 2s\tremaining: 35m 3s\n",
      "41:\tlearn: 0.5865083\ttest: 0.5682222\tbest: 0.5682222 (41)\ttotal: 9m 15s\tremaining: 34m 50s\n",
      "42:\tlearn: 0.5896949\ttest: 0.5706112\tbest: 0.5706112 (42)\ttotal: 9m 29s\tremaining: 34m 38s\n",
      "43:\tlearn: 0.5927070\ttest: 0.5748423\tbest: 0.5748423 (43)\ttotal: 9m 42s\tremaining: 34m 23s\n",
      "44:\tlearn: 0.5929134\ttest: 0.5750629\tbest: 0.5750629 (44)\ttotal: 9m 55s\tremaining: 34m 9s\n",
      "45:\tlearn: 0.6017721\ttest: 0.5851800\tbest: 0.5851800 (45)\ttotal: 10m 8s\tremaining: 33m 56s\n",
      "46:\tlearn: 0.5994155\ttest: 0.5836751\tbest: 0.5851800 (45)\ttotal: 10m 21s\tremaining: 33m 41s\n",
      "47:\tlearn: 0.6052087\ttest: 0.5855837\tbest: 0.5855837 (47)\ttotal: 10m 34s\tremaining: 33m 28s\n",
      "48:\tlearn: 0.6077720\ttest: 0.5883856\tbest: 0.5883856 (48)\ttotal: 10m 47s\tremaining: 33m 15s\n",
      "49:\tlearn: 0.6138813\ttest: 0.5947954\tbest: 0.5947954 (49)\ttotal: 11m 1s\tremaining: 33m 3s\n",
      "50:\tlearn: 0.6193449\ttest: 0.6001686\tbest: 0.6001686 (50)\ttotal: 11m 14s\tremaining: 32m 49s\n",
      "51:\tlearn: 0.6200935\ttest: 0.6014665\tbest: 0.6014665 (51)\ttotal: 11m 27s\tremaining: 32m 36s\n",
      "52:\tlearn: 0.6214887\ttest: 0.6054703\tbest: 0.6054703 (52)\ttotal: 11m 40s\tremaining: 32m 22s\n",
      "53:\tlearn: 0.6254281\ttest: 0.6084758\tbest: 0.6084758 (53)\ttotal: 11m 53s\tremaining: 32m 9s\n",
      "54:\tlearn: 0.6288667\ttest: 0.6112821\tbest: 0.6112821 (54)\ttotal: 12m 6s\tremaining: 31m 55s\n",
      "55:\tlearn: 0.6347341\ttest: 0.6186125\tbest: 0.6186125 (55)\ttotal: 12m 20s\tremaining: 31m 43s\n",
      "56:\tlearn: 0.6408160\ttest: 0.6242148\tbest: 0.6242148 (56)\ttotal: 12m 32s\tremaining: 31m 28s\n",
      "57:\tlearn: 0.6402818\ttest: 0.6238052\tbest: 0.6242148 (56)\ttotal: 12m 45s\tremaining: 31m 14s\n",
      "58:\tlearn: 0.6416862\ttest: 0.6244244\tbest: 0.6244244 (58)\ttotal: 12m 58s\tremaining: 31m 1s\n",
      "59:\tlearn: 0.6476424\ttest: 0.6300727\tbest: 0.6300727 (59)\ttotal: 13m 11s\tremaining: 30m 47s\n",
      "60:\tlearn: 0.6467881\ttest: 0.6294569\tbest: 0.6300727 (59)\ttotal: 13m 25s\tremaining: 30m 34s\n",
      "61:\tlearn: 0.6509512\ttest: 0.6339734\tbest: 0.6339734 (61)\ttotal: 13m 38s\tremaining: 30m 20s\n",
      "62:\tlearn: 0.6561841\ttest: 0.6380827\tbest: 0.6380827 (62)\ttotal: 13m 51s\tremaining: 30m 7s\n",
      "63:\tlearn: 0.6609878\ttest: 0.6422031\tbest: 0.6422031 (63)\ttotal: 14m 4s\tremaining: 29m 54s\n",
      "64:\tlearn: 0.6623102\ttest: 0.6440453\tbest: 0.6440453 (64)\ttotal: 14m 17s\tremaining: 29m 40s\n",
      "65:\tlearn: 0.6651300\ttest: 0.6471650\tbest: 0.6471650 (65)\ttotal: 14m 30s\tremaining: 29m 26s\n",
      "66:\tlearn: 0.6706943\ttest: 0.6542331\tbest: 0.6542331 (66)\ttotal: 14m 43s\tremaining: 29m 13s\n",
      "67:\tlearn: 0.6750619\ttest: 0.6574856\tbest: 0.6574856 (67)\ttotal: 14m 56s\tremaining: 29m\n",
      "68:\tlearn: 0.6787546\ttest: 0.6596446\tbest: 0.6596446 (68)\ttotal: 15m 10s\tremaining: 28m 48s\n",
      "69:\tlearn: 0.6814064\ttest: 0.6605167\tbest: 0.6605167 (69)\ttotal: 15m 23s\tremaining: 28m 34s\n",
      "70:\tlearn: 0.6864139\ttest: 0.6646431\tbest: 0.6646431 (70)\ttotal: 15m 36s\tremaining: 28m 21s\n",
      "71:\tlearn: 0.6872682\ttest: 0.6660944\tbest: 0.6660944 (71)\ttotal: 15m 49s\tremaining: 28m 8s\n",
      "72:\tlearn: 0.6901036\ttest: 0.6664632\tbest: 0.6664632 (72)\ttotal: 16m 3s\tremaining: 27m 55s\n",
      "73:\tlearn: 0.6924740\ttest: 0.6705010\tbest: 0.6705010 (73)\ttotal: 16m 16s\tremaining: 27m 42s\n",
      "74:\tlearn: 0.6945714\ttest: 0.6742458\tbest: 0.6742458 (74)\ttotal: 16m 29s\tremaining: 27m 29s\n",
      "75:\tlearn: 0.6979900\ttest: 0.6791772\tbest: 0.6791772 (75)\ttotal: 16m 42s\tremaining: 27m 15s\n",
      "76:\tlearn: 0.6992726\ttest: 0.6800152\tbest: 0.6800152 (76)\ttotal: 16m 55s\tremaining: 27m 2s\n",
      "77:\tlearn: 0.6979741\ttest: 0.6793594\tbest: 0.6800152 (76)\ttotal: 17m 8s\tremaining: 26m 48s\n",
      "78:\tlearn: 0.7025462\ttest: 0.6834985\tbest: 0.6834985 (78)\ttotal: 17m 21s\tremaining: 26m 34s\n",
      "79:\tlearn: 0.7027577\ttest: 0.6839260\tbest: 0.6839260 (79)\ttotal: 17m 34s\tremaining: 26m 21s\n",
      "80:\tlearn: 0.7029832\ttest: 0.6836408\tbest: 0.6839260 (79)\ttotal: 17m 48s\tremaining: 26m 9s\n",
      "81:\tlearn: 0.7025530\ttest: 0.6810500\tbest: 0.6839260 (79)\ttotal: 18m 1s\tremaining: 25m 56s\n",
      "82:\tlearn: 0.7071341\ttest: 0.6853901\tbest: 0.6853901 (82)\ttotal: 18m 14s\tremaining: 25m 42s\n",
      "83:\tlearn: 0.7084997\ttest: 0.6882074\tbest: 0.6882074 (83)\ttotal: 18m 27s\tremaining: 25m 29s\n",
      "84:\tlearn: 0.7090297\ttest: 0.6879834\tbest: 0.6882074 (83)\ttotal: 18m 40s\tremaining: 25m 16s\n",
      "85:\tlearn: 0.7095554\ttest: 0.6871854\tbest: 0.6882074 (83)\ttotal: 18m 54s\tremaining: 25m 3s\n",
      "86:\tlearn: 0.7109499\ttest: 0.6888862\tbest: 0.6888862 (86)\ttotal: 19m 7s\tremaining: 24m 49s\n",
      "87:\tlearn: 0.7124379\ttest: 0.6897736\tbest: 0.6897736 (87)\ttotal: 19m 20s\tremaining: 24m 36s\n",
      "88:\tlearn: 0.7177338\ttest: 0.6933286\tbest: 0.6933286 (88)\ttotal: 19m 33s\tremaining: 24m 23s\n",
      "89:\tlearn: 0.7200114\ttest: 0.6952117\tbest: 0.6952117 (89)\ttotal: 19m 46s\tremaining: 24m 10s\n",
      "90:\tlearn: 0.7202280\ttest: 0.6954118\tbest: 0.6954118 (90)\ttotal: 19m 59s\tremaining: 23m 56s\n",
      "91:\tlearn: 0.7229967\ttest: 0.6986627\tbest: 0.6986627 (91)\ttotal: 20m 12s\tremaining: 23m 42s\n",
      "92:\tlearn: 0.7239268\ttest: 0.7013505\tbest: 0.7013505 (92)\ttotal: 20m 25s\tremaining: 23m 30s\n",
      "93:\tlearn: 0.7245815\ttest: 0.7022311\tbest: 0.7022311 (93)\ttotal: 20m 39s\tremaining: 23m 17s\n",
      "94:\tlearn: 0.7268241\ttest: 0.7026910\tbest: 0.7026910 (94)\ttotal: 20m 52s\tremaining: 23m 4s\n",
      "95:\tlearn: 0.7262772\ttest: 0.7029065\tbest: 0.7029065 (95)\ttotal: 21m 5s\tremaining: 22m 51s\n",
      "96:\tlearn: 0.7262862\ttest: 0.7024713\tbest: 0.7029065 (95)\ttotal: 21m 18s\tremaining: 22m 37s\n",
      "97:\tlearn: 0.7283194\ttest: 0.7037846\tbest: 0.7037846 (97)\ttotal: 21m 32s\tremaining: 22m 24s\n",
      "98:\tlearn: 0.7275678\ttest: 0.7025462\tbest: 0.7037846 (97)\ttotal: 21m 45s\tremaining: 22m 11s\n",
      "99:\tlearn: 0.7290708\ttest: 0.7044642\tbest: 0.7044642 (99)\ttotal: 21m 58s\tremaining: 21m 58s\n",
      "100:\tlearn: 0.7312205\ttest: 0.7068174\tbest: 0.7068174 (100)\ttotal: 22m 11s\tremaining: 21m 45s\n",
      "101:\tlearn: 0.7322207\ttest: 0.7091025\tbest: 0.7091025 (101)\ttotal: 22m 25s\tremaining: 21m 32s\n",
      "102:\tlearn: 0.7326578\ttest: 0.7093103\tbest: 0.7093103 (102)\ttotal: 22m 38s\tremaining: 21m 18s\n",
      "103:\tlearn: 0.7319043\ttest: 0.7084637\tbest: 0.7093103 (102)\ttotal: 22m 51s\tremaining: 21m 5s\n",
      "104:\tlearn: 0.7343695\ttest: 0.7095692\tbest: 0.7095692 (104)\ttotal: 23m 4s\tremaining: 20m 52s\n",
      "105:\tlearn: 0.7359555\ttest: 0.7128396\tbest: 0.7128396 (105)\ttotal: 23m 17s\tremaining: 20m 39s\n",
      "106:\tlearn: 0.7360453\ttest: 0.7134826\tbest: 0.7134826 (106)\ttotal: 23m 30s\tremaining: 20m 25s\n",
      "107:\tlearn: 0.7357230\ttest: 0.7139016\tbest: 0.7139016 (107)\ttotal: 23m 43s\tremaining: 20m 12s\n",
      "108:\tlearn: 0.7362659\ttest: 0.7132663\tbest: 0.7139016 (107)\ttotal: 23m 56s\tremaining: 19m 59s\n",
      "109:\tlearn: 0.7377751\ttest: 0.7138744\tbest: 0.7139016 (107)\ttotal: 24m 10s\tremaining: 19m 46s\n",
      "110:\tlearn: 0.7395118\ttest: 0.7168034\tbest: 0.7168034 (110)\ttotal: 24m 23s\tremaining: 19m 33s\n",
      "111:\tlearn: 0.7409390\ttest: 0.7163299\tbest: 0.7168034 (110)\ttotal: 24m 36s\tremaining: 19m 20s\n",
      "112:\tlearn: 0.7435948\ttest: 0.7193652\tbest: 0.7193652 (112)\ttotal: 24m 49s\tremaining: 19m 7s\n",
      "113:\tlearn: 0.7430549\ttest: 0.7193457\tbest: 0.7193652 (112)\ttotal: 25m 2s\tremaining: 18m 53s\n",
      "114:\tlearn: 0.7446638\ttest: 0.7208378\tbest: 0.7208378 (114)\ttotal: 25m 16s\tremaining: 18m 40s\n",
      "115:\tlearn: 0.7474833\ttest: 0.7227081\tbest: 0.7227081 (115)\ttotal: 25m 28s\tremaining: 18m 27s\n",
      "116:\tlearn: 0.7481501\ttest: 0.7216690\tbest: 0.7227081 (115)\ttotal: 25m 42s\tremaining: 18m 13s\n",
      "117:\tlearn: 0.7482488\ttest: 0.7218769\tbest: 0.7227081 (115)\ttotal: 25m 55s\tremaining: 18m\n",
      "118:\tlearn: 0.7486988\ttest: 0.7231280\tbest: 0.7231280 (118)\ttotal: 26m 8s\tremaining: 17m 47s\n",
      "119:\tlearn: 0.7505174\ttest: 0.7226800\tbest: 0.7231280 (118)\ttotal: 26m 21s\tremaining: 17m 34s\n",
      "120:\tlearn: 0.7521665\ttest: 0.7249847\tbest: 0.7249847 (120)\ttotal: 26m 34s\tremaining: 17m 20s\n",
      "121:\tlearn: 0.7522713\ttest: 0.7251848\tbest: 0.7251848 (121)\ttotal: 26m 48s\tremaining: 17m 8s\n",
      "122:\tlearn: 0.7527054\ttest: 0.7258083\tbest: 0.7258083 (122)\ttotal: 27m\tremaining: 16m 54s\n",
      "123:\tlearn: 0.7546279\ttest: 0.7264589\tbest: 0.7264589 (123)\ttotal: 27m 14s\tremaining: 16m 42s\n",
      "124:\tlearn: 0.7547357\ttest: 0.7260280\tbest: 0.7264589 (123)\ttotal: 27m 27s\tremaining: 16m 28s\n",
      "125:\tlearn: 0.7555841\ttest: 0.7269027\tbest: 0.7269027 (125)\ttotal: 27m 40s\tremaining: 16m 15s\n",
      "126:\tlearn: 0.7578056\ttest: 0.7290881\tbest: 0.7290881 (126)\ttotal: 27m 54s\tremaining: 16m 2s\n",
      "127:\tlearn: 0.7600862\ttest: 0.7320375\tbest: 0.7320375 (127)\ttotal: 28m 7s\tremaining: 15m 49s\n",
      "128:\tlearn: 0.7612611\ttest: 0.7324642\tbest: 0.7324642 (128)\ttotal: 28m 20s\tremaining: 15m 36s\n",
      "129:\tlearn: 0.7605981\ttest: 0.7322691\tbest: 0.7324642 (128)\ttotal: 28m 34s\tremaining: 15m 23s\n",
      "130:\tlearn: 0.7627129\ttest: 0.7348778\tbest: 0.7348778 (130)\ttotal: 28m 47s\tremaining: 15m 9s\n",
      "131:\tlearn: 0.7631401\ttest: 0.7348787\tbest: 0.7348787 (131)\ttotal: 29m\tremaining: 14m 56s\n",
      "132:\tlearn: 0.7651743\ttest: 0.7365752\tbest: 0.7365752 (132)\ttotal: 29m 13s\tremaining: 14m 43s\n",
      "133:\tlearn: 0.7663731\ttest: 0.7374337\tbest: 0.7374337 (133)\ttotal: 29m 26s\tremaining: 14m 30s\n",
      "134:\tlearn: 0.7664778\ttest: 0.7373903\tbest: 0.7374337 (133)\ttotal: 29m 40s\tremaining: 14m 17s\n",
      "135:\tlearn: 0.7678683\ttest: 0.7371782\tbest: 0.7374337 (133)\ttotal: 29m 54s\tremaining: 14m 4s\n",
      "136:\tlearn: 0.7687227\ttest: 0.7384906\tbest: 0.7384906 (136)\ttotal: 30m 7s\tremaining: 13m 51s\n",
      "137:\tlearn: 0.7692687\ttest: 0.7395373\tbest: 0.7395373 (137)\ttotal: 30m 20s\tremaining: 13m 37s\n",
      "138:\tlearn: 0.7704424\ttest: 0.7401965\tbest: 0.7401965 (138)\ttotal: 30m 33s\tremaining: 13m 24s\n",
      "139:\tlearn: 0.7704364\ttest: 0.7404197\tbest: 0.7404197 (139)\ttotal: 30m 46s\tremaining: 13m 11s\n",
      "140:\tlearn: 0.7705379\ttest: 0.7404120\tbest: 0.7404197 (139)\ttotal: 30m 59s\tremaining: 12m 58s\n",
      "141:\tlearn: 0.7728905\ttest: 0.7416929\tbest: 0.7416929 (141)\ttotal: 31m 12s\tremaining: 12m 44s\n",
      "142:\tlearn: 0.7725632\ttest: 0.7391872\tbest: 0.7416929 (141)\ttotal: 31m 26s\tremaining: 12m 31s\n",
      "143:\tlearn: 0.7731968\ttest: 0.7417942\tbest: 0.7417942 (143)\ttotal: 31m 39s\tremaining: 12m 18s\n",
      "144:\tlearn: 0.7739524\ttest: 0.7422175\tbest: 0.7422175 (144)\ttotal: 31m 52s\tremaining: 12m 5s\n",
      "145:\tlearn: 0.7734214\ttest: 0.7409170\tbest: 0.7422175 (144)\ttotal: 32m 5s\tremaining: 11m 52s\n",
      "146:\tlearn: 0.7740642\ttest: 0.7421911\tbest: 0.7422175 (144)\ttotal: 32m 18s\tremaining: 11m 39s\n",
      "147:\tlearn: 0.7740621\ttest: 0.7428341\tbest: 0.7428341 (147)\ttotal: 32m 31s\tremaining: 11m 25s\n",
      "148:\tlearn: 0.7748227\ttest: 0.7436730\tbest: 0.7436730 (148)\ttotal: 32m 45s\tremaining: 11m 12s\n",
      "149:\tlearn: 0.7776991\ttest: 0.7458610\tbest: 0.7458610 (149)\ttotal: 32m 58s\tremaining: 10m 59s\n",
      "150:\tlearn: 0.7798380\ttest: 0.7475950\tbest: 0.7475950 (150)\ttotal: 33m 12s\tremaining: 10m 46s\n",
      "151:\tlearn: 0.7803700\ttest: 0.7478071\tbest: 0.7478071 (151)\ttotal: 33m 25s\tremaining: 10m 33s\n",
      "152:\tlearn: 0.7809060\ttest: 0.7492966\tbest: 0.7492966 (152)\ttotal: 33m 38s\tremaining: 10m 20s\n",
      "153:\tlearn: 0.7808023\ttest: 0.7493000\tbest: 0.7493000 (153)\ttotal: 33m 52s\tremaining: 10m 7s\n",
      "154:\tlearn: 0.7832646\ttest: 0.7520978\tbest: 0.7520978 (154)\ttotal: 34m 5s\tremaining: 9m 53s\n",
      "155:\tlearn: 0.7853098\ttest: 0.7542338\tbest: 0.7542338 (155)\ttotal: 34m 18s\tremaining: 9m 40s\n",
      "156:\tlearn: 0.7870892\ttest: 0.7562233\tbest: 0.7562233 (156)\ttotal: 34m 31s\tremaining: 9m 27s\n",
      "157:\tlearn: 0.7876282\ttest: 0.7577358\tbest: 0.7577358 (157)\ttotal: 34m 44s\tremaining: 9m 14s\n",
      "158:\tlearn: 0.7886991\ttest: 0.7581600\tbest: 0.7581600 (158)\ttotal: 34m 57s\tremaining: 9m\n",
      "159:\tlearn: 0.7908599\ttest: 0.7592033\tbest: 0.7592033 (159)\ttotal: 35m 10s\tremaining: 8m 47s\n",
      "160:\tlearn: 0.7900025\ttest: 0.7598182\tbest: 0.7598182 (160)\ttotal: 35m 23s\tremaining: 8m 34s\n",
      "161:\tlearn: 0.7921386\ttest: 0.7617694\tbest: 0.7617694 (161)\ttotal: 35m 37s\tremaining: 8m 21s\n",
      "162:\tlearn: 0.7903091\ttest: 0.7600515\tbest: 0.7617694 (161)\ttotal: 35m 50s\tremaining: 8m 8s\n",
      "163:\tlearn: 0.7900925\ttest: 0.7598395\tbest: 0.7617694 (161)\ttotal: 36m 3s\tremaining: 7m 54s\n",
      "164:\tlearn: 0.7901983\ttest: 0.7604706\tbest: 0.7617694 (161)\ttotal: 36m 16s\tremaining: 7m 41s\n",
      "165:\tlearn: 0.7913831\ttest: 0.7602551\tbest: 0.7617694 (161)\ttotal: 36m 29s\tremaining: 7m 28s\n",
      "166:\tlearn: 0.7921446\ttest: 0.7625530\tbest: 0.7625530 (166)\ttotal: 36m 42s\tremaining: 7m 15s\n",
      "167:\tlearn: 0.7938442\ttest: 0.7645263\tbest: 0.7645263 (167)\ttotal: 36m 55s\tremaining: 7m 2s\n",
      "168:\tlearn: 0.7948134\ttest: 0.7643261\tbest: 0.7645263 (167)\ttotal: 37m 8s\tremaining: 6m 48s\n",
      "169:\tlearn: 0.7950230\ttest: 0.7643261\tbest: 0.7645263 (167)\ttotal: 37m 21s\tremaining: 6m 35s\n",
      "170:\tlearn: 0.7950220\ttest: 0.7649572\tbest: 0.7649572 (170)\ttotal: 37m 35s\tremaining: 6m 22s\n",
      "171:\tlearn: 0.7954462\ttest: 0.7643066\tbest: 0.7649572 (170)\ttotal: 37m 48s\tremaining: 6m 9s\n",
      "172:\tlearn: 0.7957796\ttest: 0.7634710\tbest: 0.7649572 (170)\ttotal: 38m 2s\tremaining: 5m 56s\n",
      "173:\tlearn: 0.7968506\ttest: 0.7649674\tbest: 0.7649674 (173)\ttotal: 38m 15s\tremaining: 5m 42s\n",
      "174:\tlearn: 0.7962228\ttest: 0.7623392\tbest: 0.7649674 (173)\ttotal: 38m 28s\tremaining: 5m 29s\n",
      "175:\tlearn: 0.7968586\ttest: 0.7642920\tbest: 0.7649674 (173)\ttotal: 38m 41s\tremaining: 5m 16s\n",
      "176:\tlearn: 0.7971810\ttest: 0.7653694\tbest: 0.7653694 (176)\ttotal: 38m 54s\tremaining: 5m 3s\n",
      "177:\tlearn: 0.7970772\ttest: 0.7653694\tbest: 0.7653694 (176)\ttotal: 39m 7s\tremaining: 4m 50s\n",
      "178:\tlearn: 0.7973986\ttest: 0.7659928\tbest: 0.7659928 (178)\ttotal: 39m 20s\tremaining: 4m 36s\n",
      "179:\tlearn: 0.7985893\ttest: 0.7653388\tbest: 0.7659928 (178)\ttotal: 39m 34s\tremaining: 4m 23s\n",
      "180:\tlearn: 0.7982500\ttest: 0.7645228\tbest: 0.7659928 (178)\ttotal: 39m 47s\tremaining: 4m 10s\n",
      "181:\tlearn: 0.7983557\ttest: 0.7643031\tbest: 0.7659928 (178)\ttotal: 40m\tremaining: 3m 57s\n",
      "182:\tlearn: 0.7987890\ttest: 0.7653498\tbest: 0.7659928 (178)\ttotal: 40m 14s\tremaining: 3m 44s\n",
      "183:\tlearn: 0.8001983\ttest: 0.7683052\tbest: 0.7683052 (183)\ttotal: 40m 27s\tremaining: 3m 31s\n",
      "184:\tlearn: 0.7999817\ttest: 0.7682890\tbest: 0.7683052 (183)\ttotal: 40m 40s\tremaining: 3m 17s\n",
      "185:\tlearn: 0.8003071\ttest: 0.7684857\tbest: 0.7684857 (185)\ttotal: 40m 54s\tremaining: 3m 4s\n",
      "186:\tlearn: 0.8010567\ttest: 0.7684934\tbest: 0.7684934 (186)\ttotal: 41m 7s\tremaining: 2m 51s\n",
      "187:\tlearn: 0.8014809\ttest: 0.7691398\tbest: 0.7691398 (187)\ttotal: 41m 20s\tremaining: 2m 38s\n",
      "188:\tlearn: 0.8011655\ttest: 0.7687123\tbest: 0.7691398 (187)\ttotal: 41m 33s\tremaining: 2m 25s\n",
      "189:\tlearn: 0.8010617\ttest: 0.7685087\tbest: 0.7691398 (187)\ttotal: 41m 46s\tremaining: 2m 11s\n",
      "190:\tlearn: 0.8011665\ttest: 0.7680531\tbest: 0.7691398 (187)\ttotal: 41m 59s\tremaining: 1m 58s\n",
      "191:\tlearn: 0.8022455\ttest: 0.7697428\tbest: 0.7697428 (191)\ttotal: 42m 12s\tremaining: 1m 45s\n",
      "192:\tlearn: 0.8032047\ttest: 0.7703892\tbest: 0.7703892 (192)\ttotal: 42m 26s\tremaining: 1m 32s\n",
      "193:\tlearn: 0.8032027\ttest: 0.7701814\tbest: 0.7703892 (192)\ttotal: 42m 39s\tremaining: 1m 19s\n",
      "194:\tlearn: 0.8040470\ttest: 0.7723549\tbest: 0.7723549 (194)\ttotal: 42m 52s\tremaining: 1m 5s\n",
      "195:\tlearn: 0.8041558\ttest: 0.7721394\tbest: 0.7723549 (194)\ttotal: 43m 5s\tremaining: 52.8s\n",
      "196:\tlearn: 0.8050162\ttest: 0.7734135\tbest: 0.7734135 (196)\ttotal: 43m 18s\tremaining: 39.6s\n",
      "197:\tlearn: 0.8050421\ttest: 0.7740064\tbest: 0.7740064 (197)\ttotal: 43m 31s\tremaining: 26.4s\n",
      "198:\tlearn: 0.8068355\ttest: 0.7755326\tbest: 0.7755326 (198)\ttotal: 43m 45s\tremaining: 13.2s\n",
      "199:\tlearn: 0.8063034\ttest: 0.7748776\tbest: 0.7755326 (198)\ttotal: 43m 58s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "n_classes = 128\n",
    "seed = 555\n",
    "n = len(meta_features_list)\n",
    "\n",
    "_X = X.values\n",
    "_y = y.values\n",
    "\n",
    "splt = StratifiedShuffleSplit(n_splits=7, test_size=0.25, random_state=seed)\n",
    "train_index, test_index = next(splt.split(_X, _y))\n",
    "\n",
    "_X_train = _X[train_index, :]\n",
    "_X_test = _X[test_index, :]\n",
    "_y_train = _y[train_index]\n",
    "_y_test = _y[test_index]\n",
    "\n",
    "\n",
    "cat_trainval = cat.Pool(_X_train, label=_y_train)\n",
    "cat_test = cat.Pool(_X_test)\n",
    "\n",
    "params = {\n",
    "    \"iterations\": 200,\n",
    "    \"loss_function\": \"MultiClass\",\n",
    "    \"eval_metric\": \"Accuracy\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"l2_leaf_reg\": 3,\n",
    "    \"depth\": 4,\n",
    "    \"od_type\": \"Iter\",\n",
    "    \"od_wait\": 5,    \n",
    "}\n",
    "\n",
    "cv_results = cat.cv(cat_trainval, params, nfold=5, stratified=True, seed=seed, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.0370608\ttotal: 5.89s\tremaining: 19m 21s\n",
      "1:\tlearn: 0.0668380\ttotal: 12.2s\tremaining: 19m 52s\n",
      "2:\tlearn: 0.0972579\ttotal: 18.3s\tremaining: 19m 51s\n",
      "3:\tlearn: 0.1261782\ttotal: 24.6s\tremaining: 19m 50s\n",
      "4:\tlearn: 0.1559554\ttotal: 30.7s\tremaining: 19m 43s\n",
      "5:\tlearn: 0.1767352\ttotal: 37.5s\tremaining: 19m 59s\n",
      "6:\tlearn: 0.1981577\ttotal: 43.6s\tremaining: 19m 50s\n",
      "7:\tlearn: 0.2285775\ttotal: 50.9s\tremaining: 20m 8s\n",
      "8:\tlearn: 0.2549272\ttotal: 57.4s\tremaining: 20m 4s\n",
      "9:\tlearn: 0.2819195\ttotal: 1m 3s\tremaining: 19m 52s\n",
      "10:\tlearn: 0.3005570\ttotal: 1m 9s\tremaining: 19m 44s\n",
      "11:\tlearn: 0.3157669\ttotal: 1m 15s\tremaining: 19m 34s\n",
      "12:\tlearn: 0.3374036\ttotal: 1m 21s\tremaining: 19m 26s\n",
      "13:\tlearn: 0.3515424\ttotal: 1m 27s\tremaining: 19m 15s\n",
      "14:\tlearn: 0.3658955\ttotal: 1m 33s\tremaining: 19m 6s\n",
      "15:\tlearn: 0.3804627\ttotal: 1m 40s\tremaining: 18m 59s\n",
      "16:\tlearn: 0.3943873\ttotal: 1m 46s\tremaining: 18m 53s\n",
      "17:\tlearn: 0.4175236\ttotal: 1m 53s\tremaining: 18m 50s\n",
      "18:\tlearn: 0.4404456\ttotal: 1m 59s\tremaining: 18m 41s\n",
      "19:\tlearn: 0.4404456\ttotal: 2m 5s\tremaining: 18m 36s\n",
      "20:\tlearn: 0.4468723\ttotal: 2m 11s\tremaining: 18m 27s\n",
      "21:\tlearn: 0.4685090\ttotal: 2m 17s\tremaining: 18m 22s\n",
      "22:\tlearn: 0.4753642\ttotal: 2m 24s\tremaining: 18m 16s\n",
      "23:\tlearn: 0.4895030\ttotal: 2m 30s\tremaining: 18m 10s\n",
      "24:\tlearn: 0.5017138\ttotal: 2m 37s\tremaining: 18m 6s\n",
      "25:\tlearn: 0.5070694\ttotal: 2m 43s\tremaining: 17m 58s\n",
      "26:\tlearn: 0.5147815\ttotal: 2m 49s\tremaining: 17m 52s\n",
      "27:\tlearn: 0.5182091\ttotal: 2m 56s\tremaining: 17m 51s\n",
      "28:\tlearn: 0.5323479\ttotal: 3m 2s\tremaining: 17m 45s\n",
      "29:\tlearn: 0.5329906\ttotal: 3m 9s\tremaining: 17m 38s\n",
      "30:\tlearn: 0.5407027\ttotal: 3m 15s\tremaining: 17m 33s\n",
      "31:\tlearn: 0.5479863\ttotal: 3m 21s\tremaining: 17m 26s\n",
      "32:\tlearn: 0.5526992\ttotal: 3m 27s\tremaining: 17m 19s\n",
      "33:\tlearn: 0.5586975\ttotal: 3m 33s\tremaining: 17m 11s\n",
      "34:\tlearn: 0.5698372\ttotal: 3m 39s\tremaining: 17m 4s\n",
      "35:\tlearn: 0.5771208\ttotal: 3m 46s\tremaining: 17m\n",
      "36:\tlearn: 0.5764781\ttotal: 3m 53s\tremaining: 16m 55s\n",
      "37:\tlearn: 0.5822622\ttotal: 3m 59s\tremaining: 16m 48s\n",
      "38:\tlearn: 0.5826907\ttotal: 4m 5s\tremaining: 16m 41s\n",
      "39:\tlearn: 0.5848329\ttotal: 4m 11s\tremaining: 16m 34s\n",
      "40:\tlearn: 0.5929734\ttotal: 4m 18s\tremaining: 16m 29s\n",
      "41:\tlearn: 0.6079692\ttotal: 4m 24s\tremaining: 16m 21s\n",
      "42:\tlearn: 0.6073265\ttotal: 4m 30s\tremaining: 16m 14s\n",
      "43:\tlearn: 0.6201799\ttotal: 4m 36s\tremaining: 16m 8s\n",
      "44:\tlearn: 0.6246787\ttotal: 4m 43s\tremaining: 16m 2s\n",
      "45:\tlearn: 0.6274636\ttotal: 4m 49s\tremaining: 15m 56s\n",
      "46:\tlearn: 0.6341045\ttotal: 4m 55s\tremaining: 15m 50s\n",
      "47:\tlearn: 0.6495287\ttotal: 5m 2s\tremaining: 15m 44s\n",
      "48:\tlearn: 0.6518852\ttotal: 5m 8s\tremaining: 15m 39s\n",
      "49:\tlearn: 0.6525278\ttotal: 5m 15s\tremaining: 15m 32s\n",
      "50:\tlearn: 0.6587404\ttotal: 5m 21s\tremaining: 15m 26s\n",
      "51:\tlearn: 0.6610968\ttotal: 5m 27s\tremaining: 15m 19s\n",
      "52:\tlearn: 0.6602399\ttotal: 5m 33s\tremaining: 15m 12s\n",
      "53:\tlearn: 0.6591688\ttotal: 5m 39s\tremaining: 15m 6s\n",
      "54:\tlearn: 0.6610968\ttotal: 5m 46s\tremaining: 15m 1s\n",
      "55:\tlearn: 0.6606684\ttotal: 5m 52s\tremaining: 14m 54s\n",
      "56:\tlearn: 0.6604542\ttotal: 5m 58s\tremaining: 14m 48s\n",
      "57:\tlearn: 0.6703085\ttotal: 6m 5s\tremaining: 14m 42s\n",
      "58:\tlearn: 0.6711654\ttotal: 6m 11s\tremaining: 14m 36s\n",
      "59:\tlearn: 0.6715938\ttotal: 6m 17s\tremaining: 14m 29s\n",
      "60:\tlearn: 0.6752356\ttotal: 6m 23s\tremaining: 14m 22s\n",
      "61:\tlearn: 0.6754499\ttotal: 6m 30s\tremaining: 14m 15s\n",
      "62:\tlearn: 0.6820908\ttotal: 6m 36s\tremaining: 14m 9s\n",
      "63:\tlearn: 0.6846615\ttotal: 6m 42s\tremaining: 14m 2s\n",
      "64:\tlearn: 0.6842331\ttotal: 6m 48s\tremaining: 13m 55s\n",
      "65:\tlearn: 0.6953728\ttotal: 6m 54s\tremaining: 13m 48s\n",
      "66:\tlearn: 0.6998715\ttotal: 7m 1s\tremaining: 13m 43s\n",
      "67:\tlearn: 0.6996572\ttotal: 7m 7s\tremaining: 13m 36s\n",
      "68:\tlearn: 0.6953728\ttotal: 7m 14s\tremaining: 13m 31s\n",
      "69:\tlearn: 0.6953728\ttotal: 7m 20s\tremaining: 13m 25s\n",
      "70:\tlearn: 0.6958012\ttotal: 7m 26s\tremaining: 13m 18s\n",
      "71:\tlearn: 0.7024422\ttotal: 7m 32s\tremaining: 13m 12s\n",
      "72:\tlearn: 0.7020137\ttotal: 7m 39s\tremaining: 13m 6s\n",
      "73:\tlearn: 0.7097258\ttotal: 7m 45s\tremaining: 12m 59s\n",
      "74:\tlearn: 0.7208655\ttotal: 7m 51s\tremaining: 12m 53s\n",
      "75:\tlearn: 0.7240788\ttotal: 7m 57s\tremaining: 12m 46s\n",
      "76:\tlearn: 0.7238646\ttotal: 8m 3s\tremaining: 12m 40s\n",
      "77:\tlearn: 0.7236504\ttotal: 8m 9s\tremaining: 12m 33s\n",
      "78:\tlearn: 0.7253642\ttotal: 8m 15s\tremaining: 12m 26s\n",
      "79:\tlearn: 0.7277207\ttotal: 8m 22s\tremaining: 12m 20s\n",
      "80:\tlearn: 0.7279349\ttotal: 8m 28s\tremaining: 12m 14s\n",
      "81:\tlearn: 0.7279349\ttotal: 8m 34s\tremaining: 12m 8s\n",
      "82:\tlearn: 0.7277207\ttotal: 8m 41s\tremaining: 12m 2s\n",
      "83:\tlearn: 0.7296487\ttotal: 8m 47s\tremaining: 11m 55s\n",
      "84:\tlearn: 0.7345758\ttotal: 8m 53s\tremaining: 11m 49s\n",
      "85:\tlearn: 0.7343616\ttotal: 8m 59s\tremaining: 11m 43s\n",
      "86:\tlearn: 0.7337189\ttotal: 9m 6s\tremaining: 11m 37s\n",
      "87:\tlearn: 0.7386461\ttotal: 9m 12s\tremaining: 11m 30s\n",
      "88:\tlearn: 0.7390746\ttotal: 9m 18s\tremaining: 11m 24s\n",
      "89:\tlearn: 0.7412168\ttotal: 9m 25s\tremaining: 11m 18s\n",
      "90:\tlearn: 0.7418595\ttotal: 9m 31s\tremaining: 11m 11s\n",
      "91:\tlearn: 0.7416452\ttotal: 9m 37s\tremaining: 11m 5s\n",
      "92:\tlearn: 0.7429306\ttotal: 9m 43s\tremaining: 10m 58s\n",
      "93:\tlearn: 0.7433590\ttotal: 9m 49s\tremaining: 10m 52s\n",
      "94:\tlearn: 0.7433590\ttotal: 9m 55s\tremaining: 10m 45s\n",
      "95:\tlearn: 0.7506427\ttotal: 10m 1s\tremaining: 10m 39s\n",
      "96:\tlearn: 0.7521422\ttotal: 10m 8s\tremaining: 10m 33s\n",
      "97:\tlearn: 0.7544987\ttotal: 10m 14s\tremaining: 10m 26s\n",
      "98:\tlearn: 0.7555698\ttotal: 10m 20s\tremaining: 10m 20s\n",
      "99:\tlearn: 0.7555698\ttotal: 10m 27s\tremaining: 10m 14s\n",
      "100:\tlearn: 0.7568552\ttotal: 10m 33s\tremaining: 10m 8s\n",
      "101:\tlearn: 0.7587832\ttotal: 10m 39s\tremaining: 10m 2s\n",
      "102:\tlearn: 0.7572836\ttotal: 10m 46s\tremaining: 9m 56s\n",
      "103:\tlearn: 0.7579263\ttotal: 10m 53s\tremaining: 9m 50s\n",
      "104:\tlearn: 0.7587832\ttotal: 10m 59s\tremaining: 9m 44s\n",
      "105:\tlearn: 0.7570694\ttotal: 11m 6s\tremaining: 9m 38s\n",
      "106:\tlearn: 0.7568552\ttotal: 11m 12s\tremaining: 9m 31s\n",
      "107:\tlearn: 0.7572836\ttotal: 11m 18s\tremaining: 9m 25s\n",
      "108:\tlearn: 0.7577121\ttotal: 11m 24s\tremaining: 9m 18s\n",
      "109:\tlearn: 0.7594259\ttotal: 11m 30s\tremaining: 9m 12s\n",
      "110:\tlearn: 0.7589974\ttotal: 11m 37s\tremaining: 9m 6s\n",
      "111:\tlearn: 0.7587832\ttotal: 11m 43s\tremaining: 9m\n",
      "112:\tlearn: 0.7643530\ttotal: 11m 49s\tremaining: 8m 53s\n",
      "113:\tlearn: 0.7645673\ttotal: 11m 55s\tremaining: 8m 47s\n",
      "114:\tlearn: 0.7654242\ttotal: 12m 2s\tremaining: 8m 41s\n",
      "115:\tlearn: 0.7669237\ttotal: 12m 8s\tremaining: 8m 34s\n",
      "116:\tlearn: 0.7647815\ttotal: 12m 14s\tremaining: 8m 28s\n",
      "117:\tlearn: 0.7643530\ttotal: 12m 21s\tremaining: 8m 22s\n",
      "118:\tlearn: 0.7641388\ttotal: 12m 27s\tremaining: 8m 16s\n",
      "119:\tlearn: 0.7641388\ttotal: 12m 33s\tremaining: 8m 9s\n",
      "120:\tlearn: 0.7624250\ttotal: 12m 40s\tremaining: 8m 3s\n",
      "121:\tlearn: 0.7607112\ttotal: 12m 46s\tremaining: 7m 57s\n",
      "122:\tlearn: 0.7609254\ttotal: 12m 52s\tremaining: 7m 51s\n",
      "123:\tlearn: 0.7654242\ttotal: 12m 59s\tremaining: 7m 44s\n",
      "124:\tlearn: 0.7664953\ttotal: 13m 5s\tremaining: 7m 38s\n",
      "125:\tlearn: 0.7688518\ttotal: 13m 11s\tremaining: 7m 32s\n",
      "126:\tlearn: 0.7686375\ttotal: 13m 17s\tremaining: 7m 25s\n",
      "127:\tlearn: 0.7679949\ttotal: 13m 24s\tremaining: 7m 19s\n",
      "128:\tlearn: 0.7682091\ttotal: 13m 30s\tremaining: 7m 13s\n",
      "129:\tlearn: 0.7690660\ttotal: 13m 36s\tremaining: 7m 7s\n",
      "130:\tlearn: 0.7688518\ttotal: 13m 42s\tremaining: 7m\n",
      "131:\tlearn: 0.7690660\ttotal: 13m 48s\tremaining: 6m 54s\n",
      "132:\tlearn: 0.7697087\ttotal: 13m 54s\tremaining: 6m 48s\n",
      "133:\tlearn: 0.7733505\ttotal: 14m 1s\tremaining: 6m 41s\n",
      "134:\tlearn: 0.7731362\ttotal: 14m 7s\tremaining: 6m 35s\n",
      "135:\tlearn: 0.7733505\ttotal: 14m 13s\tremaining: 6m 29s\n",
      "136:\tlearn: 0.7767781\ttotal: 14m 19s\tremaining: 6m 22s\n",
      "137:\tlearn: 0.7778492\ttotal: 14m 25s\tremaining: 6m 16s\n",
      "138:\tlearn: 0.7784919\ttotal: 14m 32s\tremaining: 6m 10s\n",
      "139:\tlearn: 0.7829906\ttotal: 14m 38s\tremaining: 6m 3s\n",
      "140:\tlearn: 0.7829906\ttotal: 14m 44s\tremaining: 5m 57s\n",
      "141:\tlearn: 0.7836332\ttotal: 14m 51s\tremaining: 5m 51s\n",
      "142:\tlearn: 0.7838475\ttotal: 14m 57s\tremaining: 5m 45s\n",
      "143:\tlearn: 0.7840617\ttotal: 15m 3s\tremaining: 5m 38s\n",
      "144:\tlearn: 0.7840617\ttotal: 15m 9s\tremaining: 5m 32s\n",
      "145:\tlearn: 0.7836332\ttotal: 15m 16s\tremaining: 5m 26s\n",
      "146:\tlearn: 0.7834190\ttotal: 15m 22s\tremaining: 5m 19s\n",
      "147:\tlearn: 0.7814910\ttotal: 15m 28s\tremaining: 5m 13s\n",
      "148:\tlearn: 0.7812768\ttotal: 15m 34s\tremaining: 5m 7s\n",
      "149:\tlearn: 0.7857755\ttotal: 15m 41s\tremaining: 5m 1s\n",
      "150:\tlearn: 0.7864182\ttotal: 15m 47s\tremaining: 4m 54s\n",
      "151:\tlearn: 0.7866324\ttotal: 15m 53s\tremaining: 4m 48s\n",
      "152:\tlearn: 0.7859897\ttotal: 15m 59s\tremaining: 4m 42s\n",
      "153:\tlearn: 0.7864182\ttotal: 16m 6s\tremaining: 4m 36s\n",
      "154:\tlearn: 0.7870608\ttotal: 16m 12s\tremaining: 4m 29s\n",
      "155:\tlearn: 0.7883462\ttotal: 16m 19s\tremaining: 4m 23s\n",
      "156:\tlearn: 0.7870608\ttotal: 16m 25s\tremaining: 4m 17s\n",
      "157:\tlearn: 0.7870608\ttotal: 16m 32s\tremaining: 4m 11s\n",
      "158:\tlearn: 0.7922022\ttotal: 16m 38s\tremaining: 4m 4s\n",
      "159:\tlearn: 0.7924165\ttotal: 16m 44s\tremaining: 3m 58s\n",
      "160:\tlearn: 0.7919880\ttotal: 16m 50s\tremaining: 3m 52s\n",
      "161:\tlearn: 0.7924165\ttotal: 16m 57s\tremaining: 3m 46s\n",
      "162:\tlearn: 0.7934876\ttotal: 17m 3s\tremaining: 3m 39s\n",
      "163:\tlearn: 0.7932734\ttotal: 17m 9s\tremaining: 3m 33s\n",
      "164:\tlearn: 0.7934876\ttotal: 17m 15s\tremaining: 3m 27s\n",
      "165:\tlearn: 0.7932734\ttotal: 17m 22s\tremaining: 3m 20s\n",
      "166:\tlearn: 0.7939160\ttotal: 17m 29s\tremaining: 3m 14s\n",
      "167:\tlearn: 0.7943445\ttotal: 17m 35s\tremaining: 3m 8s\n",
      "168:\tlearn: 0.7949871\ttotal: 17m 41s\tremaining: 3m 2s\n",
      "169:\tlearn: 0.7982005\ttotal: 17m 48s\tremaining: 2m 55s\n",
      "170:\tlearn: 0.7988432\ttotal: 17m 54s\tremaining: 2m 49s\n",
      "171:\tlearn: 0.7973436\ttotal: 18m\tremaining: 2m 43s\n",
      "172:\tlearn: 0.7975578\ttotal: 18m 6s\tremaining: 2m 37s\n",
      "173:\tlearn: 0.7975578\ttotal: 18m 12s\tremaining: 2m 30s\n",
      "174:\tlearn: 0.7975578\ttotal: 18m 19s\tremaining: 2m 24s\n",
      "175:\tlearn: 0.7977721\ttotal: 18m 25s\tremaining: 2m 18s\n",
      "176:\tlearn: 0.7977721\ttotal: 18m 31s\tremaining: 2m 11s\n",
      "177:\tlearn: 0.7982005\ttotal: 18m 38s\tremaining: 2m 5s\n",
      "178:\tlearn: 0.7977721\ttotal: 18m 44s\tremaining: 1m 59s\n",
      "179:\tlearn: 0.7977721\ttotal: 18m 50s\tremaining: 1m 53s\n",
      "180:\tlearn: 0.7982005\ttotal: 18m 56s\tremaining: 1m 46s\n",
      "181:\tlearn: 0.7971294\ttotal: 19m 2s\tremaining: 1m 40s\n",
      "182:\tlearn: 0.8005570\ttotal: 19m 9s\tremaining: 1m 34s\n",
      "183:\tlearn: 0.8005570\ttotal: 19m 15s\tremaining: 1m 27s\n",
      "184:\tlearn: 0.8011997\ttotal: 19m 21s\tremaining: 1m 21s\n",
      "185:\tlearn: 0.8014139\ttotal: 19m 28s\tremaining: 1m 15s\n",
      "186:\tlearn: 0.8009854\ttotal: 19m 34s\tremaining: 1m 9s\n",
      "187:\tlearn: 0.8018423\ttotal: 19m 41s\tremaining: 1m 2s\n",
      "188:\tlearn: 0.8024850\ttotal: 19m 47s\tremaining: 56.5s\n",
      "189:\tlearn: 0.8022708\ttotal: 19m 53s\tremaining: 50.3s\n",
      "190:\tlearn: 0.8020566\ttotal: 20m\tremaining: 44s\n",
      "191:\tlearn: 0.8018423\ttotal: 20m 6s\tremaining: 37.7s\n",
      "192:\tlearn: 0.8059126\ttotal: 20m 13s\tremaining: 31.4s\n",
      "193:\tlearn: 0.8067695\ttotal: 20m 19s\tremaining: 25.2s\n",
      "194:\tlearn: 0.8074122\ttotal: 20m 26s\tremaining: 18.9s\n",
      "195:\tlearn: 0.8065553\ttotal: 20m 32s\tremaining: 12.6s\n",
      "196:\tlearn: 0.8063410\ttotal: 20m 38s\tremaining: 6.29s\n",
      "197:\tlearn: 0.8065553\ttotal: 20m 45s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "params['iterations'] = np.argmax(cv_results['test-Accuracy-mean'].values)\n",
    "cat_model = cat.train(cat_trainval, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.7927638767482517 vs 0.8844287223193472\n",
      "Precision: 0.7813182798599346 vs 0.8939847561802985\n",
      "Accuracy: 0.7930591259640103 vs 0.8849614395886889\n",
      "Log loss: 4.192678924650141 vs 0.4062003888590844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "_y_pred = cat_model.predict(cat_test, prediction_type=\"Class\").ravel().astype(np.int)\n",
    "_y_probas = cat_model.predict(cat_test, prediction_type=\"Probability\")\n",
    "_y_pred_base = get_baseline_preds(_X_test)\n",
    "_y_probas_base = get_baseline_probas(_X_test)\n",
    "\n",
    "print(\"Recall: {} vs {}\".format(recall_score(_y_test, _y_pred, average=\"macro\"), recall_score(_y_test, _y_pred_base, average=\"macro\")))\n",
    "print(\"Precision: {} vs {}\".format(precision_score(_y_test, _y_pred, average=\"macro\"), precision_score(_y_test, _y_pred_base, average=\"macro\")))\n",
    "print(\"Accuracy: {} vs {}\".format(accuracy_score(_y_test, _y_pred), accuracy_score(_y_test, _y_pred_base)))\n",
    "print(\"Log loss: {} vs {}\".format(log_loss(_y_test, _y_probas), log_loss(_y_test, _y_probas_base)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: -4.8517114\ttest: -4.8517429\tbest: -4.8517429 (0)\ttotal: 15.6s\tremaining: 4h 19m 4s\n",
      "1:\tlearn: -4.8513312\ttest: -4.8514013\tbest: -4.8514013 (1)\ttotal: 31.4s\tremaining: 4h 21m 17s\n",
      "2:\tlearn: -4.8510835\ttest: -4.8511612\tbest: -4.8511612 (2)\ttotal: 46.7s\tremaining: 4h 18m 24s\n",
      "3:\tlearn: -4.8508146\ttest: -4.8509083\tbest: -4.8509083 (3)\ttotal: 1m 1s\tremaining: 4h 16m 12s\n",
      "4:\tlearn: -4.8505639\ttest: -4.8506621\tbest: -4.8506621 (4)\ttotal: 1m 17s\tremaining: 4h 16m 11s\n",
      "5:\tlearn: -4.8502651\ttest: -4.8503694\tbest: -4.8503694 (5)\ttotal: 1m 32s\tremaining: 4h 15m 11s\n",
      "6:\tlearn: -4.8499654\ttest: -4.8500925\tbest: -4.8500925 (6)\ttotal: 1m 47s\tremaining: 4h 14m 23s\n",
      "7:\tlearn: -4.8496401\ttest: -4.8497917\tbest: -4.8497917 (7)\ttotal: 2m 3s\tremaining: 4h 15m 40s\n",
      "8:\tlearn: -4.8493904\ttest: -4.8495448\tbest: -4.8495448 (8)\ttotal: 2m 18s\tremaining: 4h 14m 51s\n",
      "9:\tlearn: -4.8491360\ttest: -4.8493030\tbest: -4.8493030 (9)\ttotal: 2m 33s\tremaining: 4h 13m 51s\n",
      "10:\tlearn: -4.8487788\ttest: -4.8489616\tbest: -4.8489616 (10)\ttotal: 2m 49s\tremaining: 4h 13m 44s\n",
      "11:\tlearn: -4.8484467\ttest: -4.8486602\tbest: -4.8486602 (11)\ttotal: 3m 5s\tremaining: 4h 14m 6s\n",
      "12:\tlearn: -4.8481041\ttest: -4.8483423\tbest: -4.8483423 (12)\ttotal: 3m 21s\tremaining: 4h 14m 36s\n",
      "13:\tlearn: -4.8478518\ttest: -4.8480905\tbest: -4.8480905 (13)\ttotal: 3m 36s\tremaining: 4h 14m 11s\n",
      "14:\tlearn: -4.8475682\ttest: -4.8478227\tbest: -4.8478227 (14)\ttotal: 3m 52s\tremaining: 4h 14m 9s\n",
      "15:\tlearn: -4.8472072\ttest: -4.8474768\tbest: -4.8474768 (15)\ttotal: 4m 8s\tremaining: 4h 14m 17s\n",
      "16:\tlearn: -4.8469077\ttest: -4.8471958\tbest: -4.8471958 (16)\ttotal: 4m 23s\tremaining: 4h 14m 3s\n",
      "17:\tlearn: -4.8466711\ttest: -4.8469728\tbest: -4.8469728 (17)\ttotal: 4m 38s\tremaining: 4h 13m 32s\n",
      "18:\tlearn: -4.8464138\ttest: -4.8467230\tbest: -4.8467230 (18)\ttotal: 4m 53s\tremaining: 4h 12m 51s\n",
      "19:\tlearn: -4.8461300\ttest: -4.8464438\tbest: -4.8464438 (19)\ttotal: 5m 9s\tremaining: 4h 12m 24s\n",
      "20:\tlearn: -4.8458644\ttest: -4.8461914\tbest: -4.8461914 (20)\ttotal: 5m 24s\tremaining: 4h 11m 53s\n",
      "21:\tlearn: -4.8456394\ttest: -4.8459691\tbest: -4.8459691 (21)\ttotal: 5m 39s\tremaining: 4h 11m 22s\n",
      "22:\tlearn: -4.8453416\ttest: -4.8456835\tbest: -4.8456835 (22)\ttotal: 5m 54s\tremaining: 4h 11m 6s\n",
      "23:\tlearn: -4.8450352\ttest: -4.8453936\tbest: -4.8453936 (23)\ttotal: 6m 9s\tremaining: 4h 10m 37s\n",
      "24:\tlearn: -4.8447518\ttest: -4.8451130\tbest: -4.8451130 (24)\ttotal: 6m 25s\tremaining: 4h 10m 25s\n",
      "25:\tlearn: -4.8444873\ttest: -4.8448561\tbest: -4.8448561 (25)\ttotal: 6m 40s\tremaining: 4h 10m 1s\n",
      "26:\tlearn: -4.8441959\ttest: -4.8445681\tbest: -4.8445681 (26)\ttotal: 6m 55s\tremaining: 4h 9m 49s\n",
      "27:\tlearn: -4.8439411\ttest: -4.8443288\tbest: -4.8443288 (27)\ttotal: 7m 10s\tremaining: 4h 9m 20s\n",
      "28:\tlearn: -4.8435038\ttest: -4.8439380\tbest: -4.8439380 (28)\ttotal: 7m 27s\tremaining: 4h 9m 39s\n",
      "29:\tlearn: -4.8432301\ttest: -4.8436856\tbest: -4.8436856 (29)\ttotal: 7m 42s\tremaining: 4h 9m 17s\n",
      "30:\tlearn: -4.8429399\ttest: -4.8434143\tbest: -4.8434143 (30)\ttotal: 7m 57s\tremaining: 4h 8m 58s\n",
      "31:\tlearn: -4.8427087\ttest: -4.8431816\tbest: -4.8431816 (31)\ttotal: 8m 13s\tremaining: 4h 8m 42s\n",
      "32:\tlearn: -4.8424384\ttest: -4.8429298\tbest: -4.8429298 (32)\ttotal: 8m 28s\tremaining: 4h 8m 19s\n",
      "33:\tlearn: -4.8421475\ttest: -4.8426609\tbest: -4.8426609 (33)\ttotal: 8m 43s\tremaining: 4h 8m 4s\n",
      "34:\tlearn: -4.8418799\ttest: -4.8424023\tbest: -4.8424023 (34)\ttotal: 8m 59s\tremaining: 4h 8m\n",
      "35:\tlearn: -4.8415308\ttest: -4.8420754\tbest: -4.8420754 (35)\ttotal: 9m 15s\tremaining: 4h 7m 43s\n",
      "36:\tlearn: -4.8412390\ttest: -4.8417956\tbest: -4.8417956 (36)\ttotal: 9m 30s\tremaining: 4h 7m 24s\n",
      "37:\tlearn: -4.8410002\ttest: -4.8415542\tbest: -4.8415542 (37)\ttotal: 9m 45s\tremaining: 4h 7m 6s\n",
      "38:\tlearn: -4.8407769\ttest: -4.8413458\tbest: -4.8413458 (38)\ttotal: 10m\tremaining: 4h 6m 44s\n",
      "39:\tlearn: -4.8404652\ttest: -4.8410406\tbest: -4.8410406 (39)\ttotal: 10m 16s\tremaining: 4h 6m 24s\n",
      "40:\tlearn: -4.8402207\ttest: -4.8408064\tbest: -4.8408064 (40)\ttotal: 10m 31s\tremaining: 4h 6m 5s\n",
      "41:\tlearn: -4.8399818\ttest: -4.8405671\tbest: -4.8405671 (41)\ttotal: 10m 46s\tremaining: 4h 5m 48s\n",
      "42:\tlearn: -4.8397129\ttest: -4.8402976\tbest: -4.8402976 (42)\ttotal: 11m 1s\tremaining: 4h 5m 31s\n",
      "43:\tlearn: -4.8393786\ttest: -4.8399762\tbest: -4.8399762 (43)\ttotal: 11m 17s\tremaining: 4h 5m 16s\n",
      "44:\tlearn: -4.8390869\ttest: -4.8397133\tbest: -4.8397133 (44)\ttotal: 11m 32s\tremaining: 4h 5m\n",
      "45:\tlearn: -4.8387448\ttest: -4.8393980\tbest: -4.8393980 (45)\ttotal: 11m 48s\tremaining: 4h 4m 59s\n",
      "46:\tlearn: -4.8384916\ttest: -4.8391558\tbest: -4.8391558 (46)\ttotal: 12m 3s\tremaining: 4h 4m 37s\n",
      "47:\tlearn: -4.8382310\ttest: -4.8389041\tbest: -4.8389041 (47)\ttotal: 12m 19s\tremaining: 4h 4m 25s\n",
      "48:\tlearn: -4.8379940\ttest: -4.8386751\tbest: -4.8386751 (48)\ttotal: 12m 34s\tremaining: 4h 4m 3s\n",
      "49:\tlearn: -4.8377127\ttest: -4.8384052\tbest: -4.8384052 (49)\ttotal: 12m 49s\tremaining: 4h 3m 46s\n",
      "50:\tlearn: -4.8374652\ttest: -4.8381595\tbest: -4.8381595 (50)\ttotal: 13m 4s\tremaining: 4h 3m 16s\n",
      "51:\tlearn: -4.8371724\ttest: -4.8378968\tbest: -4.8378968 (51)\ttotal: 13m 19s\tremaining: 4h 3m 2s\n",
      "52:\tlearn: -4.8368348\ttest: -4.8375804\tbest: -4.8375804 (52)\ttotal: 13m 35s\tremaining: 4h 2m 45s\n",
      "53:\tlearn: -4.8365444\ttest: -4.8372968\tbest: -4.8372968 (53)\ttotal: 13m 50s\tremaining: 4h 2m 24s\n",
      "54:\tlearn: -4.8362489\ttest: -4.8370186\tbest: -4.8370186 (54)\ttotal: 14m 6s\tremaining: 4h 2m 19s\n",
      "55:\tlearn: -4.8359250\ttest: -4.8367123\tbest: -4.8367123 (55)\ttotal: 14m 21s\tremaining: 4h 2m 4s\n",
      "56:\tlearn: -4.8356389\ttest: -4.8364323\tbest: -4.8364323 (56)\ttotal: 14m 36s\tremaining: 4h 1m 45s\n",
      "57:\tlearn: -4.8353673\ttest: -4.8361716\tbest: -4.8361716 (57)\ttotal: 14m 51s\tremaining: 4h 1m 24s\n",
      "58:\tlearn: -4.8351043\ttest: -4.8359249\tbest: -4.8359249 (58)\ttotal: 15m 7s\tremaining: 4h 1m 9s\n",
      "59:\tlearn: -4.8347983\ttest: -4.8356317\tbest: -4.8356317 (59)\ttotal: 15m 22s\tremaining: 4h 59s\n",
      "60:\tlearn: -4.8345056\ttest: -4.8353546\tbest: -4.8353546 (60)\ttotal: 15m 38s\tremaining: 4h 44s\n",
      "61:\tlearn: -4.8341924\ttest: -4.8350637\tbest: -4.8350637 (61)\ttotal: 15m 54s\tremaining: 4h 35s\n",
      "62:\tlearn: -4.8339122\ttest: -4.8347989\tbest: -4.8347989 (62)\ttotal: 16m 9s\tremaining: 4h 20s\n",
      "63:\tlearn: -4.8335960\ttest: -4.8344970\tbest: -4.8344970 (63)\ttotal: 16m 25s\tremaining: 4h 8s\n",
      "64:\tlearn: -4.8331900\ttest: -4.8341286\tbest: -4.8341286 (64)\ttotal: 16m 41s\tremaining: 4h 3s\n",
      "65:\tlearn: -4.8328964\ttest: -4.8338579\tbest: -4.8338579 (65)\ttotal: 16m 56s\tremaining: 3h 59m 42s\n",
      "66:\tlearn: -4.8325946\ttest: -4.8335746\tbest: -4.8335746 (66)\ttotal: 17m 11s\tremaining: 3h 59m 28s\n",
      "67:\tlearn: -4.8323026\ttest: -4.8332941\tbest: -4.8332941 (67)\ttotal: 17m 27s\tremaining: 3h 59m 11s\n",
      "68:\tlearn: -4.8320789\ttest: -4.8330922\tbest: -4.8330922 (68)\ttotal: 17m 41s\tremaining: 3h 58m 49s\n",
      "69:\tlearn: -4.8318487\ttest: -4.8328657\tbest: -4.8328657 (69)\ttotal: 17m 57s\tremaining: 3h 58m 29s\n",
      "70:\tlearn: -4.8314993\ttest: -4.8325337\tbest: -4.8325337 (70)\ttotal: 18m 12s\tremaining: 3h 58m 21s\n",
      "71:\tlearn: -4.8311792\ttest: -4.8322318\tbest: -4.8322318 (71)\ttotal: 18m 28s\tremaining: 3h 58m 3s\n",
      "72:\tlearn: -4.8309368\ttest: -4.8319939\tbest: -4.8319939 (72)\ttotal: 18m 43s\tremaining: 3h 57m 40s\n",
      "73:\tlearn: -4.8306821\ttest: -4.8317521\tbest: -4.8317521 (73)\ttotal: 18m 58s\tremaining: 3h 57m 21s\n",
      "74:\tlearn: -4.8304519\ttest: -4.8315313\tbest: -4.8315313 (74)\ttotal: 19m 12s\tremaining: 3h 56m 59s\n",
      "75:\tlearn: -4.8300687\ttest: -4.8311748\tbest: -4.8311748 (75)\ttotal: 19m 28s\tremaining: 3h 56m 45s\n",
      "76:\tlearn: -4.8297536\ttest: -4.8308873\tbest: -4.8308873 (76)\ttotal: 19m 43s\tremaining: 3h 56m 30s\n",
      "77:\tlearn: -4.8294459\ttest: -4.8305895\tbest: -4.8305895 (77)\ttotal: 19m 59s\tremaining: 3h 56m 15s\n",
      "78:\tlearn: -4.8291519\ttest: -4.8303143\tbest: -4.8303143 (78)\ttotal: 20m 14s\tremaining: 3h 55m 58s\n",
      "79:\tlearn: -4.8288995\ttest: -4.8300610\tbest: -4.8300610 (79)\ttotal: 20m 29s\tremaining: 3h 55m 36s\n",
      "80:\tlearn: -4.8286225\ttest: -4.8297997\tbest: -4.8297997 (80)\ttotal: 20m 44s\tremaining: 3h 55m 19s\n",
      "81:\tlearn: -4.8283848\ttest: -4.8295745\tbest: -4.8295745 (81)\ttotal: 20m 59s\tremaining: 3h 55m 3s\n",
      "82:\tlearn: -4.8279858\ttest: -4.8292042\tbest: -4.8292042 (82)\ttotal: 21m 15s\tremaining: 3h 54m 56s\n",
      "83:\tlearn: -4.8276426\ttest: -4.8288855\tbest: -4.8288855 (83)\ttotal: 21m 31s\tremaining: 3h 54m 42s\n",
      "84:\tlearn: -4.8273597\ttest: -4.8286207\tbest: -4.8286207 (84)\ttotal: 21m 47s\tremaining: 3h 54m 33s\n",
      "85:\tlearn: -4.8271210\ttest: -4.8283887\tbest: -4.8283887 (85)\ttotal: 22m 2s\tremaining: 3h 54m 14s\n",
      "86:\tlearn: -4.8268518\ttest: -4.8281336\tbest: -4.8281336 (86)\ttotal: 22m 17s\tremaining: 3h 53m 58s\n",
      "87:\tlearn: -4.8264844\ttest: -4.8277994\tbest: -4.8277994 (87)\ttotal: 22m 34s\tremaining: 3h 53m 52s\n",
      "88:\tlearn: -4.8261535\ttest: -4.8274755\tbest: -4.8274755 (88)\ttotal: 22m 49s\tremaining: 3h 53m 40s\n",
      "89:\tlearn: -4.8258538\ttest: -4.8271937\tbest: -4.8271937 (89)\ttotal: 23m 5s\tremaining: 3h 53m 25s\n",
      "90:\tlearn: -4.8255670\ttest: -4.8269197\tbest: -4.8269197 (90)\ttotal: 23m 20s\tremaining: 3h 53m 11s\n",
      "91:\tlearn: -4.8252569\ttest: -4.8266319\tbest: -4.8266319 (91)\ttotal: 23m 36s\tremaining: 3h 52m 59s\n",
      "92:\tlearn: -4.8250400\ttest: -4.8264250\tbest: -4.8264250 (92)\ttotal: 23m 51s\tremaining: 3h 52m 42s\n",
      "93:\tlearn: -4.8247895\ttest: -4.8261868\tbest: -4.8261868 (93)\ttotal: 24m 7s\tremaining: 3h 52m 30s\n",
      "94:\tlearn: -4.8245481\ttest: -4.8259461\tbest: -4.8259461 (94)\ttotal: 24m 22s\tremaining: 3h 52m 13s\n",
      "95:\tlearn: -4.8242766\ttest: -4.8256885\tbest: -4.8256885 (95)\ttotal: 24m 38s\tremaining: 3h 51m 59s\n",
      "96:\tlearn: -4.8240047\ttest: -4.8254164\tbest: -4.8254164 (96)\ttotal: 24m 53s\tremaining: 3h 51m 41s\n",
      "97:\tlearn: -4.8237723\ttest: -4.8251912\tbest: -4.8251912 (97)\ttotal: 25m 8s\tremaining: 3h 51m 23s\n",
      "98:\tlearn: -4.8234393\ttest: -4.8248751\tbest: -4.8248751 (98)\ttotal: 25m 24s\tremaining: 3h 51m 11s\n",
      "99:\tlearn: -4.8231442\ttest: -4.8245986\tbest: -4.8245986 (99)\ttotal: 25m 39s\tremaining: 3h 50m 54s\n",
      "100:\tlearn: -4.8228670\ttest: -4.8243391\tbest: -4.8243391 (100)\ttotal: 25m 54s\tremaining: 3h 50m 36s\n",
      "101:\tlearn: -4.8225401\ttest: -4.8240222\tbest: -4.8240222 (101)\ttotal: 26m 9s\tremaining: 3h 50m 21s\n",
      "102:\tlearn: -4.8222231\ttest: -4.8237211\tbest: -4.8237211 (102)\ttotal: 26m 25s\tremaining: 3h 50m 5s\n",
      "103:\tlearn: -4.8219455\ttest: -4.8234474\tbest: -4.8234474 (103)\ttotal: 26m 40s\tremaining: 3h 49m 48s\n",
      "104:\tlearn: -4.8216539\ttest: -4.8231613\tbest: -4.8231613 (104)\ttotal: 26m 55s\tremaining: 3h 49m 31s\n",
      "105:\tlearn: -4.8213196\ttest: -4.8228555\tbest: -4.8228555 (105)\ttotal: 27m 11s\tremaining: 3h 49m 19s\n",
      "106:\tlearn: -4.8210055\ttest: -4.8225610\tbest: -4.8225610 (106)\ttotal: 27m 26s\tremaining: 3h 49m 5s\n",
      "107:\tlearn: -4.8206376\ttest: -4.8222257\tbest: -4.8222257 (107)\ttotal: 27m 43s\tremaining: 3h 48m 57s\n",
      "108:\tlearn: -4.8203919\ttest: -4.8219967\tbest: -4.8219967 (108)\ttotal: 27m 58s\tremaining: 3h 48m 41s\n",
      "109:\tlearn: -4.8201419\ttest: -4.8217478\tbest: -4.8217478 (109)\ttotal: 28m 13s\tremaining: 3h 48m 23s\n",
      "110:\tlearn: -4.8198054\ttest: -4.8214299\tbest: -4.8214299 (110)\ttotal: 28m 29s\tremaining: 3h 48m 13s\n",
      "111:\tlearn: -4.8194213\ttest: -4.8210656\tbest: -4.8210656 (111)\ttotal: 28m 45s\tremaining: 3h 47m 59s\n",
      "112:\tlearn: -4.8191315\ttest: -4.8207800\tbest: -4.8207800 (112)\ttotal: 29m\tremaining: 3h 47m 38s\n",
      "113:\tlearn: -4.8187930\ttest: -4.8204640\tbest: -4.8204640 (113)\ttotal: 29m 16s\tremaining: 3h 47m 28s\n",
      "114:\tlearn: -4.8185110\ttest: -4.8202022\tbest: -4.8202022 (114)\ttotal: 29m 31s\tremaining: 3h 47m 11s\n",
      "115:\tlearn: -4.8181699\ttest: -4.8198660\tbest: -4.8198660 (115)\ttotal: 29m 46s\tremaining: 3h 46m 51s\n",
      "116:\tlearn: -4.8178887\ttest: -4.8196102\tbest: -4.8196102 (116)\ttotal: 30m 1s\tremaining: 3h 46m 37s\n",
      "117:\tlearn: -4.8174972\ttest: -4.8192663\tbest: -4.8192663 (117)\ttotal: 30m 17s\tremaining: 3h 46m 24s\n",
      "118:\tlearn: -4.8171913\ttest: -4.8189711\tbest: -4.8189711 (118)\ttotal: 30m 32s\tremaining: 3h 46m 8s\n",
      "119:\tlearn: -4.8168811\ttest: -4.8186778\tbest: -4.8186778 (119)\ttotal: 30m 48s\tremaining: 3h 45m 54s\n",
      "120:\tlearn: -4.8166027\ttest: -4.8184020\tbest: -4.8184020 (120)\ttotal: 31m 3s\tremaining: 3h 45m 37s\n",
      "121:\tlearn: -4.8163517\ttest: -4.8181606\tbest: -4.8181606 (121)\ttotal: 31m 18s\tremaining: 3h 45m 20s\n",
      "122:\tlearn: -4.8161015\ttest: -4.8179185\tbest: -4.8179185 (122)\ttotal: 31m 33s\tremaining: 3h 45m 2s\n",
      "123:\tlearn: -4.8158058\ttest: -4.8176298\tbest: -4.8176298 (123)\ttotal: 31m 48s\tremaining: 3h 44m 45s\n",
      "124:\tlearn: -4.8154995\ttest: -4.8173370\tbest: -4.8173370 (124)\ttotal: 32m 4s\tremaining: 3h 44m 34s\n",
      "125:\tlearn: -4.8152668\ttest: -4.8171142\tbest: -4.8171142 (125)\ttotal: 32m 19s\tremaining: 3h 44m 15s\n",
      "126:\tlearn: -4.8150552\ttest: -4.8168997\tbest: -4.8168997 (126)\ttotal: 32m 35s\tremaining: 3h 43m 58s\n",
      "127:\tlearn: -4.8147374\ttest: -4.8165968\tbest: -4.8165968 (127)\ttotal: 32m 50s\tremaining: 3h 43m 41s\n",
      "128:\tlearn: -4.8145043\ttest: -4.8163598\tbest: -4.8163598 (128)\ttotal: 33m 5s\tremaining: 3h 43m 24s\n",
      "129:\tlearn: -4.8142816\ttest: -4.8161451\tbest: -4.8161451 (129)\ttotal: 33m 20s\tremaining: 3h 43m 6s\n",
      "130:\tlearn: -4.8139002\ttest: -4.8157951\tbest: -4.8157951 (130)\ttotal: 33m 36s\tremaining: 3h 42m 57s\n",
      "131:\tlearn: -4.8135634\ttest: -4.8154824\tbest: -4.8154824 (131)\ttotal: 33m 52s\tremaining: 3h 42m 45s\n",
      "132:\tlearn: -4.8133249\ttest: -4.8152587\tbest: -4.8152587 (132)\ttotal: 34m 7s\tremaining: 3h 42m 29s\n",
      "133:\tlearn: -4.8130045\ttest: -4.8149531\tbest: -4.8149531 (133)\ttotal: 34m 23s\tremaining: 3h 42m 18s\n",
      "134:\tlearn: -4.8127392\ttest: -4.8146985\tbest: -4.8146985 (134)\ttotal: 34m 39s\tremaining: 3h 42m 1s\n",
      "135:\tlearn: -4.8125077\ttest: -4.8144787\tbest: -4.8144787 (135)\ttotal: 34m 54s\tremaining: 3h 41m 43s\n",
      "136:\tlearn: -4.8121580\ttest: -4.8141376\tbest: -4.8141376 (136)\ttotal: 35m 10s\tremaining: 3h 41m 32s\n",
      "137:\tlearn: -4.8119139\ttest: -4.8138897\tbest: -4.8138897 (137)\ttotal: 35m 24s\tremaining: 3h 41m 12s\n",
      "138:\tlearn: -4.8116107\ttest: -4.8136037\tbest: -4.8136037 (138)\ttotal: 35m 40s\tremaining: 3h 40m 56s\n",
      "139:\tlearn: -4.8113728\ttest: -4.8133794\tbest: -4.8133794 (139)\ttotal: 35m 55s\tremaining: 3h 40m 39s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-231dd2b19f6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcv_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_trainval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratified\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mcv\u001b[0;34m(pool, params, dtrain, iterations, num_boost_round, fold_count, nfold, inverted, partition_random_seed, seed, shuffle, logging_level, stratified, as_pandas, verbose, verbose_eval, plot)\u001b[0m\n\u001b[1;32m   2309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2310\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_random_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._cv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._cv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"iterations\": 1000,\n",
    "    \"loss_function\": \"MultiClass\",\n",
    "    \"eval_metric\": \"MultiClass\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"l2_leaf_reg\": 4,\n",
    "    \"depth\": 5,\n",
    "    \"od_type\": \"Iter\",\n",
    "    \"od_wait\": 10,    \n",
    "}\n",
    "\n",
    "cv_results = cat.cv(cat_trainval, params, nfold=5, stratified=True, seed=seed, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-features learning, 128 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xy = pd.concat([X, y], axis=1)\n",
    "# meta_features.loc[53, 'f0_c41']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class K (0, ..., 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- class :  14\n",
      "{'C': 6.951927961775605, 'penalty': 'l2'} -0.024665787591909554\n",
      "Recall: 0.38461538461538464 vs 0.3076923076923077\n",
      "Precision: 0.7142857142857143 vs 0.6666666666666666\n",
      "Accuracy: 0.9936427209154481 vs 0.993006993006993\n",
      "-- class :  62\n",
      "{'C': 4.281332398719393, 'penalty': 'l2'} -0.026060559781892072\n",
      "Recall: 0.23076923076923078 vs 0.38461538461538464\n",
      "Precision: 1.0 vs 0.625\n",
      "Accuracy: 0.9936427209154481 vs 0.993006993006993\n"
     ]
    }
   ],
   "source": [
    "n_classes = 128\n",
    "seed = 555\n",
    "n = len(meta_features_list)\n",
    "\n",
    "\n",
    "for class_index in [14, 62]:\n",
    "\n",
    "    print(\"-- class : \", class_index)    \n",
    "    cols = ['f{}_c{}'.format(i, class_index) for i in range(n)] \n",
    "    for c, _ in misclassifed[class_index]['wrong_classes']:        \n",
    "        cols += ['f{}_c{}'.format(i, c) for i in range(n)] \n",
    "    cols += ['size', 'width', 'height']\n",
    "    _X = X[cols].values\n",
    "#     _X = X.values\n",
    "    _y = (y == class_index).values.astype(np.int)\n",
    "\n",
    "    # clip probabilities:\n",
    "    # _X = np.clip(_X, 0.00001, 0.99999)\n",
    "    \n",
    "    splt = StratifiedShuffleSplit(n_splits=7, test_size=0.25, random_state=seed)\n",
    "    train_index, test_index = next(splt.split(_X, _y))\n",
    "\n",
    "    _X_train = _X[train_index, :]\n",
    "    _X_test = _X[test_index, :]\n",
    "    _y_train = _y[train_index]\n",
    "    _y_test = _y[test_index]\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    log_reg = LogisticRegression(random_state=seed)\n",
    "    params = {\n",
    "        \"C\": np.logspace(0, 4, 20),\n",
    "        \"penalty\": [\"l2\", ],\n",
    "    }\n",
    "    gs = GridSearchCV(log_reg, params, scoring=\"neg_log_loss\", cv=cv, n_jobs=10)    \n",
    "    gs.fit(_X_train, _y_train)\n",
    "    print(gs.best_params_, gs.best_score_)\n",
    "    # _y_probas = gs.best_estimator_.predict_proba(_X_test)\n",
    "    _y_pred = gs.best_estimator_.predict(_X_test)\n",
    "    \n",
    "    _y_pred_base = (_X_test[:, :n].mean(axis=1) > 0.5).astype(np.int)\n",
    "    \n",
    "    print(\"Recall: {} vs {}\".format(recall_score(_y_test, _y_pred), recall_score(_y_test, _y_pred_base)))\n",
    "    print(\"Precision: {} vs {}\".format(precision_score(_y_test, _y_pred), precision_score(_y_test, _y_pred_base)))\n",
    "    print(\"Accuracy: {} vs {}\".format(accuracy_score(_y_test, _y_pred), accuracy_score(_y_test, _y_pred_base)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_index = 62\n",
    "_y = (y == class_index).values.astype(np.int)\n",
    "cols = ['f{}_c{}'.format(i, class_index) for i in range(n)]\n",
    "_y_pred_base = (_X[:, np.where(X.columns.isin(cols))[0]].mean(axis=1) > 0.5).astype(np.int)\n",
    "recall_score(_y, _y_pred_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beatiful_coef(coefs, feature_names):\n",
    "    return pd.DataFrame(coefs.transpose(), index=feature_names, columns=['coef']).sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beatiful_coef(gs.best_estimator_.coef_, feature_names=X[cols].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- class :  14\n",
      "{'learning_rate': 0.01, 'max_depth': 4} -0.21471614459463984\n",
      "Recall: 0.5384615384615384 vs 0.3076923076923077\n",
      "Precision: 0.5384615384615384 vs 0.6666666666666666\n",
      "Accuracy: 0.9923712650985378 vs 0.993006993006993\n"
     ]
    }
   ],
   "source": [
    "n_classes = 128\n",
    "seed = 555\n",
    "n = len(meta_features_list)\n",
    "\n",
    "\n",
    "for class_index in [14,]:\n",
    "\n",
    "    print(\"-- class : \", class_index)\n",
    "    \n",
    "    cols = ['f{}_c{}'.format(i, class_index) for i in range(n)] \n",
    "    for c, _ in misclassifed[class_index]['wrong_classes']:        \n",
    "        cols += ['f{}_c{}'.format(i, c) for i in range(n)] \n",
    "    cols += ['size', 'width', 'height']\n",
    "    _X = X[cols].values\n",
    "    _y = (y == class_index).values.astype(np.int)\n",
    "\n",
    "    # clip probabilities:\n",
    "    # _X = np.clip(_X, 0.00001, 0.99999)\n",
    "    \n",
    "    splt = StratifiedShuffleSplit(n_splits=7, test_size=0.25, random_state=seed)\n",
    "    train_index, test_index = next(splt.split(_X, _y))\n",
    "\n",
    "    _X_train = _X[train_index, :]\n",
    "    _X_test = _X[test_index, :]\n",
    "    _y_train = _y[train_index]\n",
    "    _y_test = _y[test_index]\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    clf = XGBClassifier(random_state=seed)\n",
    "    params = {\n",
    "        \"max_depth\": [3, 4, 5],\n",
    "        \"learning_rate\": [0.01, 0.003, 0.001]\n",
    "    }\n",
    "    gs = GridSearchCV(clf, params, scoring=\"neg_log_loss\", cv=cv, n_jobs=10)    \n",
    "    gs.fit(_X_train, _y_train)\n",
    "    print(gs.best_params_, gs.best_score_) \n",
    "    _y_probas = gs.best_estimator_.predict_proba(_X_test)\n",
    "#     _y_pred = gs.best_estimator_.predict(_X_test)\n",
    "    _y_pred = (_y_probas[:, 1] > 0.5).astype(np.int)\n",
    "    \n",
    "    _y_pred_base = (_X_test[:, :n].mean(axis=1) > 0.5).astype(np.int)\n",
    "\n",
    "    print(\"Recall: {} vs {}\".format(recall_score(_y_test, _y_pred), recall_score(_y_test, _y_pred_base)))\n",
    "    print(\"Precision: {} vs {}\".format(precision_score(_y_test, _y_pred), precision_score(_y_test, _y_pred_base)))\n",
    "    print(\"Accuracy: {} vs {}\".format(accuracy_score(_y_test, _y_pred), accuracy_score(_y_test, _y_pred_base)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f0_c14: 0.1505376398563385',\n",
       " 'f1_c14: 0.0',\n",
       " 'f2_c14: 0.24946236610412598',\n",
       " 'f0_c3: 0.009677419438958168',\n",
       " 'f1_c3: 0.0010752688394859433',\n",
       " 'f2_c3: 0.08602150529623032',\n",
       " 'f0_c28: 0.10215053707361221',\n",
       " 'f1_c28: 0.03548387065529823',\n",
       " 'f2_c28: 0.07741935551166534',\n",
       " 'f0_c62: 0.0010752688394859433',\n",
       " 'f1_c62: 0.017204301431775093',\n",
       " 'f2_c62: 0.0',\n",
       " 'f0_c125: 0.07956989109516144',\n",
       " 'f1_c125: 0.13548387587070465',\n",
       " 'f2_c125: 0.04838709533214569',\n",
       " 'size: 0.0',\n",
       " 'width: 0.004301075357943773',\n",
       " 'height: 0.0021505376789718866']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"{}: {}\".format(c, v) for c, v in zip(X[cols].columns, best_model.feature_importances_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15053764, 0.        , 0.24946237, 0.00967742, 0.00107527,\n",
       "       0.08602151, 0.10215054, 0.03548387, 0.07741936, 0.00107527,\n",
       "       0.0172043 , 0.        , 0.07956989, 0.13548388, 0.0483871 ,\n",
       "       0.        , 0.00430108, 0.00215054], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import catboost as cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train = cat.Pool(_X_train, label=_y_train)\n",
    "cat_test = cat.Pool(_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"iterations\": 10,\n",
    "    \"loss_function\": \"MultiClass\",\n",
    "    \"eval_metric\": \"Accuracy\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"l2_leaf_reg\": 3,\n",
    "    \"depth\": 4,\n",
    "    \"od_type\": \"Iter\",\n",
    "    \"od_wait\": 50,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.0571204\ttest: 0.0379284\tbest: 0.0379284 (0)\ttotal: 12.2s\tremaining: 1m 49s\n",
      "1:\tlearn: 0.1171925\ttest: 0.0910784\tbest: 0.0910784 (1)\ttotal: 24.8s\tremaining: 1m 39s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-c6c1ef4c4d12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mcv\u001b[0;34m(pool, params, dtrain, iterations, num_boost_round, fold_count, nfold, inverted, partition_random_seed, seed, shuffle, logging_level, stratified, as_pandas, verbose, verbose_eval, plot)\u001b[0m\n\u001b[1;32m   2309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2310\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_random_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._cv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._cv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv_results = cat.cv(cat_train, params=params, nfold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:52: FutureWarning: 'argmax' is deprecated. Use 'idxmax' instead. The behavior of 'argmax' will be corrected to return the positional maximum in the future. Use 'series.values.argmax' to get the position of the maximum now.\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.0758574\ttotal: 11.8s\tremaining: 1m 34s\n",
      "1:\tlearn: 0.1301385\ttotal: 25.5s\tremaining: 1m 29s\n",
      "2:\tlearn: 0.1739723\ttotal: 40.1s\tremaining: 1m 20s\n",
      "3:\tlearn: 0.2784465\ttotal: 53.6s\tremaining: 1m 7s\n",
      "4:\tlearn: 0.3445378\ttotal: 1m 6s\tremaining: 53.5s\n",
      "5:\tlearn: 0.3967749\ttotal: 1m 20s\tremaining: 40.1s\n",
      "6:\tlearn: 0.4047241\ttotal: 1m 33s\tremaining: 26.7s\n",
      "7:\tlearn: 0.4397002\ttotal: 1m 46s\tremaining: 13.3s\n",
      "8:\tlearn: 0.4635476\ttotal: 1m 59s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "best_iterations = np.argmax(cv_results['test-%s-mean' % params['eval_metric']])\n",
    "params['iterations'] = best_iterations + 1\n",
    "\n",
    "cat_model = cat.train(params=params, pool=cat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "CatboostError",
     "evalue": "Data cat_features in predict()=[] are not equal data cat_features in fit()=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCatboostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-c1ffa86ea282>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_y_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_y_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose)\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \"\"\"\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_staged_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose)\u001b[0m\n\u001b[1;32m    882\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_cat_feature_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_cat_feature_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cat_feature_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCatboostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data cat_features in predict()={} are not equal data cat_features in fit()={}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cat_feature_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_cat_feature_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_empty_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCatboostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data is empty.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCatboostError\u001b[0m: Data cat_features in predict()=[] are not equal data cat_features in fit()=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]."
     ]
    }
   ],
   "source": [
    "_y_pred = cat_model.predict(cat_test, prediction_type=\"Class\").ravel().astype(np.int)\n",
    "accuracy_score(_y_test, _y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5, 116,  15, ..., 116,  54, 111])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([65, 52, 38, ..., 40, 19, 65])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train = xgb.DMatrix(_X_train, label=_y_train)\n",
    "xgb_test = xgb.DMatrix(_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.01,\n",
    "    \"gamma\": 0.1,\n",
    "    \"max_depth\": 4,\n",
    "    \"num_class\": 128,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"seed\": 1272,     \n",
    "    \"subsample\": 0.7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:4.84599+3.4308e-05\ttest-mlogloss:4.84655+0.000121354\n",
      "[1]\ttrain-mlogloss:4.83983+0.000200851\ttest-mlogloss:4.84086+0.000270838\n",
      "[2]\ttrain-mlogloss:4.83364+0.000273281\ttest-mlogloss:4.8352+0.000393063\n",
      "[3]\ttrain-mlogloss:4.82739+0.000368472\ttest-mlogloss:4.82947+0.000549695\n",
      "[4]\ttrain-mlogloss:4.8214+0.000370113\ttest-mlogloss:4.82394+0.000565074\n",
      "[5]\ttrain-mlogloss:4.81526+0.000428553\ttest-mlogloss:4.81829+0.000607652\n",
      "[6]\ttrain-mlogloss:4.80916+0.000552534\ttest-mlogloss:4.8127+0.000656493\n",
      "[7]\ttrain-mlogloss:4.80295+0.000538376\ttest-mlogloss:4.80701+0.000668191\n",
      "[8]\ttrain-mlogloss:4.79685+0.000636949\ttest-mlogloss:4.8014+0.000741941\n",
      "[9]\ttrain-mlogloss:4.79066+0.000667106\ttest-mlogloss:4.79572+0.000763709\n",
      "[10]\ttrain-mlogloss:4.7845+0.000608217\ttest-mlogloss:4.79007+0.000752721\n",
      "[11]\ttrain-mlogloss:4.77844+0.000554622\ttest-mlogloss:4.78457+0.000704097\n",
      "[12]\ttrain-mlogloss:4.77237+0.000668272\ttest-mlogloss:4.77909+0.000686543\n",
      "[13]\ttrain-mlogloss:4.76641+0.000615831\ttest-mlogloss:4.77367+0.000753354\n",
      "[14]\ttrain-mlogloss:4.76019+0.000582336\ttest-mlogloss:4.76799+0.000737255\n",
      "[15]\ttrain-mlogloss:4.75413+0.00068809\ttest-mlogloss:4.76243+0.000781937\n",
      "[16]\ttrain-mlogloss:4.74794+0.000676562\ttest-mlogloss:4.75677+0.000807818\n",
      "[17]\ttrain-mlogloss:4.74183+0.000768941\ttest-mlogloss:4.75124+0.000836646\n",
      "[18]\ttrain-mlogloss:4.73564+0.000818654\ttest-mlogloss:4.74556+0.000873296\n",
      "[19]\ttrain-mlogloss:4.72964+0.000777783\ttest-mlogloss:4.74013+0.000844884\n",
      "[20]\ttrain-mlogloss:4.72335+0.000900365\ttest-mlogloss:4.73438+0.000919404\n",
      "[21]\ttrain-mlogloss:4.71738+0.000936302\ttest-mlogloss:4.72896+0.000892658\n",
      "[22]\ttrain-mlogloss:4.71128+0.000924392\ttest-mlogloss:4.72336+0.000961334\n",
      "[23]\ttrain-mlogloss:4.7052+0.00102086\ttest-mlogloss:4.71776+0.00113044\n",
      "[24]\ttrain-mlogloss:4.69902+0.000957502\ttest-mlogloss:4.71214+0.00119042\n",
      "[25]\ttrain-mlogloss:4.69274+0.000891059\ttest-mlogloss:4.70635+0.00115751\n",
      "[26]\ttrain-mlogloss:4.68662+0.000838288\ttest-mlogloss:4.70073+0.00120573\n",
      "[27]\ttrain-mlogloss:4.68044+0.000860223\ttest-mlogloss:4.69506+0.00117722\n",
      "[28]\ttrain-mlogloss:4.67432+0.000945772\ttest-mlogloss:4.68949+0.00121377\n",
      "[29]\ttrain-mlogloss:4.66827+0.000946321\ttest-mlogloss:4.68398+0.00128798\n",
      "[30]\ttrain-mlogloss:4.6621+0.00103886\ttest-mlogloss:4.67828+0.00118556\n",
      "[31]\ttrain-mlogloss:4.65606+0.00111564\ttest-mlogloss:4.67277+0.00116114\n",
      "[32]\ttrain-mlogloss:4.65009+0.00128586\ttest-mlogloss:4.66735+0.00121132\n",
      "[33]\ttrain-mlogloss:4.64397+0.00129505\ttest-mlogloss:4.66174+0.00117479\n",
      "[34]\ttrain-mlogloss:4.63784+0.00143148\ttest-mlogloss:4.6561+0.00106416\n",
      "[35]\ttrain-mlogloss:4.63177+0.00143578\ttest-mlogloss:4.65053+0.00108707\n",
      "[36]\ttrain-mlogloss:4.62572+0.00143127\ttest-mlogloss:4.645+0.00127577\n",
      "[37]\ttrain-mlogloss:4.61954+0.00146004\ttest-mlogloss:4.63934+0.00131172\n",
      "[38]\ttrain-mlogloss:4.6135+0.00144635\ttest-mlogloss:4.63377+0.0012106\n",
      "[39]\ttrain-mlogloss:4.60741+0.00161652\ttest-mlogloss:4.62818+0.00125932\n",
      "[40]\ttrain-mlogloss:4.60142+0.00168712\ttest-mlogloss:4.62271+0.00133475\n",
      "[41]\ttrain-mlogloss:4.59537+0.00174335\ttest-mlogloss:4.61712+0.00133196\n",
      "[42]\ttrain-mlogloss:4.5894+0.00175805\ttest-mlogloss:4.61162+0.00132171\n",
      "[43]\ttrain-mlogloss:4.58333+0.00167554\ttest-mlogloss:4.6061+0.00142406\n",
      "[44]\ttrain-mlogloss:4.57733+0.00172886\ttest-mlogloss:4.60061+0.00147112\n",
      "[45]\ttrain-mlogloss:4.57137+0.00182842\ttest-mlogloss:4.59518+0.00163177\n",
      "[46]\ttrain-mlogloss:4.56536+0.00170084\ttest-mlogloss:4.58969+0.00170238\n",
      "[47]\ttrain-mlogloss:4.55934+0.00160953\ttest-mlogloss:4.58418+0.00183443\n",
      "[48]\ttrain-mlogloss:4.55331+0.00178312\ttest-mlogloss:4.57877+0.00188833\n",
      "[49]\ttrain-mlogloss:4.54728+0.00180811\ttest-mlogloss:4.57324+0.00192223\n",
      "[50]\ttrain-mlogloss:4.54118+0.00183499\ttest-mlogloss:4.56764+0.00195189\n",
      "[51]\ttrain-mlogloss:4.53512+0.00199666\ttest-mlogloss:4.56207+0.00201434\n",
      "[52]\ttrain-mlogloss:4.52914+0.00199205\ttest-mlogloss:4.55661+0.00206224\n",
      "[53]\ttrain-mlogloss:4.52316+0.00202069\ttest-mlogloss:4.55114+0.00225836\n",
      "[54]\ttrain-mlogloss:4.51706+0.0021021\ttest-mlogloss:4.54557+0.00227785\n",
      "[55]\ttrain-mlogloss:4.51097+0.0021733\ttest-mlogloss:4.54006+0.00225728\n",
      "[56]\ttrain-mlogloss:4.50505+0.00224574\ttest-mlogloss:4.53467+0.00240057\n",
      "[57]\ttrain-mlogloss:4.49916+0.00210775\ttest-mlogloss:4.52923+0.00254318\n",
      "[58]\ttrain-mlogloss:4.49309+0.00212168\ttest-mlogloss:4.52366+0.00248265\n",
      "[59]\ttrain-mlogloss:4.48716+0.00199501\ttest-mlogloss:4.51823+0.00270187\n",
      "[60]\ttrain-mlogloss:4.48111+0.00201751\ttest-mlogloss:4.51265+0.00268754\n",
      "[61]\ttrain-mlogloss:4.47516+0.00187613\ttest-mlogloss:4.50727+0.00268776\n",
      "[62]\ttrain-mlogloss:4.46915+0.00195616\ttest-mlogloss:4.50182+0.00265409\n",
      "[63]\ttrain-mlogloss:4.46316+0.00197067\ttest-mlogloss:4.49636+0.00275926\n",
      "[64]\ttrain-mlogloss:4.45707+0.00211484\ttest-mlogloss:4.4908+0.00284039\n",
      "[65]\ttrain-mlogloss:4.45113+0.00216211\ttest-mlogloss:4.48541+0.00288913\n",
      "[66]\ttrain-mlogloss:4.44515+0.00214525\ttest-mlogloss:4.47988+0.00298009\n",
      "[67]\ttrain-mlogloss:4.43921+0.00213915\ttest-mlogloss:4.47444+0.00289736\n",
      "[68]\ttrain-mlogloss:4.43334+0.00212663\ttest-mlogloss:4.46908+0.00296516\n",
      "[69]\ttrain-mlogloss:4.42741+0.00209179\ttest-mlogloss:4.46367+0.00319475\n",
      "[70]\ttrain-mlogloss:4.42152+0.00208162\ttest-mlogloss:4.45825+0.00328362\n",
      "[71]\ttrain-mlogloss:4.41557+0.00215109\ttest-mlogloss:4.45282+0.00330046\n",
      "[72]\ttrain-mlogloss:4.40963+0.00223672\ttest-mlogloss:4.44737+0.00332778\n",
      "[73]\ttrain-mlogloss:4.40379+0.00229476\ttest-mlogloss:4.44203+0.00341511\n",
      "[74]\ttrain-mlogloss:4.39801+0.00239556\ttest-mlogloss:4.43675+0.00354281\n",
      "[75]\ttrain-mlogloss:4.39212+0.00259573\ttest-mlogloss:4.43134+0.00363054\n",
      "[76]\ttrain-mlogloss:4.38612+0.0026157\ttest-mlogloss:4.42588+0.00367807\n",
      "[77]\ttrain-mlogloss:4.3802+0.00257913\ttest-mlogloss:4.42039+0.00374592\n",
      "[78]\ttrain-mlogloss:4.37426+0.00254185\ttest-mlogloss:4.41508+0.00396051\n",
      "[79]\ttrain-mlogloss:4.36835+0.00273805\ttest-mlogloss:4.40971+0.00392396\n",
      "[80]\ttrain-mlogloss:4.36247+0.00272331\ttest-mlogloss:4.40433+0.00387414\n",
      "[81]\ttrain-mlogloss:4.35667+0.00265427\ttest-mlogloss:4.399+0.00393545\n",
      "[82]\ttrain-mlogloss:4.35069+0.00270078\ttest-mlogloss:4.39354+0.0039331\n",
      "[83]\ttrain-mlogloss:4.34468+0.00271526\ttest-mlogloss:4.38807+0.00396406\n",
      "[84]\ttrain-mlogloss:4.33882+0.00261002\ttest-mlogloss:4.38269+0.00398854\n",
      "[85]\ttrain-mlogloss:4.333+0.00277164\ttest-mlogloss:4.37733+0.0041258\n",
      "[86]\ttrain-mlogloss:4.32714+0.0027442\ttest-mlogloss:4.37193+0.00431236\n",
      "[87]\ttrain-mlogloss:4.32122+0.00264355\ttest-mlogloss:4.3665+0.00426565\n",
      "[88]\ttrain-mlogloss:4.31524+0.00263658\ttest-mlogloss:4.36098+0.00431979\n",
      "[89]\ttrain-mlogloss:4.3093+0.00274657\ttest-mlogloss:4.35556+0.00433544\n",
      "[90]\ttrain-mlogloss:4.30348+0.00267001\ttest-mlogloss:4.35025+0.00427191\n",
      "[91]\ttrain-mlogloss:4.29763+0.00266626\ttest-mlogloss:4.34485+0.00431752\n",
      "[92]\ttrain-mlogloss:4.29185+0.00279431\ttest-mlogloss:4.33958+0.00418416\n",
      "[93]\ttrain-mlogloss:4.28586+0.00281811\ttest-mlogloss:4.33412+0.00417201\n",
      "[94]\ttrain-mlogloss:4.27999+0.00276863\ttest-mlogloss:4.32872+0.0042568\n",
      "[95]\ttrain-mlogloss:4.27407+0.00273118\ttest-mlogloss:4.32325+0.00430638\n",
      "[96]\ttrain-mlogloss:4.26829+0.00266833\ttest-mlogloss:4.31792+0.00433449\n",
      "[97]\ttrain-mlogloss:4.2624+0.0026509\ttest-mlogloss:4.31251+0.00433708\n",
      "[98]\ttrain-mlogloss:4.25647+0.00261592\ttest-mlogloss:4.30708+0.00433533\n",
      "[99]\ttrain-mlogloss:4.25051+0.00260218\ttest-mlogloss:4.30157+0.00438743\n",
      "[100]\ttrain-mlogloss:4.24467+0.00266505\ttest-mlogloss:4.29621+0.00434066\n",
      "[101]\ttrain-mlogloss:4.23877+0.0027782\ttest-mlogloss:4.29079+0.00450937\n",
      "[102]\ttrain-mlogloss:4.23297+0.00273609\ttest-mlogloss:4.28548+0.00456911\n",
      "[103]\ttrain-mlogloss:4.22709+0.00267363\ttest-mlogloss:4.28016+0.00465904\n",
      "[104]\ttrain-mlogloss:4.22129+0.00274882\ttest-mlogloss:4.27487+0.00467036\n",
      "[105]\ttrain-mlogloss:4.21549+0.00277536\ttest-mlogloss:4.26958+0.00469331\n",
      "[106]\ttrain-mlogloss:4.20972+0.00283047\ttest-mlogloss:4.26428+0.00458174\n",
      "[107]\ttrain-mlogloss:4.20392+0.00279899\ttest-mlogloss:4.25902+0.00472951\n",
      "[108]\ttrain-mlogloss:4.19815+0.00272605\ttest-mlogloss:4.25375+0.00480086\n",
      "[109]\ttrain-mlogloss:4.19238+0.0027931\ttest-mlogloss:4.24844+0.00490981\n",
      "[110]\ttrain-mlogloss:4.1866+0.00280516\ttest-mlogloss:4.24314+0.00496942\n",
      "[111]\ttrain-mlogloss:4.18071+0.00277077\ttest-mlogloss:4.2377+0.00497652\n",
      "[112]\ttrain-mlogloss:4.1749+0.0029014\ttest-mlogloss:4.23237+0.00493902\n",
      "[113]\ttrain-mlogloss:4.169+0.00294668\ttest-mlogloss:4.22695+0.00501784\n",
      "[114]\ttrain-mlogloss:4.16323+0.00298538\ttest-mlogloss:4.22167+0.0050771\n",
      "[115]\ttrain-mlogloss:4.15739+0.00297743\ttest-mlogloss:4.21626+0.00512155\n",
      "[116]\ttrain-mlogloss:4.15157+0.00306802\ttest-mlogloss:4.21095+0.00522981\n",
      "[117]\ttrain-mlogloss:4.14593+0.00307953\ttest-mlogloss:4.2058+0.00532574\n",
      "[118]\ttrain-mlogloss:4.14004+0.00307231\ttest-mlogloss:4.20042+0.0055406\n",
      "[119]\ttrain-mlogloss:4.13418+0.00315398\ttest-mlogloss:4.19506+0.0055521\n",
      "[120]\ttrain-mlogloss:4.1283+0.00327502\ttest-mlogloss:4.18971+0.00560827\n",
      "[121]\ttrain-mlogloss:4.12252+0.00329206\ttest-mlogloss:4.18434+0.00558891\n",
      "[122]\ttrain-mlogloss:4.11674+0.00337142\ttest-mlogloss:4.17912+0.00570981\n",
      "[123]\ttrain-mlogloss:4.11096+0.00344449\ttest-mlogloss:4.1738+0.00568396\n",
      "[124]\ttrain-mlogloss:4.10521+0.00341297\ttest-mlogloss:4.16849+0.00563608\n",
      "[125]\ttrain-mlogloss:4.09952+0.00336066\ttest-mlogloss:4.1632+0.00575089\n",
      "[126]\ttrain-mlogloss:4.09376+0.00340966\ttest-mlogloss:4.15791+0.00576442\n",
      "[127]\ttrain-mlogloss:4.08792+0.00350938\ttest-mlogloss:4.1526+0.00586169\n",
      "[128]\ttrain-mlogloss:4.08218+0.0034453\ttest-mlogloss:4.14734+0.0059033\n",
      "[129]\ttrain-mlogloss:4.07628+0.00353167\ttest-mlogloss:4.14188+0.00591244\n",
      "[130]\ttrain-mlogloss:4.07056+0.00359983\ttest-mlogloss:4.13665+0.00576536\n",
      "[131]\ttrain-mlogloss:4.06468+0.00361525\ttest-mlogloss:4.13118+0.00582874\n",
      "[132]\ttrain-mlogloss:4.05897+0.00360723\ttest-mlogloss:4.1259+0.00588255\n",
      "[133]\ttrain-mlogloss:4.0533+0.00370829\ttest-mlogloss:4.12065+0.00591465\n",
      "[134]\ttrain-mlogloss:4.04755+0.00375741\ttest-mlogloss:4.11536+0.00616655\n",
      "[135]\ttrain-mlogloss:4.04183+0.00364874\ttest-mlogloss:4.11018+0.00629302\n",
      "[136]\ttrain-mlogloss:4.03608+0.00368942\ttest-mlogloss:4.10482+0.00643655\n",
      "[137]\ttrain-mlogloss:4.03036+0.00371878\ttest-mlogloss:4.09958+0.00640562\n",
      "[138]\ttrain-mlogloss:4.02472+0.00389029\ttest-mlogloss:4.09442+0.00655355\n",
      "[139]\ttrain-mlogloss:4.01901+0.00399962\ttest-mlogloss:4.08916+0.00675775\n",
      "[140]\ttrain-mlogloss:4.01343+0.00401609\ttest-mlogloss:4.08402+0.00675715\n",
      "[141]\ttrain-mlogloss:4.00769+0.00394455\ttest-mlogloss:4.07877+0.00680237\n",
      "[142]\ttrain-mlogloss:4.00206+0.00392825\ttest-mlogloss:4.07364+0.00687845\n",
      "[143]\ttrain-mlogloss:3.99629+0.00384941\ttest-mlogloss:4.06837+0.0068956\n",
      "[144]\ttrain-mlogloss:3.99057+0.00384364\ttest-mlogloss:4.06308+0.00695753\n",
      "[145]\ttrain-mlogloss:3.98491+0.00388035\ttest-mlogloss:4.0579+0.00698105\n",
      "[146]\ttrain-mlogloss:3.97922+0.00394466\ttest-mlogloss:4.05267+0.00715293\n",
      "[147]\ttrain-mlogloss:3.97355+0.00396248\ttest-mlogloss:4.04747+0.00716437\n",
      "[148]\ttrain-mlogloss:3.96788+0.00406112\ttest-mlogloss:4.04227+0.00714737\n",
      "[149]\ttrain-mlogloss:3.96209+0.00412599\ttest-mlogloss:4.03691+0.00714135\n",
      "[150]\ttrain-mlogloss:3.95647+0.00417246\ttest-mlogloss:4.03171+0.00721206\n",
      "[151]\ttrain-mlogloss:3.95067+0.00416619\ttest-mlogloss:4.02634+0.00723807\n",
      "[152]\ttrain-mlogloss:3.94507+0.0041045\ttest-mlogloss:4.02116+0.00727204\n",
      "[153]\ttrain-mlogloss:3.9394+0.00406834\ttest-mlogloss:4.01596+0.0072076\n",
      "[154]\ttrain-mlogloss:3.9337+0.00400104\ttest-mlogloss:4.01066+0.00731108\n",
      "[155]\ttrain-mlogloss:3.92795+0.00400227\ttest-mlogloss:4.00534+0.00731053\n",
      "[156]\ttrain-mlogloss:3.92229+0.00413768\ttest-mlogloss:4.0001+0.00738212\n",
      "[157]\ttrain-mlogloss:3.91669+0.00413745\ttest-mlogloss:3.99494+0.00748503\n",
      "[158]\ttrain-mlogloss:3.91115+0.00422215\ttest-mlogloss:3.98985+0.00749976\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-acde6fb1afbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m cv_results = xgb.cv(params=params, dtrain=xgb_train,\n\u001b[1;32m      2\u001b[0m                     \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     nfold=5, verbose_eval=1)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mcv\u001b[0;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks, shuffle)\u001b[0m\n\u001b[1;32m    404\u001b[0m                            evaluation_result_list=None))\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iteration, fobj)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;34m\"\"\"\"Update the boosters for one iteration\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 898\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv_results = xgb.cv(params=params, dtrain=xgb_train,\n",
    "                    num_boost_round=1000, early_stopping_rounds=10,\n",
    "                    nfold=5, verbose_eval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:52: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "best_num_round = np.argmin(cv_results['test-%s-mean' % params['eval_metric']])\n",
    "\n",
    "model = xgb.train(params, dtrain=xgb_train, num_boost_round=best_num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005826271186440678"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_y_pred = model.predict(xgb_test).astype(np.int)\n",
    "accuracy_score(_y_test, _y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  5, 116,  18, ..., 116,  54, 111]),\n",
       " array([65, 52, 38, ..., 40, 19, 65]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_y_pred, _y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100,  39,  65, 101,  31,  56,  39,  65,  31, 101, 400, 400])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_X_test[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f0': 3886,\n",
       " 'f1': 3644,\n",
       " 'f10': 2187,\n",
       " 'f11': 2456,\n",
       " 'f2': 3394,\n",
       " 'f3': 4451,\n",
       " 'f4': 8753,\n",
       " 'f5': 4269,\n",
       " 'f6': 3998,\n",
       " 'f7': 3827,\n",
       " 'f8': 4336,\n",
       " 'f9': 7765}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train_meta framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, tpe, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "CV_SPLIT = StratifiedKFold(n_splits=5, shuffle=True, random_state=555)\n",
    "MODEL = Pipeline\n",
    "SCORINGS = [\"neg_log_loss\", \"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 10\n",
    "scorings = SCORINGS\n",
    "cv = CV_SPLIT\n",
    "estimator_cls = MODEL\n",
    "model_params = {\n",
    "    \"steps\": [\n",
    "        (\"scaler\", StandardScaler),\n",
    "        (\"log_reg\", LogisticRegression)\n",
    "    ]\n",
    "}\n",
    "model_hp_params = {\n",
    "    \"log_reg\": {\n",
    "        \"C\": hp.loguniform(\"C\", 0, 4),\n",
    "        \"random_state\": hp.randint(\"random_state\", 12345)\n",
    "    }\n",
    "}\n",
    "model_hp_params.update(model_params)\n",
    "fit_params = {}\n",
    "n_jobs = 10\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def hp_score(model_hp_params):\n",
    "    if estimator_cls == Pipeline:\n",
    "        steps=model_hp_params['steps']\n",
    "        nsteps = []\n",
    "        for name, fn in steps:\n",
    "            nsteps.append((name, fn(**model_hp_params[name]) if name in model_hp_params else fn()))\n",
    "        estimator = estimator_cls(steps=nsteps)\n",
    "    else:\n",
    "        estimator = estimator_cls(**model_hp_params)\n",
    "\n",
    "    scores = cross_validate(estimator, _X, _y, cv=cv, scoring=scorings,\n",
    "                            fit_params=fit_params, \n",
    "                            return_train_score=True,\n",
    "                            n_jobs=n_jobs, verbose=debug)\n",
    "\n",
    "    print(\"CV scores:\")\n",
    "    for scoring in scorings:\n",
    "        print(\"{} : \\n\\t train: {} \\n\\t test: {}\".format(scoring, \n",
    "                                                   scores[\"train_{}\".format(scoring)].tolist(),\n",
    "                                                   scores[\"test_{}\".format(scoring)].tolist()))\n",
    "\n",
    "    mean_test_loss = np.abs(np.mean(scores[\"test_{}\".format(scorings[0])]))\n",
    "    return {\n",
    "        'loss': mean_test_loss,\n",
    "        'status': STATUS_OK\n",
    "    }\n",
    "\n",
    "\n",
    "def hp_optimize(score_fn, params_space, max_evals):\n",
    "    trials = Trials()\n",
    "    best_params = fmin(score_fn, params_space, algo=tpe.suggest, trials=trials, max_evals=max_evals, verbose=debug)\n",
    "    return best_params, trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores:\n",
      "neg_log_loss : \n",
      "\t train: [-3.5480578202532933, -3.573541458420192, -3.556483475390815, -3.5734635082382846, -3.559566390562097] \n",
      "\t test: [-3.876761071432662, -3.7987699002087667, -3.861823010894172, -3.8203330980363828, -3.856333740654891]\n",
      "accuracy : \n",
      "\t train: [0.21692276990620635, 0.21612452604270604, 0.2190988835725678, 0.22164948453608246, 0.21369539551357733] \n",
      "\t test: [0.15546875, 0.17109375, 0.15137254901960784, 0.15717722534081796, 0.16708023159636062]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   5 out of   5 | elapsed:    3.7s finished\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores:\n",
      "neg_log_loss : \n",
      "\t train: [-3.5335622329113594, -3.560580454612384, -3.543369753758897, -3.559195938468275, -3.5462211255181173] \n",
      "\t test: [-3.8788090871378316, -3.7920604390463724, -3.8588589626925316, -3.8285753684615313, -3.8568271982794786]\n",
      "accuracy : \n",
      "\t train: [0.21951706246258232, 0.21971662342845738, 0.22149122807017543, 0.22620935765265662, 0.2170405352223534] \n",
      "\t test: [0.15390625, 0.175, 0.15294117647058825, 0.16038492381716118, 0.16790736145574855]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   5 out of   5 | elapsed:    3.7s finished\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores:\n",
      "neg_log_loss : \n",
      "\t train: [-3.5335107548027653, -3.560536443855174, -3.543325165563332, -3.5591428628079376, -3.5461749022047337] \n",
      "\t test: [-3.8787994822713303, -3.792036417279269, -3.8588529614396476, -3.8286617195329233, -3.856841095793991]\n",
      "accuracy : \n",
      "\t train: [0.21951706246258232, 0.21971662342845738, 0.22169059011164274, 0.22620935765265662, 0.2170405352223534] \n",
      "\t test: [0.15390625, 0.175, 0.15294117647058825, 0.16038492381716118, 0.16790736145574855]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   5 out of   5 | elapsed:    3.7s finished\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "[Parallel(n_jobs=10)]: Done   5 out of   5 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores:\n",
      "neg_log_loss : \n",
      "\t train: [-3.5301695480330655, -3.5577411073066374, -3.5405150856591243, -3.555634485038779, -3.543262366624294] \n",
      "\t test: [-3.878288170768221, -3.7906628162536706, -3.858487046629541, -3.836962701899376, -3.8580629924731498]\n",
      "accuracy : \n",
      "\t train: [0.22031530632608262, 0.22051486729195768, 0.22208931419457736, 0.22759714512291832, 0.21881149153876425] \n",
      "\t test: [0.15390625, 0.17578125, 0.15372549019607842, 0.161186848436247, 0.1687344913151365]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "[Parallel(n_jobs=10)]: Done   5 out of   5 | elapsed:    3.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores:\n",
      "neg_log_loss : \n",
      "\t train: [-3.6412149405083256, -3.6623541350780595, -3.647051159054922, -3.6636754148795325, -3.649798346226116] \n",
      "\t test: [-3.910411087285746, -3.8537241934283726, -3.9015714957865724, -3.8591563945811527, -3.8921377188584545]\n",
      "accuracy : \n",
      "\t train: [0.2021552584314508, 0.19976052684094991, 0.2031499202551834, 0.20796986518636004, 0.20070838252656434] \n",
      "\t test: [0.14609375, 0.15703125, 0.1419607843137255, 0.14194065757818766, 0.1555004135649297]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores:\n",
      "neg_log_loss : \n",
      "\t train: [-3.5418475465750525, -3.5678841565703174, -3.550749534889018, -3.567418335967986, -3.5537636996286] \n",
      "\t test: [-3.877359549333575, -3.7957722507117437, -3.8603290674897295, -3.821554630797394, -3.855913665578232]\n",
      "accuracy : \n",
      "\t train: [0.21772101376970665, 0.2179205747355817, 0.2204944178628389, 0.22303727200634418, 0.2154663518299882] \n",
      "\t test: [0.15390625, 0.171875, 0.15058823529411763, 0.15797914995990378, 0.16708023159636062]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   5 out of   5 | elapsed:    3.6s finished\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "[Parallel(n_jobs=10)]: Done   5 out of   5 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores:\n",
      "neg_log_loss : \n",
      "\t train: [-3.5302253370252634, -3.557786554505024, -3.5405603840559934, -3.555695370667215, -3.543309809502242] \n",
      "\t test: [-3.878294492633208, -3.7906850542229513, -3.858491684502919, -3.8367612337592343, -3.8580357675958092]\n",
      "accuracy : \n",
      "\t train: [0.22051486729195768, 0.22051486729195768, 0.22208931419457736, 0.22759714512291832, 0.21861471861471862] \n",
      "\t test: [0.15390625, 0.17578125, 0.15372549019607842, 0.161186848436247, 0.1687344913151365]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores:\n",
      "neg_log_loss : \n",
      "\t train: [-3.530103692287255, -3.557686402848865, -3.5404602807250956, -3.555561235270802, -3.543205117480558] \n",
      "\t test: [-3.8782802025325758, -3.790635995336726, -3.858481555953372, -3.837212224952376, -3.858094539802838]\n",
      "accuracy : \n",
      "\t train: [0.22031530632608262, 0.22051486729195768, 0.22208931419457736, 0.22759714512291832, 0.21861471861471862] \n",
      "\t test: [0.15390625, 0.17578125, 0.15372549019607842, 0.161186848436247, 0.1687344913151365]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   5 out of   5 | elapsed:    3.7s finished\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores:\n",
      "neg_log_loss : \n",
      "\t train: [-3.5328783101332215, -3.559998474759919, -3.542783448043214, -3.558495327145858, -3.5456159604034503] \n",
      "\t test: [-3.8786859094869968, -3.791770079503877, -3.858772218786317, -3.8297887195309777, -3.8570205853319]\n",
      "accuracy : \n",
      "\t train: [0.21971662342845738, 0.21991618439433247, 0.22188995215311005, 0.22660586835844568, 0.21802439984258165] \n",
      "\t test: [0.15390625, 0.175, 0.15294117647058825, 0.16038492381716118, 0.16790736145574855]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   5 out of   5 | elapsed:    3.6s finished\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores:\n",
      "neg_log_loss : \n",
      "\t train: [-3.53383311165198, -3.5608121551076777, -3.5436031694436383, -3.5594717026322664, -3.546461708470491] \n",
      "\t test: [-3.8788592191898728, -3.7921754444658866, -3.858895201865167, -3.828140446035383, -3.8567572498683806]\n",
      "accuracy : \n",
      "\t train: [0.21931750149670726, 0.21971662342845738, 0.22129186602870812, 0.22620935765265662, 0.2170405352223534] \n",
      "\t test: [0.15390625, 0.17421875, 0.15294117647058825, 0.1595829991980754, 0.16790736145574855]\n",
      "Best parameters: \n",
      "{'steps': [('scaler', <class 'sklearn.preprocessing.data.StandardScaler'>), ('log_reg', <class 'sklearn.linear_model.logistic.LogisticRegression'>)], 'random_state': 7824, 'C': 10.739394454022523}\n",
      "Best trial : \n",
      "{'result': {'status': 'ok', 'loss': 3.8421858327821345}, 'exp_key': None, 'owner': None, 'state': 2, 'tid': 5, 'misc': {'tid': 5, 'idxs': {'random_state': [5], 'C': [5]}, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'vals': {'random_state': [7824], 'C': [10.739394454022523]}, 'workdir': None}, 'version': 0, 'book_time': datetime.datetime(2018, 4, 28, 21, 42, 26, 806000), 'spec': None, 'refresh_time': datetime.datetime(2018, 4, 28, 21, 42, 30, 604000)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   5 out of   5 | elapsed:    3.6s finished\n"
     ]
    }
   ],
   "source": [
    "best_params, trials = hp_optimize(hp_score, model_hp_params, max_evals=n_trials)\n",
    "best_params.update(model_params)\n",
    "\n",
    "print(\"Best parameters: \\n{}\".format(best_params))\n",
    "print(\"Best trial : \\n{}\".format(trials.best_trial))\n",
    "\n",
    "# print(\"Train meta model on complete dataset\")\n",
    "# estimator = estimator_cls(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10.739394454022523,\n",
       " 'random_state': 7824,\n",
       " 'steps': [('scaler', sklearn.preprocessing.data.StandardScaler),\n",
       "  ('log_reg', sklearn.linear_model.logistic.LogisticRegression)]}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params.update(model_params)\n",
    "estimator = estimator_cls(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "CV_SPLIT = StratifiedKFold(n_splits=5, shuffle=True, random_state=555)\n",
    "MODEL = CatBoostClassifier\n",
    "SCORINGS = [\"neg_log_loss\", ] #\"precision_macro\", \"recall_macro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 2\n",
    "scorings = SCORINGS\n",
    "cv = CV_SPLIT\n",
    "estimator_cls = MODEL\n",
    "model_params = {\n",
    "    \"iterations\": 5,\n",
    "    \"loss_function\": \"MultiClass\",\n",
    "    \"od_type\": \"Iter\",\n",
    "    \"od_wait\": 50,\n",
    "    \"bootstrap_type\": \"Bernoulli\",\n",
    "    \"task_type\": \"CPU\",\n",
    "    \"verbose\": True,\n",
    "    \"metric_period\": 1\n",
    "}\n",
    "model_hp_params = {\n",
    "    \"depth\": 2 + hp.randint(\"depth\", 5),\n",
    "    \"learning_rate\": hp.quniform(\"learning_rate\", 0.001, 0.5, 0.005),\n",
    "    \"l2_leaf_reg\": 2 + hp.randint(\"l2_leaf_reg\", 2),\n",
    "    \"random_seed\": hp.randint(\"random_seed\", 12345),\n",
    "    \"subsample\": hp.quniform(\"subsample\", 0.5, 1.0, 0.01)\n",
    "}\n",
    "model_hp_params.update(model_params)\n",
    "fit_params = {}\n",
    "n_jobs = 10\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = X.values\n",
    "_y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def hp_score(model_hp_params):\n",
    "    \n",
    "    estimator = estimator_cls(**model_hp_params)\n",
    "\n",
    "    scores = cross_validate(estimator, _X, _y, cv=cv, scoring=scorings,\n",
    "                            fit_params=fit_params,\n",
    "                            return_train_score=True,                            \n",
    "                            n_jobs=n_jobs)\n",
    "\n",
    "    print(\"CV scores:\")\n",
    "    for scoring in scorings:\n",
    "        print(\"{} : \\n\\t train: {} \\n\\t test: {}\".format(scoring, \n",
    "                                                   scores[\"train_{}\".format(scoring)].tolist(),\n",
    "                                                   scores[\"test_{}\".format(scoring)].tolist()))\n",
    "\n",
    "    mean_test_loss = np.abs(np.mean(scores[\"test_{}\".format(scoring)]))\n",
    "    return {\n",
    "        'loss': mean_test_loss,\n",
    "        'status': STATUS_OK\n",
    "    }\n",
    "\n",
    "\n",
    "def hp_optimize(score_fn, params_space, max_evals):\n",
    "    trials = Trials()\n",
    "    best_params = fmin(score_fn, params_space, algo=tpe.suggest, trials=trials, max_evals=max_evals, verbose=debug)\n",
    "    return best_params, trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: -4.7117042\ttotal: 20.6s\tremaining: 1m 22s\n",
      "0:\tlearn: -4.7280812\ttotal: 20.7s\tremaining: 1m 22s\n",
      "0:\tlearn: -4.7249822\ttotal: 20.9s\tremaining: 1m 23s\n",
      "0:\tlearn: -4.6774325\ttotal: 22.2s\tremaining: 1m 28s\n",
      "0:\tlearn: -4.7068981\ttotal: 23.7s\tremaining: 1m 34s\n",
      "1:\tlearn: -4.5168791\ttotal: 41.5s\tremaining: 1m 2s\n",
      "1:\tlearn: -4.5250657\ttotal: 41.7s\tremaining: 1m 2s\n",
      "1:\tlearn: -4.6246349\ttotal: 41.8s\tremaining: 1m 2s\n",
      "1:\tlearn: -4.6276355\ttotal: 42.6s\tremaining: 1m 3s\n",
      "1:\tlearn: -4.5363106\ttotal: 46.8s\tremaining: 1m 10s\n",
      "2:\tlearn: -4.4961720\ttotal: 1m 2s\tremaining: 41.9s\n",
      "2:\tlearn: -4.3414154\ttotal: 1m 3s\tremaining: 42.2s\n",
      "2:\tlearn: -4.4109650\ttotal: 1m 3s\tremaining: 42.3s\n",
      "2:\tlearn: -4.4711836\ttotal: 1m 5s\tremaining: 43.8s\n",
      "2:\tlearn: -4.4313898\ttotal: 1m 7s\tremaining: 44.9s\n",
      "3:\tlearn: -4.3371345\ttotal: 1m 24s\tremaining: 21s\n",
      "3:\tlearn: -4.3623223\ttotal: 1m 25s\tremaining: 21.3s\n",
      "3:\tlearn: -4.2483936\ttotal: 1m 26s\tremaining: 21.6s\n",
      "3:\tlearn: -4.3404164\ttotal: 1m 27s\tremaining: 21.8s\n",
      "3:\tlearn: -4.2623334\ttotal: 1m 28s\tremaining: 22.2s\n",
      "4:\tlearn: -4.2955514\ttotal: 1m 46s\tremaining: 0us\n",
      "4:\tlearn: -4.2300309\ttotal: 1m 47s\tremaining: 0us\n",
      "4:\tlearn: -4.2237970\ttotal: 1m 47s\tremaining: 0us\n",
      "4:\tlearn: -4.1754108\ttotal: 1m 47s\tremaining: 0us\n",
      "4:\tlearn: -4.1793533\ttotal: 1m 47s\tremaining: 0us\n",
      "CV scores:\n",
      "neg_log_loss : \n",
      "\t train: [-4.230030944644081, -4.2237969963568265, -4.295551363333924, -4.179353263800418, -4.175410806600211] \n",
      "\t test: [-4.25157665135408, -4.231374858290454, -4.305536161939012, -4.183279148082734, -4.195846683771765]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-477:\n",
      "Process ForkPoolWorker-476:\n",
      "Process ForkPoolWorker-478:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-480:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-479:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-e83826ef7155>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp_optimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_hp_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best parameters: \\n{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best trial : \\n{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-808ea67c3bd4>\u001b[0m in \u001b[0;36mhp_optimize\u001b[0;34m(score_fn, params_space, max_evals)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhp_optimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         )\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             return_argmin=return_argmin)\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     verbose=verbose)\n\u001b[1;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 840\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-808ea67c3bd4>\u001b[0m in \u001b[0;36mhp_score\u001b[0;34m(model_hp_params)\u001b[0m\n\u001b[1;32m      8\u001b[0m                             \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                             \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                             n_jobs=n_jobs)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CV scores:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_params, trials = hp_optimize(hp_score, model_hp_params, max_evals=n_trials)\n",
    "best_params.update(model_params)\n",
    "\n",
    "print(\"Best parameters: \\n{}\".format(best_params))\n",
    "print(\"Best trial : \\n{}\".format(trials.best_trial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
