{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.vgg import vgg16_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FurnitureVGG16BNSSD300Like(nn.Module):\n",
    "    fm_sizes = (9, 5, 3, 1)\n",
    "\n",
    "    def __init__(self, num_classes, pretrained=True):\n",
    "        super(FurnitureVGG16BNSSD300Like, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = (1, 1, 1, 1)\n",
    "        self.in_channels = (512, 256, 256, 256)\n",
    "\n",
    "        self.extractor = VGG16Extractor300(pretrained=pretrained)\n",
    "        self.cls_layers = nn.ModuleList()\n",
    "        for i in range(len(self.in_channels)):\n",
    "            self.cls_layers += [\n",
    "                nn.Conv2d(self.in_channels[i], self.num_anchors[i] * self.num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "        n_boxes = sum([i ** 2 for i in self.fm_sizes])\n",
    "        self.boxes_to_classes = []\n",
    "        for i in range(num_classes):\n",
    "            self.boxes_to_classes.append(nn.Linear(n_boxes, 1))\n",
    "\n",
    "        self.boxes_to_classes = nn.ModuleList(self.boxes_to_classes)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.drop = nn.Dropout(p=0.4)\n",
    "        self.final_classifier = nn.Linear(num_classes, num_classes)\n",
    "\n",
    "        for param in self.extractor.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_preds = []\n",
    "        xs = self.extractor(x)\n",
    "        for i, x in enumerate(xs):\n",
    "            cls_pred = self.cls_layers[i](x)\n",
    "            cls_pred = cls_pred.permute(0, 2, 3, 1).contiguous()\n",
    "            cls_preds.append(cls_pred.view(cls_pred.size(0), -1, self.num_classes))\n",
    "        cls_preds = torch.cat(cls_preds, 1)\n",
    "\n",
    "        merged_cls_preds = []\n",
    "        for i, m in enumerate(self.boxes_to_classes):\n",
    "            merged_cls_preds.append(m(cls_preds[:, :, i]))\n",
    "        merged_cls_preds = torch.cat(merged_cls_preds, 1)\n",
    "\n",
    "        out = self.relu(merged_cls_preds)\n",
    "        out = self.drop(out)\n",
    "        out = self.final_classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class VGG16Extractor300(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super(VGG16Extractor300, self).__init__()\n",
    "\n",
    "        self.features = vgg16_bn(pretrained=pretrained).features[0:-1]  # Ignore the last max poolling\n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "\n",
    "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1)\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1)\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1)\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3)\n",
    "\n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1)\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3)\n",
    "\n",
    "        self.top_features = nn.ModuleList([\n",
    "            self.conv6,\n",
    "            self.conv7,\n",
    "            self.conv8_1,\n",
    "            self.conv8_2,\n",
    "            self.conv9_1,\n",
    "            self.conv9_2,\n",
    "            self.conv10_1,\n",
    "            self.conv10_2,\n",
    "            self.conv11_1,\n",
    "            self.conv11_2,\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        hs = []\n",
    "        h = self.features(x)\n",
    "        h = F.max_pool2d(h, kernel_size=3, stride=1, padding=1, ceil_mode=True)\n",
    "\n",
    "        h = F.relu(self.conv6(h))\n",
    "        h = F.relu(self.conv7(h))\n",
    "\n",
    "        h = F.relu(self.conv8_1(h))\n",
    "        h = F.relu(self.conv8_2(h))\n",
    "        hs.append(h)  # conv8_2\n",
    "\n",
    "        h = F.relu(self.conv9_1(h))\n",
    "        h = F.relu(self.conv9_2(h))\n",
    "        hs.append(h)  # conv9_2\n",
    "\n",
    "        h = F.relu(self.conv10_1(h))\n",
    "        h = F.relu(self.conv10_2(h))\n",
    "        hs.append(h)  # conv10_2\n",
    "\n",
    "        h = F.relu(self.conv11_1(h))\n",
    "        h = F.relu(self.conv11_2(h))\n",
    "        hs.append(h)  # conv11_2\n",
    "        return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FurnitureVGG16BNSSD300Like(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 3, 300, 300)\n",
    "y = model.extractor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([4, 512, 9, 9]), torch.Size([4, 256, 5, 5]), torch.Size([4, 256, 3, 3]), torch.Size([4, 256, 1, 1])]\n"
     ]
    }
   ],
   "source": [
    "print([i.shape for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.852030263919617"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.log(1.0/128.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inception-ResnetV2 extractor for 350x350 SSD-like classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretrainedmodels.models.inceptionresnetv2 import inceptionresnetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FurnitureInceptionResNetV4350SSDLike_v2(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained='imagenet'):\n",
    "        super(FurnitureInceptionResNetV4350SSDLike_v2, self).__init__()\n",
    "\n",
    "        self.extractor = Extractor350_v2(pretrained=pretrained)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = (1, 1, 1)\n",
    "        self.in_channels = self.extractor.channels\n",
    "\n",
    "        self.cls_layers = nn.ModuleList()\n",
    "        for i in range(len(self.in_channels)):\n",
    "            self.cls_layers += [\n",
    "                nn.Conv2d(self.in_channels[i], self.num_anchors[i] * self.num_classes,\n",
    "                          kernel_size=3, padding=1),\n",
    "                nn.Sigmoid()\n",
    "            ]\n",
    "\n",
    "        n_levels = len(self.extractor.featuremap_sizes)\n",
    "        self.boxes_to_classes = []\n",
    "        for i in range(num_classes):\n",
    "            self.boxes_to_classes.append(nn.Linear(n_levels, 1))\n",
    "        self.boxes_to_classes = nn.ModuleList(self.boxes_to_classes)\n",
    "\n",
    "        self.inner_classifier = nn.Linear(n_levels * num_classes, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.4)\n",
    "        self.final_classifier = nn.Linear(2 * num_classes, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        cls_preds = []\n",
    "        xs = self.extractor(x)\n",
    "        # Transform output feature maps to bbox predictions\n",
    "        for i, x in enumerate(xs):\n",
    "            cls_pred = self.cls_layers[i](x)\n",
    "            cls_pred = cls_pred.permute(0, 2, 3, 1).contiguous()\n",
    "            cls_pred = cls_pred.view(cls_pred.size(0), -1, self.num_classes)\n",
    "            # Sum all predictions of all boxes at single level\n",
    "            cls_pred = torch.sum(cls_pred, dim=1).unsqueeze(1)\n",
    "            cls_preds.append(cls_pred)\n",
    "\n",
    "        # Two ways to aggregate\n",
    "        # A) Predictions from each bbox level are transformed with FC to a single probability\n",
    "        # for each target class\n",
    "        cls_preds_a = torch.cat(cls_preds, dim=1)\n",
    "        merged_cls_preds = []\n",
    "        for i, m in enumerate(self.boxes_to_classes):\n",
    "            merged_cls_preds.append(m(cls_preds_a[:, :, i]))\n",
    "        merged_cls_preds = torch.cat(merged_cls_preds, 1)\n",
    "        out_a = self.relu(merged_cls_preds)\n",
    "\n",
    "        # B) Predictions from each bbox level are transformed with FC to a vector of output probabilities\n",
    "        cls_preds_b = torch.cat(cls_preds, dim=2).squeeze(1)\n",
    "        out_b = self.inner_classifier(cls_preds_b)\n",
    "        out_b = self.relu(out_b)\n",
    "        \n",
    "        # Aggregate results:\n",
    "        out = torch.cat([out_a, out_b], dim=1)\n",
    "\n",
    "        out = self.drop(out)\n",
    "        out = self.final_classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Extractor350_v2(nn.Module):\n",
    "    featuremap_sizes = (20, 9, 1)\n",
    "    channels = (256, 256, 256)\n",
    "\n",
    "    def __init__(self, pretrained):\n",
    "        super(Extractor350_v2, self).__init__()\n",
    "\n",
    "        model = inceptionresnetv2(pretrained=pretrained)\n",
    "        self.stem = nn.Sequential(\n",
    "            model.conv2d_1a,\n",
    "            model.conv2d_2a,\n",
    "            model.conv2d_2b,\n",
    "            model.maxpool_3a,\n",
    "            model.conv2d_3b,\n",
    "            model.conv2d_4a,\n",
    "            model.maxpool_5a,\n",
    "        )\n",
    "\n",
    "        self.low_features_a = nn.Sequential(\n",
    "            model.mixed_5b,\n",
    "            model.repeat,\n",
    "        )\n",
    "\n",
    "        self.low_features_b = nn.Sequential(\n",
    "            model.mixed_6a,\n",
    "            model.repeat_1\n",
    "        )\n",
    "\n",
    "        self.mid_features = nn.Sequential(\n",
    "            model.mixed_7a,\n",
    "            model.repeat_2,\n",
    "            model.block8\n",
    "        )\n",
    "\n",
    "        self.top_features = nn.Sequential(\n",
    "            model.conv2d_7b,\n",
    "            model.avgpool_1a,\n",
    "\n",
    "        )\n",
    "        self.smooth2 = nn.Sequential(\n",
    "            nn.Conv2d(1088, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.smooth3 = nn.Sequential(\n",
    "            nn.Conv2d(2080, 320, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(320, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.top_smooth = nn.Sequential(\n",
    "            nn.Conv2d(1536, 256, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # aliases\n",
    "        self.smooth_layers = nn.ModuleList([\n",
    "            self.smooth2,\n",
    "            self.smooth3,\n",
    "            self.top_smooth,\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        x = self.stem(x)\n",
    "\n",
    "        x = self.low_features_a(x)\n",
    "\n",
    "        x = self.low_features_b(x)\n",
    "        out.append(self.smooth2(x))\n",
    "\n",
    "        x = self.mid_features(x)\n",
    "        out.append(self.smooth3(x))\n",
    "\n",
    "        x = self.top_features(x)\n",
    "        out.append(self.top_smooth(x))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FurnitureInceptionResNetV4350SSDLike_v2(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 3, 350, 350)\n",
    "y = model.extractor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([4, 256, 20, 20]), torch.Size([4, 256, 9, 9]), torch.Size([4, 256, 1, 1])]\n"
     ]
    }
   ],
   "source": [
    "print([i.shape for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RetinaNet for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.models.resnet import resnet50\n",
    "\n",
    "\n",
    "class FurnitureRetinaNetClassification(nn.Module):\n",
    "    num_anchors = 1\n",
    "\n",
    "    def __init__(self, num_classes, pretrained=True):\n",
    "        super(FurnitureRetinaNetClassification, self).__init__()\n",
    "        self.fpn = FPN50(pretrained)\n",
    "        self.num_classes = num_classes\n",
    "        self.cls_head = self._make_head(self.num_anchors * self.num_classes)\n",
    "        \n",
    "        n_levels = 5\n",
    "        self.inner_classifier = nn.Linear(n_levels * num_classes, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.4)\n",
    "        self.final_classifier = nn.Linear(num_classes, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        fms = self.fpn(x)\n",
    "        cls_preds = []\n",
    "        for fm in fms:\n",
    "            cls_pred = self.cls_head(fm)\n",
    "            cls_pred = cls_pred.permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.num_classes)\n",
    "            cls_pred = F.relu(cls_pred)\n",
    "            cls_pred, _ = torch.max(cls_pred, dim=1)\n",
    "            cls_preds.append(cls_pred)\n",
    "\n",
    "        cls_preds = torch.cat(cls_preds, dim=1)\n",
    "        out = self.inner_classifier(cls_preds)\n",
    "        out = self.drop(out)\n",
    "        out = self.final_classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_head(self, out_planes):\n",
    "        layers = []\n",
    "        for _ in range(4):\n",
    "            layers.append(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1))\n",
    "            layers.append(nn.ReLU(True))\n",
    "        layers.append(nn.Conv2d(256, out_planes, kernel_size=3, stride=1, padding=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class FPN50(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super(FPN50, self).__init__()\n",
    "        \n",
    "        self.resnet = resnet50(pretrained=pretrained)\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            self.resnet.conv1,\n",
    "            self.resnet.bn1,\n",
    "            self.resnet.relu,\n",
    "            self.resnet.maxpool            \n",
    "        )\n",
    "        \n",
    "        self.low_features = nn.Sequential(\n",
    "            self.resnet.layer1,\n",
    "            self.resnet.layer2,\n",
    "        )\n",
    "        self.layer3 = self.resnet.layer3        \n",
    "        self.layer4 = self.resnet.layer4                \n",
    "        \n",
    "        self.conv6 = nn.Conv2d(2048, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv7 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Top-down layers\n",
    "        self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # Lateral layers\n",
    "        self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.latlayer2 = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # Smooth layers\n",
    "        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Aliases\n",
    "        self.mid_features = nn.ModuleList([\n",
    "            self.layer3,\n",
    "            self.layer4,            \n",
    "        ])\n",
    "        \n",
    "        self.top_features = nn.ModuleList([\n",
    "            self.conv6,\n",
    "            self.conv7,\n",
    "            self.toplayer,\n",
    "            self.latlayer1,\n",
    "            self.latlayer2,\n",
    "            self.smooth1,\n",
    "            self.smooth2\n",
    "        ])        \n",
    "        \n",
    "    def _upsample_add(self, x, y):\n",
    "        \"\"\"Upsample and add two feature maps.\n",
    "\n",
    "        Args:\n",
    "          x: (Variable) top feature map to be upsampled.\n",
    "          y: (Variable) lateral feature map.\n",
    "\n",
    "        Returns:\n",
    "          (Variable) added feature map.\n",
    "\n",
    "        Note in PyTorch, when input size is odd, the upsampled feature map\n",
    "        with `F.upsample(..., scale_factor=2, mode='nearest')`\n",
    "        maybe not equal to the lateral feature map size.\n",
    "\n",
    "        e.g.\n",
    "        original input size: [N,_,15,15] ->\n",
    "        conv2d feature map size: [N,_,8,8] ->\n",
    "        upsampled feature map size: [N,_,16,16]\n",
    "\n",
    "        So we choose bilinear upsample which supports arbitrary output sizes.\n",
    "        \"\"\"\n",
    "        _, _, h, w = y.size()\n",
    "        return F.upsample(x, size=(h, w), mode='bilinear', align_corners=True) + y \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bottom-up\n",
    "        c1 = self.stem(x)\n",
    "        c3 = self.low_features(c1)\n",
    "        c4 = self.layer3(c3)\n",
    "        c5 = self.layer4(c4)        \n",
    "        p6 = self.conv6(c5)\n",
    "        p7 = self.conv7(F.relu(p6))\n",
    "        # Top-down\n",
    "        p5 = self.toplayer(c5)\n",
    "        p4 = self._upsample_add(p5, self.latlayer1(c4))\n",
    "        p4 = self.smooth1(p4)\n",
    "        p3 = self._upsample_add(p4, self.latlayer2(c3))\n",
    "        p3 = self.smooth2(p3)\n",
    "        return p3, p4, p5, p6, p7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FurnitureRetinaNetClassification(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 3, 350, 350)\n",
    "y = model.fpn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([4, 256, 44, 44]), torch.Size([4, 256, 22, 22]), torch.Size([4, 256, 11, 11]), torch.Size([4, 256, 6, 6]), torch.Size([4, 256, 3, 3])]\n"
     ]
    }
   ],
   "source": [
    "print([i.shape for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pretrainedmodels.models.inceptionresnetv2 import inceptionresnetv2\n",
    "\n",
    "\n",
    "class FurnitureInceptionResNet350RetinaLike(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained='imagenet'):\n",
    "        super(FurnitureInceptionResNet350RetinaLike, self).__init__()\n",
    "\n",
    "        self.fpn = InceptionResnetFPN350(pretrained=pretrained)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.cls_head = self._make_head(self.num_classes)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.base_classifier = nn.Linear(1536, num_classes)\n",
    "\n",
    "        self.boxes_classifier = nn.Linear(num_classes, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.final_classifier = nn.Linear(num_classes, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c5, fms = self.fpn(x)\n",
    "\n",
    "        # A) Standard classification:\n",
    "        out_a = self.avgpool(c5)\n",
    "        out_a = out_a.view(out_a.size(0), -1)\n",
    "        out_a = self.base_classifier(out_a)\n",
    "\n",
    "        # B) Boxes classification:\n",
    "        cls_preds = 0\n",
    "        for fm in fms:\n",
    "            cls_pred = self.cls_head(fm)\n",
    "            cls_pred = cls_pred.permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.num_classes)\n",
    "            cls_pred = F.relu(cls_pred)\n",
    "            cls_pred, _ = torch.max(cls_pred, dim=1)\n",
    "            cls_preds += cls_pred\n",
    "        out_b = self.boxes_classifier(cls_preds)\n",
    "\n",
    "        # Merge A + B\n",
    "        out = out_a + out_b\n",
    "        out = self.final_classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_head(self, out_planes):\n",
    "        layers = []\n",
    "        for _ in range(4):\n",
    "            layers.append(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1))\n",
    "            layers.append(nn.ReLU(True))\n",
    "        layers.append(nn.Conv2d(256, out_planes, kernel_size=3, stride=1, padding=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class InceptionResnetFPN350(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained):\n",
    "        super(InceptionResnetFPN350, self).__init__()\n",
    "\n",
    "        model = inceptionresnetv2(pretrained=pretrained)\n",
    "        self.stem = nn.Sequential(\n",
    "            model.conv2d_1a,\n",
    "            model.conv2d_2a,\n",
    "            model.conv2d_2b,\n",
    "            model.maxpool_3a,\n",
    "            model.conv2d_3b,\n",
    "            model.conv2d_4a,\n",
    "            model.maxpool_5a,\n",
    "        )\n",
    "\n",
    "        self.low_features = nn.Sequential(\n",
    "            model.mixed_5b,\n",
    "            model.repeat,\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            model.mixed_6a,\n",
    "            model.repeat_1\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            model.mixed_7a,\n",
    "            model.repeat_2,\n",
    "            model.block8,\n",
    "            model.conv2d_7b,\n",
    "        )\n",
    "\n",
    "        self.conv6 = nn.Conv2d(1536, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv7 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Top-down layers\n",
    "        self.toplayer = nn.Conv2d(1536, 256, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # Lateral layers\n",
    "        self.latlayer1 = nn.Conv2d(1088, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.latlayer2 = nn.Conv2d(320, 256, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # Smooth layers\n",
    "        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Aliases\n",
    "        self.mid_features = nn.ModuleList([\n",
    "            self.layer3,\n",
    "            self.layer4,\n",
    "        ])\n",
    "\n",
    "        self.top_features = nn.ModuleList([\n",
    "            self.conv6,\n",
    "            self.conv7,\n",
    "            self.toplayer,\n",
    "            self.latlayer1,\n",
    "            self.latlayer2,\n",
    "            self.smooth1,\n",
    "            self.smooth2\n",
    "        ])\n",
    "\n",
    "    def _upsample_add(self, x, y):\n",
    "        _, _, h, w = y.size()\n",
    "        return F.upsample(x, size=(h, w), mode='bilinear', align_corners=True) + y\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bottom-up\n",
    "        c1 = self.stem(x)\n",
    "        c3 = self.low_features(c1)\n",
    "        c4 = self.layer3(c3)\n",
    "        c5 = self.layer4(c4)\n",
    "        p6 = self.conv6(c5)\n",
    "        p7 = self.conv7(F.relu(p6))\n",
    "        # Top-down\n",
    "        p5 = self.toplayer(c5)\n",
    "        p4 = self._upsample_add(p5, self.latlayer1(c4))\n",
    "        p4 = self.smooth1(p4)\n",
    "        p3 = self._upsample_add(p4, self.latlayer2(c3))\n",
    "        p3 = self.smooth2(p3)\n",
    "        return c5, (p3, p4, p5, p6, p7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FurnitureInceptionResNet350RetinaLike(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 3, 350, 350)\n",
    "y = model.fpn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1536, 9, 9]) [torch.Size([4, 256, 41, 41]), torch.Size([4, 256, 20, 20]), torch.Size([4, 256, 9, 9]), torch.Size([4, 256, 5, 5]), torch.Size([4, 256, 3, 3])]\n"
     ]
    }
   ],
   "source": [
    "print(y[0].shape, [i.shape for i in y[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
